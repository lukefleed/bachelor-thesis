\section{Statistical Coding} \label{sec:statistical_coding}

This section explores a technique called \emph{statistical coding}: a method for compressing a sequence of symbols (\emph{texts}) drawn from a finite alphabet $\Sigma$. The idea is to divide the process in two key stages: modeling and coding. During the modeling phase, statistical characteristics of the input sequence are analyzed to construct a model, typically estimating the probability $P(\sigma)$ for each symbol $\sigma \in \Sigma$. In the coding phase, this model is utilized to generate codewords for the symbols, which are then employed to compress the input sequence. Two popular statistical coding methods will be focused upon: Huffman coding and Arithmetic coding.

\subsection{Huffman Coding} \label{subsec:huffman_coding}

Compared to the methods seen in Section~\ref{sec:integer_coding}, Huffman Codes, introduced by David A. Huffman in his landmark 1952 paper \cite{Huffman1952}, offer broader applicability. They construct optimal prefix-free codes for a given set of symbol probabilities, without requiring specific assumptions about the underlying distribution itself (beyond non-zero probabilities). This versatility makes them suitable for diverse data types, including text where symbol frequencies often lack a simple mathematical pattern.

For instance, in English text, the letter \emph{e} is far more frequent than \emph{z}, and simple integer codes based on alphabetical order would be highly inefficient. Huffman coding directly addresses this by assigning shorter codewords to more frequent symbols.

\paragraph{Construction of Huffman Codes} The construction algorithm is greedy and builds a binary tree bottom-up. Each symbol $\sigma \in \Sigma$ initially forms a leaf node, typically weighted by its probability $P(\sigma)$ or its frequency count $n_\sigma$. The algorithm repeatedly selects the two nodes (initially leaves, later internal nodes representing merged subtrees) with the smallest current weights, merges them into a new internal node whose weight is the sum of the two merged weights, and places the two selected nodes as its children. This process continues until only one node, the root, remains.

The prefix-free code for each symbol $\sigma$ is then determined by the path from the root to the leaf corresponding to $\sigma$. Conventionally, a $0$ is assigned to traversing a left branch and a $1$ to a right branch (or vice versa). The concatenation of these bits along the path forms the Huffman code for the symbol. More formal descriptions and variations can be found in \cite{ferragina2023pearls, sayood2002lossless, han2002mathematics, ElementsofInformationTheory}.

\begin{figure}[hbtp]
    \centering % Centra la figura
    \begin{tikzpicture}[
        level distance=12mm, % Distanza tra livelli
        level 1/.style={sibling distance=40mm}, % Distanza tra fratelli livello 1
        level 2/.style={sibling distance=20mm}, % Distanza tra fratelli livello 2
        level 3/.style={sibling distance=12mm}, % Distanza tra fratelli livello 3
        every node/.style={draw, circle, inner sep=1pt, font=\footnotesize, minimum size=5mm}, % Stile per nodi principali (cerchiati)
        char label/.style={draw=none, font=\footnotesize, anchor=north}, % Stile per etichette caratteri (solo testo, SENZA cerchio)
        edge label/.style={font=\tiny, draw=none, inner sep=0.5pt}, % Stile per etichette archi (solo testo, SENZA cerchio)
        edge from parent/.style={draw, -{Latex[length=1.5mm]}}, % Stile freccia archi
        edge from parent path={(\tikzparentnode.south) -- (\tikzchildnode.north)} % Percorso archi
        ]
        % Definizione dell'albero
        \node {1.00} % Nodo radice
        child { node {0.45} % Nodo interno ca
                child { node (c_leaf) {0.20} % Nodo foglia c (probabilità)
                        edge from parent node[left, edge label] {0}; % Etichetta arco 0
                        % Etichetta carattere sotto la foglia (usa lo stile 'char label')
                        \node[char label, below=1mm of c_leaf] {c};
                    }
                child { node (a_leaf) {0.25} % Nodo foglia a (probabilità)
                        edge from parent node[right, edge label] {1}; % Etichetta arco 1
                        % Etichetta carattere sotto la foglia (usa lo stile 'char label')
                        \node[char label, below=1mm of a_leaf] {a};
                    }
                edge from parent node[left, edge label] {0} % Etichetta arco 0
            }
        child { node {0.55} % Nodo interno bde
                child { node (b_leaf) {0.25} % Nodo foglia b (probabilità)
                        edge from parent node[left, edge label] {0}; % Etichetta arco 0
                        % Etichetta carattere sotto la foglia (usa lo stile 'char label')
                        \node[char label, below=1mm of b_leaf] {b};
                    }
                child { node {0.30} % Nodo interno de
                        child { node (d_leaf) {0.15} % Nodo foglia d (probabilità)
                                edge from parent node[left, edge label] {0}; % Etichetta arco 0
                                % Etichetta carattere sotto la foglia (usa lo stile 'char label')
                                \node[char label, below=1mm of d_leaf] {d};
                            }
                        child { node (e_leaf) {0.15} % Nodo foglia e (probabilità)
                                edge from parent node[right, edge label] {1}; % Etichetta arco 1
                                % Etichetta carattere sotto la foglia (usa lo stile 'char label')
                                \node[char label, below=1mm of e_leaf] {e};
                            }
                        edge from parent node[right, edge label] {1} % Etichetta arco 1
                    }
                edge from parent node[right, edge label] {1} % Etichetta arco 1
            };
    \end{tikzpicture}
    % Caption della figura
    \caption[Huffman tree example]{Huffman tree for the example probabilities ($P(a)= 0.25, P(b)=0.25, P(c)=0.2, P(d)=0.15, P(e)=0.15$). The resulting codes (0 for left, 1 for right) are: $C(a)=\texttt{01}$, $C(b)=\texttt{10}$, $C(c)=\texttt{00}$, $C(d)=\texttt{110}$, $C(e)=\texttt{111}$.}
    \label{fig:huffman_tree_example} % Etichetta per il riferimento alla figura
\end{figure}

\begin{example}[Huffman Coding Construction] \label{ex:huffman_construction}
    Let $\Sigma = \{a, b, c, d, e\}$ with probabilities $P(a)= 0.25$, $P(b)=0.25$, $P(c)=0.2$, $P(d)=0.15$, $P(e)=0.15$.
    \begin{enumerate}
        \item Initial nodes: (a: 0.25), (b: 0.25), (c: 0.2), (d: 0.15), (e: 0.15).
        \item Merge smallest: d and e. Create node (de: 0.30). Current nodes: (a: 0.25), (b: 0.25), (c: 0.2), (de: 0.30).
        \item Merge smallest: c and a. Create node (ca: 0.45). Current nodes: (b: 0.25), (de: 0.30), (ca: 0.45). (Note: Choosing c and a over c and b is arbitrary here; another valid tree exists).
        \item Merge smallest: b and de. Create node (bde: 0.55). Current nodes: (ca: 0.45), (bde: 0.55).
        \item Merge last two: ca and bde. Create root (root: 1.00).
    \end{enumerate}
    The resulting tree and codes (assigning 0 to left, 1 to right) are shown in Figure~\ref{fig:huffman_tree_example}.
\end{example}

Let $L_C = \sum_{\sigma \in \Sigma} P(\sigma) \cdot l(\sigma)$ be the average codeword length for a prefix-free code $C$, where $l(\sigma)$ is the length of the codeword assigned to symbol $\sigma$. The Huffman coding algorithm produces a code $C_H$ that is optimal among all possible prefix-free codes for the given probability distribution.

\begin{theorem}[Optimality of Huffman Codes] \label{thm:huffman_optimality}
    Let $C_H$ be a Huffman code generated for a given probability distribution $P$ over alphabet $\Sigma$. For any other prefix-free code $C'$ for the same distribution, the average codeword length satisfies $L_{C_H} \leq L_{C'}$.
\end{theorem}

This optimality signifies that no other uniquely decodable code assigning fixed codewords to symbols can achieve a shorter average length. The proof typically relies on induction or an exchange argument, demonstrating that any deviation from the greedy merging strategy cannot improve the average length \cite{ferragina2023pearls, sayood2002lossless, han2002mathematics, ElementsofInformationTheory}.

The length of individual Huffman codewords can vary. In the worst case, the longest codeword might approach $|\Sigma|-1$ bits (in a highly skewed distribution). However, a tighter bound related to the minimum probability $p_{\min}$ exists: the maximum length is $O(\log(1/p_{\min}))$ \cite{navarro2016compact}. If probabilities derive from empirical frequencies in a text of length $n$, then $p_{\min} \ge 1/n$, bounding the maximum codeword length by $O(\log n)$. The encoding process itself, once the tree (or equivalent structure) is built, is typically linear in the length of the input sequence $S$, i.e., $O(|S|)$.

Decoding uses the Huffman Tree (or an equivalent lookup structure). Bits are read sequentially from the compressed stream, traversing the tree from the root according to the bit values (e.g., 0 for left, 1 for right) until a leaf node is reached. The symbol associated with that leaf is output, and the process restarts from the root for the next symbol. The total decoding time is proportional to the total number of bits in the compressed sequence. Since individual codes have length $O(\log n)$ in the empirical case, decoding a single symbol takes at most $O(\log n)$ bit reads and tree traversals.

While optimal among prefix codes, Huffman coding still assigns an integer number of bits to each symbol. This leads to a slight inefficiency compared to the theoretical entropy limit, as quantified by the following theorem.

\begin{theorem} \label{thm:huffman_bounds}
    Let $\mathcal{H} = \sum_{\sigma \in \Sigma} P(\sigma)\log_2(1/P(\sigma))$ be the entropy of a source emitting symbols from $\Sigma$ according to distribution $P$. The average length $L_H$ of the corresponding Huffman code is bounded by
    \begin{equation*}
        \mathcal{H} \le L_H < \mathcal{H} + 1.
    \end{equation*}
\end{theorem}
\begin{proof}
    The lower bound $\mathcal{H} \le L_H$ follows from Shannon's source coding theorem (Theorem~\ref{thm:source_coding_theorem}), which states that $\mathcal{H}$ is the minimum possible average length for any uniquely decodable code.

    For the upper bound, assign to each symbol $\sigma$ the ideal codeword length $l'_\sigma = -\log_2 P(\sigma)$. Since codeword lengths must be integers, set $l_\sigma = \lceil -\log_2 P(\sigma) \rceil$. Then
    \begin{equation*}
        l_\sigma < -\log_2 P(\sigma) + 1.
    \end{equation*}
    These $l_\sigma$ satisfy Kraft's inequality:
    \begin{equation*}
        \sum_{\sigma \in \Sigma} 2^{-l_\sigma} \le \sum_{\sigma \in \Sigma} 2^{-(-\log_2 P(\sigma))} = \sum_{\sigma \in \Sigma} P(\sigma) = 1,
    \end{equation*}
    so a prefix code $C'$ with these lengths exists. Its average length is
    \begin{equation*}
        L_{C'} = \sum_{\sigma \in \Sigma} P(\sigma) l_\sigma < \sum_{\sigma \in \Sigma} P(\sigma) (-\log_2 P(\sigma) + 1) = \mathcal{H} + 1.
    \end{equation*}
    Since Huffman coding is optimal (Theorem~\ref{thm:huffman_optimality}), $L_H \le L_{C'} < \mathcal{H} + 1$. Thus,
    \begin{equation*}
        \mathcal{H} \le L_H < \mathcal{H} + 1.
    \end{equation*}
\end{proof}

This theorem highlights that the average Huffman code length is always within one bit of the source entropy. The gap $(L_H - \mathcal{H})$ represents the inefficiency due to the constraint of using integer bit lengths for each symbol's codeword. This gap is significant only when some symbol probabilities are very high (close to 1).

\subsection{Arithmetic Coding} \label{subsec:arithmetic_coding}

Introduced conceptually by Peter Elias in the 1960s and later developed into practical algorithms by Rissanen \cite{rissanen1976generalized} and Pasco \cite{pasco1976source} in the 1970s, Arithmetic Coding offers a more powerful approach to statistical compression than Huffman coding. Its key advantage lies in its ability to approach the theoretical entropy limit more closely, often achieving better compression ratios, especially when dealing with skewed probability distributions or when encoding sequences rather than individual symbols.

Unlike Huffman coding, which assigns a distinct, fixed-length (integer number of bits) prefix-free code to each symbol, Arithmetic coding represents an entire sequence of symbols as a single fraction within the unit interval $[0, 1)$. The length of the binary representation of this fraction effectively corresponds to the information content (entropy) of the entire sequence, allowing for an average representation that can use a fractional number of bits per symbol. This overcomes the inherent inefficiency of Huffman coding, which is bounded by $\mathcal{H} \le L_H < \mathcal{H} + 1$. Arithmetic coding aims to achieve a compressed size very close to $n\mathcal{H}$ bits for a sequence of length $n$.

\subsubsection{Encoding and Decoding}
Let $S = S[1]S[2]\ldots S[n]$ be the input sequence of symbols drawn from alphabet $\Sigma$, and let $P(\sigma)$ be the probability of symbol $\sigma$ according to the chosen statistical model.

The core idea of the encoding process (Algorithm~\ref{alg:arithmetic_coding}) is to progressively narrow down a sub-interval of $[0, 1)$. Initially, the interval is $[l_0, h_0) = [0, 1)$. For each symbol $S[i]$ in the sequence, the current interval $[l_{i-1}, h_{i-1})$ of size $s_{i-1} = h_{i-1} - l_{i-1}$ is partitioned into smaller sub-intervals, one for each symbol $\sigma \in \Sigma$. The size of the sub-interval for $\sigma$ is proportional to its probability, $s_{i-1} \cdot P(\sigma)$. The algorithm then selects the sub-interval corresponding to the actual symbol $S[i]$ and makes it the new current interval $[l_i, h_i)$ for the next step. The cumulative probability function $C(\sigma) = \sum_{\sigma' < \sigma} P(\sigma')$ (summing probabilities of symbols lexicographically smaller than $\sigma$) is used to efficiently calculate the start ($l_i$) of the correct sub-interval. After processing all $n$ symbols, the final interval is $[l_n, h_n) = [l_n, l_n + s_n)$, where $s_n = \prod_{i=1}^n P(S[i])$.

\begin{algorithm}[hbtp] % Use hbtp placement
    \caption{Arithmetic Coding} \label{alg:arithmetic_coding}
    \small
    \begin{algorithmic}[1] % Add line numbers
        \Require Sequence $S=S[1..n]$, Probabilities $P(\sigma)$ for $\sigma \in \Sigma$
        \Ensure A sub-interval $[l_n, l_n+s_n)$ uniquely identifying $S$.
        \State Compute cumulative probabilities $C(\sigma) = \sum_{\sigma' < \sigma} P(\sigma')$
        \State $l \gets 0$
        \State $s \gets 1$ \Comment{Initial interval [0, 1), size 1}
        \For{$i = 1$ to $n$}
        \State $l_{new} \gets l + s \cdot C(S[i])$ \Comment{Calculate start of sub-interval}
        \State $s_{new} \gets s \cdot P(S[i])$ \Comment{Calculate size of sub-interval}
        \State $l \gets l_{new}$
        \State $s \gets s_{new}$
        \EndFor
        \State \Return $[l, l+s)$ \Comment{Final interval represents the sequence}
    \end{algorithmic}
\end{algorithm}

The final output of the encoder is not the interval itself, but rather a binary fraction $x$ that falls within this final interval $[l_n, l_n + s_n)$ and can be represented with the fewest possible bits. Practical implementations use techniques to incrementally output bits as soon as they are determined (i.e., when the interval lies entirely within $[0, 0.5)$ or $[0.5, 1)$) and rescale the interval to maintain precision using fixed-point arithmetic \cite{moffat1998arithmetic, ferragina2023pearls}.

The decoding process (Algorithm~\ref{alg:arithmetic_decoding}) essentially reverses the encoding. The decoder needs the compressed bitstream (representing the fraction $x$), the same probability model $P(\sigma)$, and the original sequence length $n$. It starts with the interval $[0, 1)$. In each step $i$, it determines which symbol $\sigma$'s sub-interval $[l + s \cdot C(\sigma), l + s \cdot C(\sigma) + s \cdot P(\sigma))$ contains the encoded fraction $x$. That symbol $\sigma$ must be $S[i]$. The decoder outputs $\sigma$ and updates its current interval to be this sub-interval, just as the encoder did. This is repeated $n$ times to reconstruct the original sequence $S$.

\begin{algorithm}[hbtp]
    \caption{Arithmetic Decoding} \label{alg:arithmetic_decoding}
    \small
    \begin{algorithmic}[1]
        \Require Encoded fraction $x$, Probabilities $P(\sigma)$, Sequence length $n$.
        \Ensure Original sequence $S[1..n]$.
        \State Compute cumulative probabilities $C(\sigma) = \sum_{\sigma' < \sigma} P(\sigma')$
        \State $l \gets 0$
        \State $s \gets 1$
        \State $S \gets \text{empty sequence}$
        \For{$i = 1$ to $n$}
        \State Find symbol $\sigma$ s.t $l+s \cdot C(\sigma) \le x < l+s \cdot (C(\sigma)+P(\sigma))$
        \State Append $\sigma$ to $S$
        \State $l_{new} \gets l + s \cdot C(\sigma)$
        \State $s_{new} \gets s \cdot P(\sigma)$
        \State $l \gets l_{new}$
        \State $s \gets s_{new}$
        \EndFor
        \State \Return $S$
    \end{algorithmic}
\end{algorithm}

\subsubsection{Efficiency of Arithmetic Coding}
The final interval size $s_n = \prod_{i=1}^n P(S[i])$ is crucial. If empirical probabilities $P(\sigma) = n_\sigma / n$ are used, where $n_\sigma$ is the frequency of $\sigma$ in $S$, then $s_n = \prod_{\sigma \in \Sigma} (n_\sigma / n)^{n_\sigma}$. As noted before, the number of bits required to uniquely specify a number within an interval of size $s_n$ is approximately $-\log_2 s_n$.

Calculating $-\log_2 s_n$ with empirical probabilities gives:
\begin{align*}
    -\log_2 \left( \prod_{\sigma \in \Sigma} \left(\frac{n_\sigma}{n}\right)^{n_\sigma} \right) & = -\sum_{\sigma \in \Sigma} n_\sigma \log_2 \left(\frac{n_\sigma}{n}\right)            \\
                                                                                                & = n \sum_{\sigma \in \Sigma} \frac{n_\sigma}{n} \log_2 \left(\frac{n}{n_\sigma}\right) \\
                                                                                                & = n\mathcal{H}
\end{align*}
where $\mathcal{H}$ is the empirical (0-th order) entropy of the sequence $S$. This demonstrates that the \emph{ideal} number of bits needed by arithmetic coding matches the entropy of the sequence exactly.

The connection between the final interval size $s_n$ and the actual number of output bits deserves clarification. The encoder needs to transmit a binary representation of \emph{some} number $x$ that lies within the final interval $[l_n, l_n + s_n)$. To ensure the decoder can uniquely identify this interval (and thus the sequence), the chosen number $x$ must be distinguishable from any number lying in adjacent potential intervals. This requires a certain precision. The minimum number of bits $k$ needed to represent such an $x$ as a dyadic fraction (i.e., a number of the form $N/2^k$) must satisfy $2^{-k} \le s_n$. This condition ensures that the precision $2^{-k}$ is fine enough to pinpoint a unique value within the target interval of size $s_n$. Taking logarithms, this implies $k \ge -\log_2 s_n$. To guarantee that such a fraction actually exists within the interval, and to handle the process of incrementally outputting bits, practical arithmetic coding requires slightly more bits than the theoretical minimum $-\log_2 s_n$. A careful analysis shows that at most 2 extra bits are needed beyond the ideal $n\mathcal{H}$.

\begin{theorem} \label{thm:arithmetic_bits}
    The number of bits emitted by arithmetic coding for a sequence $S$ of $n$ symbols, using probabilities $P(\sigma)$ derived from the empirical frequencies within $S$, is at most $2 + n\mathcal{H}$, where $\mathcal{H}$ is the empirical entropy of the sequence $S$.
\end{theorem}
\begin{proof}
    Formal proofs can be found in standard texts on information theory and data compression \cite{ferragina2023pearls, sayood2002lossless, han2002mathematics, ElementsofInformationTheory}. The core idea, as outlined above, relates the required number of bits $k$ to the final interval size $s_n = 2^{-n\mathcal{H}}$ via $k \approx -\log_2 s_n = n\mathcal{H}$. The additive constant accounts for representing a specific point within the interval.
\end{proof}

\begin{remark}
    Practical arithmetic coders do not use floating-point numbers due to precision issues. They employ integer arithmetic, maintaining the interval bounds $[L, H)$ as large integers within a fixed range (e.g., 16 or 32 bits). As the conceptual interval shrinks, common leading bits of $L$ and $H$ are output, and the integer interval is rescaled (e.g., doubled) to occupy the full range again, effectively shifting the conceptual interval. Special handling ("underflow") is needed when the interval becomes very small but straddles the midpoint (e.g., 0.5), preventing immediate output of the next bit. These implementation details ensure correctness and efficiency with fixed-precision arithmetic \cite{moffat1998arithmetic}.
\end{remark}
