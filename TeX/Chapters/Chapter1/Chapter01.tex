% Chapter 1

\chapter{Introduction} % Chapter title

\label{ch:introduction} % For referencing the chapter elsewhere, use \autoref{ch:introduction}

\section{Why Succinct Data Structures?}
\label{sec:why_succinct}

The digital age is characterized by an exponential and seemingly boundless increase in data generation and collection. This phenomenon, while not new, has dramatically shifted in scale and nature over time. We have progressed from early structured databases and text digitization to the vast heterogeneity of the World Wide Web. Subsequently, major scientific endeavors began producing unprecedented data volumes: consider genomics projects sequencing DNA, large-scale climate simulations, or astronomical observatories surveying the cosmos. The rise of social networks then turned billions of users into continuous producers of content (text, images, video) and relational data. More recently, the Internet of Things (IoT) has deployed sensors capturing real-time environmental and operational data, while the ascent of Large Language Models (LLMs) and generative AI relies on training over astronomical datasets and necessitates efficient representations for the models themselves.

This relentless data production often outpaces our ability to process it effectively. While storage technology (hard drives, SSDs, cloud storage) continually improves, allowing us to archive petabytes and exabytes, a persistent bottleneck remains the main memory (RAM) capacity of computers. Accessing data in RAM is crucial for performance, being orders of magnitude faster (typically a factor of $\sim 10^5$) than accessing secondary storage \cite{navarro2016compact}. Consequently, fitting the necessary data - and critically, the auxiliary structures built upon it - into RAM is vital for countless data-intensive applications.

Indeed, the challenge frequently lies not just with the size of the raw data, but with the footprint of the data structures required for efficient querying and manipulation. Classical indices, trees, graphs, and other structures, while enabling fast operations, can demand significantly more space than the data they represent - sometimes one or two orders of magnitude larger. The human genome is a compelling example: the sequence of roughly 3.3 billion bases can be stored in under 800 megabytes using a simple 2-bit encoding. However, powerful structures like suffix trees, essential for many pattern matching and sequence analysis tasks, can easily consume tens of gigabytes, exceeding the RAM capacity of typical machines \cite{navarro2016compact}.

Several paradigms exist to cope with massive datasets: secondary-memory algorithms optimizing disk access, streaming algorithms processing data "on the fly" with limited memory, and distributed algorithms partitioning data and computation across clusters. Each has strengths and limitations, often involving trade-offs in performance, accuracy, or applicability. Furthermore, the proliferation of devices with constrained computational and memory resources (from smartphones to embedded sensors) creates scenarios where space is a primary constraint.

Data compression offers a well-established method for reducing storage space. However, most compression algorithms require full or partial decompression before the data can be randomly accessed or queried, making them unsuitable for tasks requiring direct interaction with the compressed form. This is precisely where Succinct Data Structures emerge as a powerful alternative. Situated at the intersection of Data Structures and Information Theory, they aim to represent data using space close to its theoretical minimum (often related to information-theoretic measures like entropy) while simultaneously supporting efficient queries directly on the compressed representation \cite{navarro2016compact}.

Succinct data structures strive to achieve the best of both worlds: the space efficiency of compression and the operational efficiency of traditional data structures. By compressing both the data itself and the indexing overhead needed to query it, they enable larger datasets and more complex structures to fit within faster levels of the memory hierarchy (e.g., cache instead of RAM, RAM instead of disk). This not only facilitates solving larger problems on a single machine but can also yield performance improvements due to better memory locality and reduced data transfer costs.

% The development of these structures has historical roots, with the formal study often traced back to Jacobson's 1988 Ph.D. thesis \cite{Jacobson}. A significant leap forward occurred in the early 2000s with the formalization and efficient implementation of fundamental Rank and Select operations on bitvectors. These operations became cornerstones for many subsequent structures, such as the influential Fully Indexable Dictionaries (FIDs) by Raman, Raman, and Rao, which effectively utilized techniques like Elias-Fano coding for sparse sets \cite{RRR2002}. Building on these primitives, innovations like Wavelet Trees were introduced for handling larger alphabets and general sequences \cite{GrossiWT2003}. Further research, including work by Sadakane and Grossi, focused on pushing the space usage of these structures closer to theoretical entropy limits while maintaining efficient query capabilities \cite{sadakane2006squeezing}. These milestones, among others, have contributed to a vibrant field with practical implementations and growing adoption across various application domains.

\section{Results and Contributions}
\label{sec:results_contributions}

This thesis introduces a novel representation for node-weighted Directed Acyclic Graphs (DAGs), specifically designed to efficiently support path-based aggregation queries.

Building upon foundational concepts in data compression (\autoref{ch:Chapter2}) and established succinct techniques for sequences, particularly Rank and Select operations (\autoref{ch:Chapter3}), this work addresses the challenge of managing potentially complex path information in weighted DAGs. A key motivation stems from the observation that problems like querying occurrences within degenerate strings (\autoref{sec:degenerate_strings}) can be effectively modeled using weighted DAGs (\autoref{subsec:dag_from_degenerate}), but the application extends to other domains involving path analysis in acyclic graph structures.

The core technical contribution, presented in \autoref{chap:succinct_dags}, is a space-efficient DAG representation that supports a generalized \Rank{} query (\ref{def:rank_dag}). This query aims to characterize the set of possible cumulative weights achievable on paths from a source vertex to a target vertex $N$, considering the contribution of $N$'s weight itself.

Our approach hinges on a strategic partitioning of the DAG's vertices into explicit nodes ($V_E$, whose path weight information is stored directly) and implicit nodes ($V_I$). For implicit nodes, the path weight information ($\mathcal{O}$-sets, \ref{def:o_set}) is reconstructed on demand by following a path defined by a carefully chosen successor function $\sigma$ (\ref{def:sigma_function}). This traversal relies on compact offset sequences ($\mathcal{I}_v$) stored for each implicit node $v$, which map indices in $\mathcal{O}_v$ to indices in the $\mathcal{O}$-set of its successor $\sigma(v)$. We provide algorithms (\textsc{GetValue} at \ref{alg:get_value}, \textsc{GetOSet} at \ref{alg:get_o_set}) for reconstructing $\mathcal{O}$-set information (\autoref{subsec:reconstructing_o_sets}) and computing the final \Rank{} query (\autoref{subsec:computing_rank}) based on this representation.

A crucial aspect is the analysis of compression strategies for the structure's components (\autoref{sec:compression_strategies}), including vertex weights, successor information, and the associated data sequences ($\mathcal{O}_v$ and $\mathcal{I}_v$). We leverage techniques like variable-length integer coding (potentially using implementations like those discussed in \autoref{app:compressed_intvec_engineering}), Elias-Fano coding for monotonic sequences (\autoref{sec:elias_fano_code}), and Run-Length Encoding (RLE) to minimize space (\autoref{subsec:compressing_associated_data_sequences}).

Significantly, we demonstrate (\autoref{sec:below_entropy}) that the space usage of our proposed structure can be substantially lower than the theoretical $0^{th}$-order entropy bound required for a lossless representation of the entire input graph $G=(V, E, w)$. This efficiency is achieved because our structure is specifically tailored to the \Rank{} query and does not need to store the full graph topology (all edges $E$), thereby offering a highly space-efficient solution for its designated task compared to both general-purpose graph encodings and naive precomputation approaches.

\section{Structure of the thesis}
\label{sec:structure}

This thesis is organized as follows. \autoref{ch:introduction}, the current chapter, provides the motivation for studying succinct data structures, outlines the main contributions of this thesis concerning succinct representations for weighted DAGs, and describes the overall structure of the document. \autoref{ch:Chapter2} then lays the theoretical groundwork by reviewing fundamental concepts from information theory and essential compression techniques relevant for building compact representations, such as entropy, source coding, integer coding, and statistical coding. Following this, \autoref{ch:Chapter3} introduces core building blocks used in many succinct data structures, focusing on Rank and Select operations, their implementation on bitvectors and wavelet trees, and their application to degenerate strings. The primary research contribution is presented in \autoref{chap:succinct_dags}, which details the proposed succinct representation for weighted DAGs, including the mathematical framework, the structure itself, query algorithms, compression strategies, and space efficiency analysis. \autoref{chap:conclusion} summarizes the findings and contributions, discusses limitations, and proposes future research directions aimed at enhancing query time predictability. Finally, \autoref{app:compressed_intvec_engineering} provides practical implementation details for a compressed integer vector structure supporting efficient random access, complementing the theoretical discussion of compression techniques.
