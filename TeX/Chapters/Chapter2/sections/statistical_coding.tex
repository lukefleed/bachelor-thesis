\clearpage
\section{Statistical Coding}

TODO: Introduction to statistical coding, from \cite{han2002mathematics} and \cite{ferragina2023pearls}

\subsection{Huffman Coding}

TODO: Chapter 4 of \cite{sayood2002lossless}, chapter 12 of \cite{ferragina2023pearls}, 3.8 from \cite{han2002mathematics}, 5.6/7/8 of \cite{ElementsofInformationTheory}, 2.6 of \cite{navarro2016compact}. \\


\noindent Introduction of the type of source and the situation we are in.\\

% \noindent As a preliminary for giving the theorem that provides an algorithm for constructing optimum codes (the Huffman code), we give the following lemma, from the book \cite{han2002mathematics}

% \begin{lemma}
%     Let us consider a source $X$ subject to a probability distribution $Q$ on a source alphabet $\mathcal{X} = \{0, 1, \dots, J-1\}$ with $J = m(K-1) + 1$. We assume that the probabilities $Q(0), Q(1), \dots, Q(J-1)$ are ordered in non-increasing order. and denote by
% \end{lemma}

Talk about construction, encoding and decoding, canonical Huffman codes, and the optimality of Huffman codes.

\begin{example}
    TODO: Classic example of Huffman coding with for example $\mathcal{X} = \{a,b,c,d,e\}$ and $P(a)= 0.25$, $P(b)=0.25$, $P(c)=0.2$, $P(d)=0.15$, $P(e)=0.15$. Do a nice tree and show the encoding of each symbol.
\end{example}

\begin{theorem}
    Let $H$ be the entropy of a source emitting the symbols of an alphabet $\Sigma$, hence $H = \sum_{\sigma \in \Sigma} P(\sigma)\log_2\left(\frac{1}{P(\sigma)}\right)$. Then, the average length of the Huffman code is $L \leq H < L + 1$.
\end{theorem}
\begin{proof}
    TODO: from \cite{ferragina2023pearls}, page 215.
\end{proof}
TODO and TBD: There are a lot of comments that can be done after this theorem. Still to decide what to include.

\subsection{Arithmetic Coding}
