\clearpage
\section{Statistical Coding} \label{sec:statistical_coding}

This section explores a technique called \emph{statistical coding}: a method for compressing a sequence of symbols (\emph{texts}) drawn from a finite alphabet $\Sigma$. The idea is to divide the process in two key stages: modeling and coding. During the modeling phase, statistical characteristics of the input sequence are analyzed to construct a model. In the coding phase, this model is utilized to generate codewords for the symbols of $\Sigma$, which are then employed to compress the input sequence. We will focus on two popular statistical coding methods: Huffman coding and Arithmetic coding.

\subsection{Huffman Coding}

Compared to the methods seen in \autoref{sec:integer_coding}, Huffman Codes (introduced by Huffman in his landmark paper \cite{Huffman1952} in the 1950s) offer a broader applicability as they do not require any specific assumptions about the probability distribution, only that all probabilities are non-zero. This versatility makes them suitable for all distributions, including those where there is no clear relationship between symbol number and probability, such as in text data. \vspace{0.4cm}

\noindent For example, in text, characters typically range from "\emph{a}" to "\emph{z}" and are often mapped to a contiguous range, such as $0$ to $25$ or $97$ to $122$ in ASCII. However, there is no direct correlation between a symbol's number and its frequency rank.

% Sources for this section: Chapter 4 of \cite{sayood2002lossless}, chapter 12 of \cite{ferragina2023pearls}, 3.8 from \cite{han2002mathematics}, 5.6/7/8 of \cite{ElementsofInformationTheory}, 2.6 of \cite{navarro2016compact} and the origianl paper \cite{Huffman1952}\\

% \noindent The Huffman coding procedure produces an optimal prefix code; however, this doesn't imply that produces an optimal encoding. It's optimal in the sense that no prefix code can have a smaller average length.

\paragraph*{Construction of Huffman Codes} The construction of Huffman codes is a greedy algorithm based on the idea of building a binary tree, where each leaf corresponds to a symbol in the alphabet $\Sigma$. The tree is built in a bottom-up fashion, starting with the symbols as leaves (we define their \emph{size} as the number of occurrences) and iteratively merging the two nodes with the smallest probabilities until a single node is left. The code for each symbol is then obtained by traversing the tree from the root to the leaf, assigning a $0$ for each left branch and a $1$ for each right branch (or vice-versa). The resulting code is the path from the root to the leaf. More details can be found in \cite{ferragina2023pearls,sayood2002lossless,han2002mathematics,ElementsofInformationTheory}

\begin{example}
    TODO: Classic example of Huffman coding with for example $\mathcal{X} = \{a,b,c,d,e\}$ and $P(a)= 0.25$, $P(b)=0.25$, $P(c)=0.2$, $P(d)=0.15$, $P(e)=0.15$. Do a nice tree and show the encoding of each symbol.
\end{example}

\noindent Let $L_C = \Sigma_{\sigma \in \Sigma} L(\sigma) \cdot P[\sigma]$ be the average length of the codewords produced by a prefix-free code $C$, that encodes every symbol $\sigma \in \Sigma$ with a codeword of length $L(\sigma)$. The Huffman coding produces optimal prefix codes (not in the sense that produces an optimal encoding, but in the sense that no prefix code can have a smaller average length). This is formalized in the following theorem.

\begin{theorem}[Optimality of Huffman Codes] \label{thm:huffman_optimality}
    Let $C$ be an Huffman Code and $L_C$ is the shortest possible average length among all prefix-free codes $C'$. That is, $L_C \leq L_{C'}$
\end{theorem}

\noindent This can also be interpreted as the \emph{the minimality of the average depth} of the Huffman tree. A proof can be found in most information theory books \cite{ferragina2023pearls,sayood2002lossless,han2002mathematics,ElementsofInformationTheory}. \vspace{0.4cm}

\noindent In the worst case, an Huffman Code can have a length of $|\Sigma|-1$ bits, which is the same as the number of internal nodes in the tree. However, its length is limited also by $\lfloor \log_\varPhi \frac{1}{p_{\min}} \rfloor$, where $p_{\min}$ is the smallest probability in the set and $\varPhi$ is the golden ratio. \cite{navarro2016compact}. Thus, if the probabilities come from the observed frequencies of the symbols in the text, let's say $n$ symbols, then $p_{\min} \geq \frac{1}{n}$ and the maximum length of the code is $\log_\varPhi n $. In particular, the encoding process is linear in the size of the input text \footnote{In the RAM model, $O(\log n)$ bits can be manipulated in $O(1)$, so the this is true also in practice}.

The decoding process uses the Huffman Tree. It starts by reading consecutive bits from the stream and traversing the tree from the root towards a leaf based on the read bits. Upon reaching a leaf, we output the symbol it represents and then reset back to the root of the tree. Consequently, the overall decoding duration scales proportionally with the length of the compressed sequence in bits, denoted as $O(n(H(Pr) + 1))$. Since the codes are of length $O(log n)$, it follows that any symbol can be decoded within $O(log n)$ time.

\begin{theorem}
    Let $H$ be the entropy of a source emitting the symbols of an alphabet $\Sigma$, hence $H = \sum_{\sigma \in \Sigma} P(\sigma)\log_2\left(\frac{1}{P(\sigma)}\right)$. Then, the average length of the Huffman code is bounded by $H < L_H < H + 1$, where $L_H$ is the average length of the Huffman code.
\end{theorem}
\begin{proof}
    The first inequality comes from Shannon's source coding theorem (\autoref{thm:source_coding_theorem}). Let's define $l_\sigma = \lceil -\log_2 P(\sigma) \rceil$ as the length of the code for symbol $\sigma$, which is the smallest integer such upper bounding Shannon's optimal codeword length. We can easily derive that $\sum_{\sigma \in \Sigma} 2^{-l_\sigma} \leq 1$. Thus, recalling Kraft's inequality (\autoref{thm:kraft_inequality}), we have that exists a binary tree with $|\Sigma|$ leaves and depths $l_\sigma$ for each leaf. This tree is a prefix code, and its average codeword length is $L_C = \sum_{\sigma \in \Sigma} P(\sigma) \cdot l_\sigma$. By optimality of the Huffman code (\ref{thm:huffman_optimality}), we have that $L_H \leq L_C$; thus from the definition of entropy $H$ and from the inequality $l_\sigma < 1 + \log_2\left(\frac{1}{P(\sigma)}\right)$, we have that $H < L_H < H + 1$.
\end{proof}

TBD: Do I talk about canonical Huffman codes? Do I talk more about the optimality of Huffman codes?

\subsection{Arithmetic Coding}

Introduced by Elias in the 1960s and then refined by Rissanen and Pasco in the 1970s \cite{pasco1976source}. Arithmetic coding is a more general technique than Huffman coding, as it can achieve a better compression ratio (it can code symbols arbitrary close to 0-th order entropy) by encoding a sequence of symbols as a single number in the interval $[0,1)$. This number is then converted into a binary representation. \vspace{0.4cm}

\noindent Consider any prefix coder, like Huffman coding. If we apply Huffman coding to a sequence of symbols, it must use \emph{at least one bit} (or more generally, an integer number of bits) per symbol, that is more then ten times the entropy of the source. This makes any prefix coder far from the $0$-th order entropy of the source. From the definition of Huffman Coding we can easily derive that it can be optimal only if $-\log p$ is a natural number, thus if and only if $p = 2^{-k}$ for some $k \in \mathbb{N}$. Arithmetic coding relaxes the request to define a prefix-free code for each symbol, and instead defines a strategy in which every bit of the output can be used to represent more than one symbol.

\subsubsection*{Encoding and Decoding Process}
Let $S[1,n]$ be a sequence of symbols drawn from an alphabet $\Sigma$ and $P(\sigma)$ be the probability of symbol $\sigma \in \Sigma$. The encoding process of arithmetic coding is described in algorithm \ref{alg:arithmetic_coding}. The algorithm is an iterative process: it starts by initializing the interval $[0,1)$ and then iterates over the symbols of the input sequence, splitting for each symbol the interval into a sub-interval with length proportional to the probability of the symbol. We then replace the current interval with the sub-interval corresponding to the next symbol and continue until all symbols are processed.

\begin{algorithm}
    \caption{Arithmetic Coding} \label{alg:arithmetic_coding}
    \begin{algorithmic}
        \Require $S[1,n]$, $P(\sigma)$ for each $\sigma \in \Sigma$
        \Ensure A sub-interval $[l,l+s)$ of $[0,1)$
        \State Compute the cumulative probabilities $C(\sigma) = \sum_{\sigma' \in \Sigma: \sigma' \leq \sigma} P(\sigma')$
        \State $s_0 = 1$, $l_0 = 0, i = 1$
        \While {$i \leq n$}
        \State $s_i = s_{i-1} \cdot P(S[i])$
        \State $l_i = l_{i-1} + s_{i-1} \cdot C(S[i])$
        \State $i = i + 1$
        \EndWhile
        \State \Return $x \in [l_n, l_n + s_n), n$
    \end{algorithmic}
\end{algorithm}

\noindent At the end, the algorithm doesn't output a sub-interval, but a single number $x$ in the interval $[l_n, l_n + s_n)$ and the length of the input sequence; this number has to be a dyadic fraction, that is a fraction with a denominator that is a power of $2$. The encoding sequence is then given by the numerator of $x$ in its binary representation, using $k$ bits, where $k$ is the power of $2$ that is the denominator of $x$. The details on how to choose a dyadic number in the interval $[l_n, l_n + s_n)$ that can be encoded in a few bits can be found in \cite{ferragina2023pearls,han2002mathematics,sayood2002lossless}.


\noindent The decoding process shown at \ref{alg:arithmetic_decoding}. Since the same statistical model is used by the encoder and decoder, the decoder can reconstruct the original sequence by following the same steps as the encoder.

\begin{algorithm}
    \caption{Arithmetic Decoding} \label{alg:arithmetic_decoding}
    \begin{algorithmic}
        \Require The binary sequence $b[1,k]$ representing the compressed output, $P(\sigma)$ for each $\sigma \in \Sigma$, $n$
        \Ensure The sequence $S[1,n]$
        \State Compute the cumulative probabilities $C(\sigma) = \sum_{\sigma' \in \Sigma: \sigma' \leq \sigma} P(\sigma')$
        \State $s_0 = 1$, $l_0 = 0, i = 1$
        \While {$i \leq n$}
        \State Split the interval $[l_{i-1}, l_{i-1} + s_{i-1})$ into $|\Sigma|$ sub-intervals
        \State Find the $\sigma$ corresponding to the sub-interval containing $x$
        \State $S = S.append(\sigma)$
        \State $s_i = s_{i-1} \cdot P(\sigma)$
        \State $l_i = l_{i-1} + s_{i-1} \cdot C(\sigma)$
        \State $i = i + 1$
        \EndWhile
        \State \Return $S$
    \end{algorithmic}
\end{algorithm}

\subsubsection*{Efficiency of Arithmetic Coding}

It's easy to derive that if we choose the probabilities of the symbols to be the empirical frequencies of the symbols in the input sequence, denoting with $p_s = \frac{n_s}{n}$ the probability of symbol $s$ in the sequence, then the size of the final interval will be

\begin{equation}
    s_n = \prod_{i=1}^n p_{S[i]}  = \frac{n_{S[1]} \cdot n_{S[2]} \cdots n_{S[n]}}{n^n} = \prod_{s \in \Sigma} \left (\frac{n_s}{n} \right)^{n_s}
\end{equation}

What is interesting about this formula, is that the size of the final interval, is independent from the order of the symbols in the sequence, but only from the number of times each symbol appears in the sequence. This means that the output is invariant to permutations of the input sequence. We can now state the following theorem. Thus any interval of size $\gamma$ contains a dyadic number with the power of the denominator equal to $-log \gamma + 1$, thus the number of bits needed by the encoder is at most

\begin{equation}
    1 - \log \prod_{s \in \Sigma} \left (\frac{n_s}{n} \right)^{n_s} = 1 + \sum_{s \in \Sigma} n_s \log \frac{n}{n_s}
\end{equation}

\noindent That is just one bit far from the empirical entropy of the sequence. In can also be proved \cite{ferragina2023pearls, han2002mathematics, sayood2002lossless} that the number of bits emitted by arithmetic coding for a sequence $S$ of $n$ symbols is at most $2 + n\mathcal{H}$, where $\mathcal{H}$ is the empirical entropy of the sequence $S$.

\begin{theorem}
    The number of bits emitted by arithmetic coding for a sequence $S$ of $n$ symbols is at most $2 + n\mathcal{H}$, where $\mathcal{H}$ is the empirical entropy of the sequence $S$.
\end{theorem}

TBD: Do I add the proof?
