\section{Source and Code} \label{sec:source_and_codes}

TODO: Some introduction about the source and coding, maybe with some very simple example like the morse code that uses a single dot to represent the most common symbol.

% We enrich the understanding of entropy by establishing its core role in setting the fundamental limit for information compression. This process involves condensing data by assigning shorter descriptions to more frequent outcomes and longer descriptions to less frequent ones. For example, Morse code uses a single dot to represent the most common symbol. Within this chapter, we ascertain the minimum average description length for a random variable. \cite{ElementsofInformationTheory}

\subsection{Codes}

A source characterized by a random process generates symbols from a specific alphabet at each time step. The objective is to transform this output sequence into a more concise representation. This data reduction technique, known as \emph{source coding} or \emph{data compression}, utilizes a code to represent the original symbols more efficiently. The device that performs this transformation is termed an \emph{encoder}, and the process itself is referred to as \emph{encoding}. \cite{han2002mathematics}

\begin{definition}[Source Code]\label{def:code}
    A source code for a random variable $X$ is a mapping from the set of possible outcomes of $X$, called $\mathcal{X}$, to $\mathcal{D}^*$, the set of all finite-length strings of symbols from a $\mathcal{D}$-ary alphabet. Let $C(X)$ denote the codeword assigned to $x$ and let $l(x)$ denote length of $C(x)$
\end{definition}

\begin{definition}[Expected length]\label{def:expected_length}
    The expected length $L(C)$ of a source code $C$ for a random variable $X$ with probability mass function $P_X(x)$ is defined as
    \begin{equation}
        L(C) = \sum_{x\in\mathcal{X}} P_X(x)l(x)
    \end{equation}
    where $l(x)$ is the length of the codeword assigned to $x$.
\end{definition}

\noindent Let's assume from now for simplicity that the $\mathcal{D}$-ary alphabet is $\mathcal{D} = \{0, 1, \ldots, D-1\}$.

\begin{example}\label{ex:source_code}
    Let's consider a source code for a random variable $X$ with $\mathcal{X} = \{a, b, c, d\}$ and $P_X(a) = 0.5$, $P_X(b) = 0.25$, $P_X(c) = 0.125$ and $P_X(d) = 0.125$. The code is defined as
    \begin{align*}
        C(a) & = 0   \\
        C(b) & = 10  \\
        C(c) & = 110 \\
        C(d) & = 111
    \end{align*}
    The entropy of $X$ is
    \begin{equation*}
        H(X) = 0.5\log 2 + 0.25\log 4 + 0.125\log 8 + 0.125\log 8 = 1.75 \text{ bits}
    \end{equation*}
    The expected length of this code is also $1.75$:
    \begin{equation*}
        L(C) = 0.5 \cdot 1 + 0.25 \cdot 2 + 0.125 \cdot 3 + 0.125 \cdot 3 = 1.75 \text{ bits}
    \end{equation*}
    In this example we have seen a code that is optimal in the sense that the expected length of the code is equal to the entropy of the random variable.
\end{example}

\begin{example}[Morse Code]\label{ex:morse_code}
    TODO from \cite{ElementsofInformationTheory}
\end{example}

\begin{definition}[Nonsingular Code]\label{def:nonsingular_code}
    A code is nonsingular if every element of the range of $X$ maps to a different element of $\mathcal{D}^*$. Thus:
    \begin{equation}
        x \neq y \Rightarrow C(x) \neq C(y)
    \end{equation}
\end{definition}

\noindent While a single unique code can represent a single value from our source $X$ without ambiguity, our real goal is often to transmit sequences of these values. In such scenarios, we could ensure the receiver can decode the sequence by inserting a special symbol, like a "comma," between each codeword. However, this approach wastes the special symbol's potential. To overcome this inefficiency, especially when dealing with sequences of symbols from $X$, we can leverage the concept of self-punctuating or instantaneous codes. These codes possess a special property: the structure of the code itself inherently indicates the end of each codeword, eliminating the need for a separate punctuation symbol. The following definitions formalize this concept. \cite{ElementsofInformationTheory}

\begin{definition}[Extension of a Code]\label{def:extension_code}
    The extension $C^*$ of a code $C$ is the mapping from finite-length sequences of symbols from $\mathcal{X}$ to finite-length strings of symbols from the $\mathcal{D}$-ary alphabet defined by
    \begin{equation}
        C^*(x_1x_2\ldots x_n) = C(x_1)C(x_2)\ldots C(x_n)
    \end{equation}
    where $C(x_1)C(x_2)\ldots C(x_n)$ denotes the concatenation of the codewords assigned to $x_1, x_2, \ldots, x_n$.
\end{definition}

\begin{example}
    If $C(x_1) = 0$ and $C(x_2) = 110$, then $C^*(x_1x_2) = 0110$.
\end{example}

\begin{definition}[Unique Decodability]\label{def:unique_decodability}
    A code $C$ is uniquely decodable if its extension is nonsingular
\end{definition}

\noindent Thus, any encoded string in a uniquely decodable code has only one possibile source string that could have generated it.

\begin{definition}[Prefix Code]\label{def:prefix_code}
    A \marginpar{Also called instantaneous code} code is a prefix code if no codeword is a prefix of any other codeword.
\end{definition}

\noindent Imagine receiving a string of coded symbols. An \emph{instantaneous code} allows us to decode each symbol as soon as we reach the end of its corresponding codeword. We don't need to wait and see what comes next. Because the code itself tells us where each codeword ends, it's like the code "punctuates itself" with invisible commas separating the symbols.  This let us decode the entire message by simply reading the string and adding commas between the codewords without needing to see any further symbols. Consider the example \ref{ex:source_code} seen a the beginning of this section, where the binary string \texttt{01011111010} is decoded as \texttt{0,10,111,110,10} because the code used naturally separates the symbols. \cite{ElementsofInformationTheory}. Figure \ref{fig:codes} shows the relationship between different types of codes.

% \begin{remark}[On the lower bound provided by Shannon Entropy]
%     Shannon entropy, defined in \ref{def:entropy}, gives us a lower bound for the average length of a uniquely decodable code. Given a letter with frequency $p_i$, it carries $-\log p_i$  bits of information.
% \end{remark}

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[scale=2.2,transform shape]
        \node[set,fill=gray!10,text width=4cm] (all) at (0,-0.6)  {};
        \node[font=\fontsize{4pt}{5pt}\selectfont] at (0,-2.3) {All codes};
        \node[set,fill=blue!20,text width=3cm] (rea) at (0,-0.4)  {};
        \node[font=\fontsize{4pt}{5pt}\selectfont] at (0,-1.5) {Nonsingular codes};
        \node[set,fill=red!20,text width=2cm] (int) at (0,-0.2)  {};
        \node[font=\fontsize{4pt}{5pt}\selectfont] at (0,-0.65) {Uniquely decodable codes};
        \node[set,fill=olive!10,text width=1cm,font=\fontsize{4pt}{5pt}\selectfont] (nat) at (0,0) {Instantaneous codes};
    \end{tikzpicture}
    \caption{Relationship between different types of codes \label{fig:codes}}
\end{figure}

\subsection{Kraft's Inequality}

We would like to construct instantaneous codes that are optimal in the sense that the expected length of the code is equal to the entropy of the random variable. However, we can't assign short codewords to all symbols and hope to be still prefix-free. Kraft's inequality provides a necessary and sufficient condition for the existence of a prefix code with given codeword lengths.
\vspace{0.4cm}

\noindent Let's denote the size of the source and code alphabets with $J = |\mathcal{X}|$ and $K = |\mathcal{D}|$, respectively. Different proofs of the following theorem can be found in \cite{ElementsofInformationTheory,han2002mathematics}, here we report the one from \cite{han2002mathematics}, however the one proposed in \cite{ElementsofInformationTheory} is also very interesting, based on the concept of a source tree.

\begin{theorem}[Kraft's Inequality]\label{thm:kraft_inequality}
    The codeword length $l(x)$, $x \in \mathcal{X}$, of any separable code $C$ must satisfy the inequality
    \begin{equation}\label{eq:kraft_inequality}
        \sum_{x\in\mathcal{X}} K^{-l(x)} \leq 1
    \end{equation}
\end{theorem}
\begin{proof}
    Consider the left hand side of the inequality \ref{eq:kraft_inequality} and consider its $n$-th power
    \begin{align}
        \left( \sum_{x\in\mathcal{X}} K^{-l(x)} \right)^n & = \sum_{x_1\in\mathcal{X}} \sum_{x_2\in\mathcal{X}} \ldots \sum_{x_n\in\mathcal{X}} K^{-l(x_1)} K^{-l(x_2)} \ldots K^{-l(x_n)} \nonumber \\
                                                          & = \sum_{x^n \in\mathcal{X^n}} K^{-l(x^n)}
    \end{align}
    Where $l(x^n) = l(x_1) + l(x_2) + \ldots + l(x_n)$ is the length of the concatenation of the codewords assigned to $x_1, x_2, \ldots, x_n$. If we consider the all the extended codewords of length $m$ we have
    \begin{equation}
        \sum_{x^n \in\mathcal{X^n}} K^{-l(x^n)} = \sum_{m=1}^{n l_max} A(m) K^{-m}
    \end{equation}
    where $A(m)$ is the number source sequences of length $n$ whose codewords have length $m$ and $l_{max}$ is the maximum length of the codewords in the code. Since the code is separable, we have that $A(m) \leq K^m$ and therefore each term of the sum is less than or equal to $1$. Hence
    \begin{equation}
        \left( \sum_{x\in\mathcal{X}} K^{-l(x)} \right)^n \leq n l_{max}
    \end{equation}
    That is
    \begin{equation}
        \sum_{x\in\mathcal{X}} K^{-l(x)} \leq (n l_{max})^{1/n}
    \end{equation}
    Taking the limit as $n$ goes to infinity and using the fact that $(n l_{max})^{1/n} = e^{1/n \log (n l_{max})} \to 1$ we have that
    \begin{equation}
        \sum_{x\in\mathcal{X}} K^{-l(x)} \leq 1
    \end{equation}
    That concludes the proof.
\end{proof}

\subsection{Source Coding Theorem}

Some introduction from \cite{ElementsofInformationTheory,Shannon1948,KolmogorovComplexity,han2002mathematics}

\begin{theorem}[Source Coding Theorem]\label{thm:source_coding_theorem}
    TODO from \cite{ElementsofInformationTheory,han2002mathematics}
\end{theorem}
\begin{proof}
    TODO from \cite{ElementsofInformationTheory,han2002mathematics}
\end{proof}







\clearpage
