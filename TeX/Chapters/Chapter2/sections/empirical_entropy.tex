
\section{Empirical Entropy}
Before digging into the concept of empirical entropy, let's begin with the notion of binary entropy. Consider an alphabet $\mathcal{U}$, where $\mathcal{U} = \{0, 1\}$. Let's assume it emits symbols with probabilities $p_0$ and $p_1 = 1 - p_0$. The entropy of this source can be calculated using the formula:
\[
    H(p_0) = -p_0 \log_2 p_0 - (1 - p_0) \log_2 (1 - p_0)
\]
We can extend this concept to scenarios where the elements are no longer individual bits, but sequences of these bits emitted by the source. Initially, let's assume the source is \emph{memoryless} (or \emph{zero-order}), meaning the probability of emitting a symbol doesn't depend on previously emitted symbols. In this case, we can consider chunks of $n$ bits as our elements. Our alphabet becomes $\Sigma = \{0, 1\}^n$, and the Shannon Entropy of two independent symbols $x, y \in \Sigma$ will be the sum of their entropies. Thus, if the source emits symbols from an alphabet $\Sigma = [0,\sigma]$ where each symbol has a probability $p_s$, the entropy of the source becomes:

\[
    H(p_1, \ldots, p_{\sigma}) = - \sum_{s=1}^{\sigma} p_s \log p_s = \sum_{s=1}^{\sigma} p_s \log \frac{1}{p_s}
\]

\begin{remark}
    If all symbols have a probability of $p_s =1$, then the entropy is $0$, and all other probabilities are $0$. If all symbols have the same probability $\frac{1}{\sigma}$, then the entropy is $\log \sigma$. So given a sequence of $n$ elements from an alphabet $\Sigma$, belonging to $\mathcal{U} = \Sigma^n$, its entropy is straightforwardly $n \mathcal{H}(p_1, \ldots, p_{\sigma})$
\end{remark}

% \noindent This approach however relies on the assumption that individual sequences originate from a single source. Let's consider relaxing this assumption and explore how Shannon Entropy can be used to define an entropy concept for text.

\subsection{Bit Sequences}
Let's consider a bit sequence, $B[1, n]$, which we aim to compress without access to an explicit model of a known bit source. Instead, we only have access to $B$. Although lacking a precise model, we may reasonably anticipate that B exhibits a bias towards either more $0s$ or more $1s$. Hence, we might attempt to compress $B$ based on this characteristic. Specifically, we say that $B$ is generated by a zero-order source emitting $0s$ and $1s$. Assuming $m$ represents the count of $1s$ in $B$, it's reasonable to posit that the source emits $1s$ with a probability of $p = m/n$. This leads us to the concept of zero-order empirical entropy:

\begin{definition}[Zero-order empirical entropy]
    Given a bit sequence $B[1, n]$ with $m$ $1s$ and $n-m$ $0s$, the zero-order empirical entropy of $B$ is defined as:
    \begin{equation}
        \mathcal{H}_0(B) = \mathcal{H} \left( \frac{m}{n} \right) =\frac{m}{n} \log \frac{n}{m} + \frac{n-m}{n} \log \frac{n}{n-m}
    \end{equation}
\end{definition}
\noindent The concept of zero-order empirical entropy carries significant weight: it indicates that if we attempt to compress $B$ using a fixed code $C(1)$ for $1s$ and $C(0)$ for $0s$, then it's impossible to compress $B$ to fewer than $\mathcal{H}_0(B)$ bits per symbol. Otherwise, we would have $m |C(1)| + (n-m) |C(0)| < n \mathcal{H}_0(B)$, which violates the lower bound established by Shannon entropy.

\paragraph{Connection with worst case entropy}
TBD if to add this paragraph, from \cite{navarro2016compact} 2.3.1

\subsection{Entropy of a Text}

The zero-order empirical entropy of a string $S[1, n]$, where each symbol $s$ occurs $n_s$ times in $S$, is similarly determined by the Shannon entropy of its observed probabilities:

\begin{definition}[Zero-order empirical entropy of a text]
    Given a text $S[1, n]$ with $n_s$ occurrences of symbol $s$, the zero-order empirical entropy of $S$ is defined as:
    \begin{equation}
        \mathcal{H}_0(S) = \mathcal{H} \left( \frac{n_1}{n} , \ldots, \frac{n_{\sigma}}{n} \right) =  \sum_{s=1}^{\sigma} \frac{n_s}{n} \log \frac{n}{n_s}
    \end{equation}
\end{definition}

\begin{example}\label{ex:0_order_entropy_abracadabra}
    Let $S = \text{"abracadabra"}$. We have that $n = 11$, $n_a = 5$, $n_b = 2$, $n_c = 1$, $n_d = 1$, $n_r = 2$. The zero-order empirical entropy of $S$ is:
    \[
        \mathcal{H}_0(S) = \frac{5}{11} \log \frac{11}{5} + 2 \cdot \frac{2}{11} \log \frac{11}{2} + 2 \cdot \frac{1}{11} \log \frac{11}{1} \approx 2.04
    \]
    Thus, we could expect to compress $S$ to $n H_0 (S) \approx 22.44$ bits, which is lower than the $n \log \sigma  = 11 \cdot \log 5 \approx 25.54$ bits of the worst-case entropy of a general string of length $n$ over an alphabet of size $\sigma = 5$.
\end{example}

\noindent However, this definition falls short because in most natural languages, symbol choices aren't independent. For example, in English text, the sequence "\emph{don'}" is almost always followed by "\emph{t}". Higher-order entropy (\autoref{sec:higher_order_entropy}) is a more accurate measure of the entropy of a text, as it considers the probability of a symbol given the preceding symbols
