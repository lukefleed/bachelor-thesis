\section{Empirical Entropy}
Before digging into the concept of empirical entropy, let's begin with the notion of binary entropy. Consider an alphabet $\mathcal{U}$, where $\mathcal{U} = \{0, 1\}$. Let's assume it emits symbols with probabilities $p_0$ and $p_1 = 1 - p_0$. The entropy of this source can be calculated using the formula:
\[
    H(p_0) = -p_0 \log_2 p_0 - (1 - p_0) \log_2 (1 - p_0)
\]
We can extend this concept to scenarios where the elements are no longer individual bits, but sequences of these bits emitted by the source. Initially, let's assume the source is \emph{memoryless} (or \emph{zero-order}), meaning the probability of emitting a symbol doesn't depend on previously emitted symbols. In this case, we can consider chunks of $n$ bits as our elements. Our alphabet becomes $\Sigma = \{0, 1\}^n$, and the Shannon Entropy of two independent symbols $x, y \in \Sigma$ will be the sum of their entropies. Thus, if the source emits symbols from a general alphabet $\Sigma$ of size $|\Sigma| = \sigma$, where each symbol $s \in \Sigma$ has a probability $p_s$ (with $\sum_{s \in \Sigma} p_s = 1$), the Shannon entropy of the source is given by:

\[
    H(P) = H(p_1, \ldots, p_{\sigma}) = - \sum_{s \in \Sigma} p_s \log p_s = \sum_{s \in \Sigma} p_s \log \frac{1}{p_s}
\]

\begin{remark}
    If all symbols have a probability of $p_s =1$, then the entropy is $0$, and all other probabilities are $0$. If all symbols have the same probability $\frac{1}{\sigma}$, then the entropy is $\log \sigma$. So given a sequence of $n$ elements from an alphabet $\Sigma$, belonging to $\mathcal{U} = \Sigma^n$, its entropy is straightforwardly $n \mathcal{H}(p_1, \ldots, p_{\sigma})$
\end{remark}

% \noindent This approach however relies on the assumption that individual sequences originate from a single source. Let's consider relaxing this assumption and explore how Shannon Entropy can be used to define an entropy concept for text.

\subsection{Bit Sequences}
In many practical scenarios, however, we do not know the true probabilities $p_s$ of the underlying source. Instead, we might only have access to a sequence generated by the source. The concept of empirical entropy allows us to estimate the information content based directly on the observed frequencies within that sequence. Let's first examine this for binary sequences.

Let's consider a bit sequence, $B[1, n]$, which we aim to compress without access to an explicit model of a known bit source. Instead, we only have access to $B$. Although lacking a precise model, we may reasonably anticipate that B exhibits a bias towards either more $0s$ or more $1s$. Hence, we might attempt to compress $B$ based on this characteristic. Specifically, we say that $B$ is generated by a zero-order source emitting $0s$ and $1s$. Assuming $m$ represents the count of $1s$ in $B$, it's reasonable to posit that the source emits $1s$ with a probability of $p = m/n$. This leads us to the concept of zero-order empirical entropy:

\begin{definition}[Zero-order empirical entropy]
    Given a bit sequence $B[1, n]$ with $m$ $1s$ and $n-m$ $0s$, the zero-order empirical entropy of $B$ is defined as:
    \begin{equation}
        \mathcal{H}_0(B) = \mathcal{H} \left( \frac{m}{n} \right) =\frac{m}{n} \log \frac{n}{m} + \frac{n-m}{n} \log \frac{n}{n-m}
    \end{equation}
\end{definition}
\noindent The concept of zero-order empirical entropy carries significant weight: it indicates that if we attempt to compress $B$ using a fixed code $C(1)$ for $1s$ and $C(0)$ for $0s$, then it's impossible to compress $B$ to fewer than $\mathcal{H}_0(B)$ bits per symbol. Otherwise, we would have $m |C(1)| + (n-m) |C(0)| < n \mathcal{H}_0(B)$, which violates the lower bound established by Shannon entropy.

\paragraph{Connection with worst case entropy}
It is interesting to note a connection between the zero-order empirical entropy $\mathcal{H}_0(B)$ and the worst-case entropy $H_{wc}$ previously introduced (\autoref{sec:worst_case_entropy}). Consider the specific set $\mathcal{B}_{n,m}$ comprising all possible binary sequences of length $n$ that contain exactly $m$ ones, like our sequence $B$. The worst-case entropy necessary to assign a unique identifier to each sequence \emph{within this set} is $H_{wc}(\mathcal{B}_{n,m}) = \log |\mathcal{B}_{n,m}| = \log \binom{n}{m}$. Using Stirling's approximation for the binomial coefficient, it can be demonstrated that this quantity is closely related to the total empirical entropy: $H_{wc}(\mathcal{B}_{n,m}) \approx n \mathcal{H}_0(B) - O(\log n)$. Thus, $n \mathcal{H}_0(B)$ approximates the minimum number of bits required, on average per sequence, to distinguish among all sequences sharing the same number of 0s and 1s, providing another perspective on the meaning of empirical entropy \cite{navarro2016compact}.

\subsection{Entropy of a Text}

The zero-order empirical entropy of a string $S[1, n]$, where each symbol $s$ occurs $n_s$ times in $S$, is similarly determined by the Shannon entropy of its observed probabilities:

\begin{definition}[Zero-order empirical entropy of a text]
    Given a text $S[1, n]$ with $n_s$ occurrences of symbol $s$, the zero-order empirical entropy of $S$ is defined as:
    \begin{equation}
        \mathcal{H}_0(S) = \mathcal{H} \left( \frac{n_1}{n} , \ldots, \frac{n_{\sigma}}{n} \right) =  \sum_{s=1}^{\sigma} \frac{n_s}{n} \log \frac{n}{n_s}
    \end{equation}
\end{definition}

\begin{example}\label{ex:0_order_entropy_abracadabra}
    Let $S = \text{"abracadabra"}$. We have that $n = 11$, $n_a = 5$, $n_b = 2$, $n_c = 1$, $n_d = 1$, $n_r = 2$. The zero-order empirical entropy of $S$ is:
    \[
        \mathcal{H}_0(S) = \frac{5}{11} \log \frac{11}{5} + 2 \cdot \frac{2}{11} \log \frac{11}{2} + 2 \cdot \frac{1}{11} \log \frac{11}{1} \approx 2.04
    \]
    Thus, we could expect to compress $S$ to $n H_0 (S) \approx 22.44$ bits, which is lower than the $n \log \sigma  = 11 \cdot \log 5 \approx 25.54$ bits of the worst-case entropy of a general string of length $n$ over an alphabet of size $\sigma = 5$.
\end{example}

\noindent However, this definition falls short because in most natural languages, symbol choices aren't independent. For example, in English text, the sequence "\emph{don'}" is almost always followed by "\emph{t}". Higher-order entropy (\autoref{sec:higher_order_entropy}) is a more accurate measure of the entropy of a text, as it considers the probability of a symbol given the preceding symbols. This principle was at the base of the development of the famous Morse Code and then the Huffman code (\autoref{sec:statistical_coding}).
