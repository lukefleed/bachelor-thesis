\section{Empirical Entropy}
The concept of empirical entropy builds upon the foundational notion of Shannon entropy. For a binary source emitting symbols from the alphabet $\mathcal{U} = \{0, 1\}$ with respective probabilities $p_0$ and $p_1 = 1 - p_0$, the Shannon entropy is defined as:
\[
    H(p_0) = -p_0 \log_2 p_0 - (1 - p_0) \log_2 (1 - p_0)
\]
This definition can be extended to sequences generated by such sources. Consider first a memoryless (or zero-order) source, where the probability of emitting a symbol is independent of previously emitted symbols. For such a source generating sequences of length $n$, the alphabet of possible sequences is $\Sigma^n$, where $\Sigma$ is the alphabet of individual symbols. If the source emits symbols from a general alphabet $\Sigma$ of size $|\Sigma| = \sigma$, with each symbol $s \in \Sigma$ having a probability $p_s$ (such that $\sum_{s \in \Sigma} p_s = 1$), the Shannon entropy of the source is given by:
\[
    H(P) = H(p_1, \ldots, p_{\sigma}) = - \sum_{s \in \Sigma} p_s \log p_s = \sum_{s \in \Sigma} p_s \log \frac{1}{p_s}
\]
For a memoryless source, the entropy of a sequence of length $n$ is $n H(P)$.

\begin{remark}
    If a single symbol $s_i$ occurs with probability $p_{s_i}=1$ (implying all other symbols have probability 0), then the entropy $H(P) = 0$. Conversely, if all symbols have the same probability $p_s = \frac{1}{\sigma}$, the entropy reaches its maximum value, $H(P) = \log \sigma$. Consequently, for a sequence of $n$ symbols drawn independently from this uniform distribution, the total entropy is $n \log \sigma$.
\end{remark}

\subsection{Bit Sequences}
In practice, the true probabilities $p_s$ governing the source are often unknown. Often, observation is limited to a single sequence generated by the source. Empirical entropy provides a method to estimate the information content based directly on the observed frequencies within that sequence. This concept is first examined for binary sequences.

Consider a binary sequence $B[1, n]$, for which compression is sought without access to an explicit model of the source. Instead, only the sequence $B$ itself is available. Without a source model, it can be hypothesised that $B$ may exhibit a statistical bias (e.g., towards more $0$s or more $1$s). Consequently, compression can be attempted based on this observed characteristic. Specifically, $B$ can be modelled as the output of a zero-order source. If $m$ denotes the count of $1$s in $B$, it is postulated that the source emits $1$s with probability $p = m/n$. This motivates the definition of zero-order empirical entropy:

\begin{definition}[Zero-order empirical entropy]
    Given a binary sequence $B[1, n]$ containing $m$ occurrences of $1$ and $n-m$ occurrences of $0$, the zero-order empirical entropy of $B$ is defined as:
    \begin{equation*}
        \mathcal{H}_0(B) = \mathcal{H} \left( \frac{m}{n} \right) = \frac{m}{n} \log \frac{n}{m} + \frac{n-m}{n} \log \frac{n}{n-m}
    \end{equation*}
    where logarithms are typically base 2 for information content measured in bits.
\end{definition}
The zero-order empirical entropy establishes a lower bound: if compression of $B$ is attempted using a fixed codeword $C(1)$ for $1$s and $C(0)$ for $0$s, it is impossible to compress $B$ to fewer than $n \mathcal{H}_0(B)$ total bits. Achieving a compressed length $m |C(1)| + (n-m) |C(0)| < n \mathcal{H}_0(B)$ would contradict the source coding theorem derived from Shannon entropy.

\paragraph{Connection with worst case entropy}
A connection exists between the zero-order empirical entropy $\mathcal{H}_0(B)$ and the worst-case entropy $H_{wc}$ (\autoref{sec:worst_case_entropy}). Define the set $\mathcal{B}_{n,m}$ as the collection of all binary sequences of length $n$ containing exactly $m$ ones. The sequence $B$ belongs to this set. The number of bits required to assign a unique identifier to each sequence \emph{within this specific set} $\mathcal{B}_{n,m}$, which corresponds to the worst-case entropy if selection within the set is uniform, is $H_{wc}(\mathcal{B}_{n,m}) = \log |\mathcal{B}_{n,m}| = \log \binom{n}{m}$. Using Stirling's approximation for the binomial coefficient, it can be shown that this quantity relates to the total empirical entropy $n \mathcal{H}_0(B)$:
\[ \log \binom{n}{m} \approx n \mathcal{H}_0(B) - O(\log n) \]
Thus, the total empirical entropy $n \mathcal{H}_0(B)$ approximates the number of bits required to uniquely identify a sequence within the set $\mathcal{B}_{n,m}$, offering an interpretation of empirical entropy related to the combinatorial complexity of sequences with a fixed composition \cite{navarro2016compact}.

\subsection{Entropy of a Text}

Analogously, for a text $S[1, n]$ drawn from an alphabet $\Sigma = \{s_1, \dots, s_\sigma\}$, where each symbol $s \in \Sigma$ occurs $n_s$ times in $S$ (such that $\sum_{s \in \Sigma} n_s = n$), the zero-order empirical entropy is defined based on the observed relative frequencies:

\begin{definition}[Zero-order empirical entropy of a text]
    Given a text $S[1, n]$ over alphabet $\Sigma$, where symbol $s$ appears $n_s$ times, the zero-order empirical entropy of $S$ is:
    \begin{equation*}
        \mathcal{H}_0(S) = \mathcal{H} \left( \frac{n_{s_1}}{n} , \ldots, \frac{n_{s_\sigma}}{n} \right) =  \sum_{s \in \Sigma} \frac{n_s}{n} \log \frac{n}{n_s}
    \end{equation*}
\end{definition}

\begin{example}\label{ex:0_order_entropy_abracadabra}
    Consider $S = \text{"abracadabra"}$. The length is $n = 11$. The symbol counts are $n_a = 5$, $n_b = 2$, $n_c = 1$, $n_d = 1$, $n_r = 2$. The alphabet size is $\sigma=5$. The zero-order empirical entropy of $S$ is:
    \begin{align*}
        \mathcal{H}_0(S) = {} & \frac{5}{11} \log_2 \frac{11}{5} + \frac{2}{11} \log_2 \frac{11}{2} + \frac{1}{11} \log_2 \frac{11}{1} \\
                              & + \frac{1}{11} \log_2 \frac{11}{1} + \frac{2}{11} \log_2 \frac{11}{2} \approx 2.04 \text{ bits/symbol}
    \end{align*}
    This suggests a theoretical lower bound for compression based on symbol frequencies of $n \mathcal{H}_0(S) \approx 11 \times 2.04 \approx 22.44$ bits for the entire sequence. This is lower than the $n \log \sigma = 11 \log_2 5 \approx 25.54$ bits corresponding to the uniform distribution over the alphabet.
\end{example}

This zero-order definition has limitations, particularly for sources like natural language text where symbol occurrences often exhibit dependencies. For instance, in English, the character \emph{q} is almost invariably followed by \emph{u}. Higher-order entropy models (\autoref{sec:higher_order_entropy}), which account for the conditional probability of a symbol given its preceding context, provide a more refined measure of the information content for such sources. This principle of exploiting context underlies compression techniques such as Huffman coding (\autoref{sec:statistical_coding}); however, Huffman coding itself is typically applied in a zero-order fashion unless specifically adapted for context.
