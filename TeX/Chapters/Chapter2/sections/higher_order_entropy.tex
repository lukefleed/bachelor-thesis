\section{Higher Order Entropy} \label{sec:higher_order_entropy}

\noindent The zero-order empirical entropy $\mathcal{H}_0(S)$, discussed in the previous section, provides a useful baseline for compression by considering the frequency of individual symbols. However, it operates under the implicit assumption that symbols are generated independently, a condition seldom met in practice, especially for data like natural language text. For instance, the probability of encountering the letter 'u' in English text dramatically increases if the preceding letter is 'q'. To capture such dependencies and obtain a more accurate measure of the information content considering local context, we introduce the concept of \emph{higher-order empirical entropy}. This approach conditions the probability of a symbol's occurrence on the sequence of $k$ symbols that immediately precede it.

\begin{definition}[Redundancy] \label{def:redundancy}
    For an information source $X$ generating symbols from an alphabet $\Sigma$, the redundancy $R$ is the difference between the maximum possible entropy per symbol and the actual entropy $H(X)$ of the source:
    \begin{equation}
        R = \log_2 |\Sigma| - H(X)
    \end{equation}
    % Note: Added base 2 explicitly, assuming bits.
\end{definition}

\noindent This redundancy value, $R$, quantifies the degree of predictability or statistical structure inherent in the source. A high redundancy signifies that the source is far from random, exhibiting patterns (like non-uniform symbol probabilities or inter-symbol dependencies) that can potentially be exploited for compression. Conversely, a source with low redundancy behaves more randomly, leaving less room for compression beyond the theoretical minimum dictated by $H(X)$.

\noindent However, evaluating redundancy directly using Definition \ref{def:redundancy} often proves impractical, as determining the true source entropy $H(X)$ for the process generating a given string $S$ is typically unfeasible. This limitation necessitates alternative, empirical approaches. To address this issue, we introduce the concept of the \emph{k-th order empirical entropy} of a string $S$, denoted as $\mathcal{H}_k(S)$. In statistical coding (\autoref{sec:statistical_coding}), we will see a scenario where $k=0$, relying on symbol frequencies within the string. Now, with $\mathcal{H}_k(S)$, our objective is to extend the entropy concept by examining the frequencies of $k$-grams in string $S$. This requires analyzing subsequences of symbols with a length of $k$, thereby capturing the \emph{compositional structure} of $S$ \cite{ferragina2023pearls}.

\noindent Let $S$ be a string of length $n = |S|$ over an alphabet $\Sigma$ of size $|\Sigma| = \sigma$. Let $\omega$ denote a $k$-gram (a sequence of $k$ symbols from $\Sigma$), and let $n_\omega$ be the number of occurrences of $\omega$ in $S$. Let $n_{\omega \sigma_i}$ be the number of times the $k$-gram $\omega$ is followed by the symbol $\sigma_i \in \Sigma$ in $S$. \footnote{We use the notation $\omega \in \Sigma^k$ for a $k$-gram.}

\begin{definition}[k-th Order Empirical Entropy] \label{def:kth_order_empirical_entropy}
    The \emph{k-th order empirical entropy} of a string $S$ is defined as:
    \begin{equation}\label{eq:kth_order_entropy_formula1}
        \mathcal{H}_k(S) = \frac{1}{n} \sum_{\omega \in \Sigma^k} \left ( \sum_{\sigma_i \in \Sigma} n_{\omega\sigma_i} \log_2 \left ( \frac{n_\omega}{n_{\omega\sigma_i}} \right) \right )
        % Corrected summation limit and added base 2
        % Note: Ensure n_{\omega \sigma_i} is 0 if the combination never occurs, and handle log(0) appropriately (e.g., 0 log(inf) = 0 convention).
    \end{equation}
    where terms with $n_{\omega\sigma_i} = 0$ contribute zero to the sum.
\end{definition}

\noindent This definition calculates the average conditional entropy based on the preceding $k$ symbols. An equivalent and often more intuitive way to express this is by averaging the zero-order empirical entropies of the sequences formed by the symbols following each distinct $k$-gram context:
\begin{equation}\label{eq:kth_order_entropy_formula2}
    \mathcal{H}_k(S) = \sum_{\omega \in \Sigma^k, n_\omega > 0} \frac{n_\omega}{n} \cdot \mathcal{H}_0(S_\omega)
\end{equation}
where $S_\omega$ is the string formed by concatenating all symbols that immediately follow an occurrence of the $k$-gram $\omega$ in $S$ (its length is $|S_\omega| = n_\omega$). The sum is taken over all $k$-grams $\omega$ that actually appear in $S$ (i.e., $n_\omega > 0$).

\begin{example}
    Consider the example \ref{ex:0_order_entropy_abracadabra}, where $S = \text{"abracadabra"}$ ($n=11$) and $\Sigma = \{a, b, c, d, r\}$ ($\sigma=5$). The zero-order empirical entropy is $\mathcal{H}_0(S) \approx 2.04$. Now, let's calculate the first-order ($k=1$) empirical entropy using Equation \ref{eq:kth_order_entropy_formula2}. The contexts are the single characters:
    \begin{itemize}[noitemsep, topsep=0pt] % Reduced spacing for the list
        \item Context 'a' ($n_a=5$): Following symbols are 'b', 'c', 'd', 'b', '\$' (assuming end-of-string marker). $S_a = \text{"bcdb\$"}$. $\mathcal{H}_0(S_a) \approx 1.922$ bits/symbol (assuming $\$$ is a unique symbol).
        \item Context 'b' ($n_b=2$): Following symbols are 'r', 'r'. $S_b = \text{"rr"}$. $\mathcal{H}_0(S_b) = 0$ bits/symbol.
        \item Context 'c' ($n_c=1$): Following symbol is 'a'. $S_c = \text{"a"}$. $\mathcal{H}_0(S_c) = 0$ bits/symbol.
        \item Context 'd' ($n_d=1$): Following symbol is 'a'. $S_d = \text{"a"}$. $\mathcal{H}_0(S_d) = 0$ bits/symbol.
        \item Context 'r' ($n_r=2$): Following symbols are 'a', 'a'. $S_r = \text{"aa"}$. $\mathcal{H}_0(S_r) = 0$ bits/symbol.
    \end{itemize}
    Therefore, the first-order empirical entropy of $S$ is:
    \[
        \mathcal{H}_1(S) = \frac{n_a}{n}\mathcal{H}_0(S_a) + \frac{n_b}{n}\mathcal{H}_0(S_b) + \frac{n_c}{n}\mathcal{H}_0(S_c) + \frac{n_d}{n}\mathcal{H}_0(S_d) + \frac{n_r}{n}\mathcal{H}_0(S_r)
    \]
    \[
        \mathcal{H}_1(S) = \frac{5}{11} \cdot (1.922) + \frac{2}{11} \cdot 0 + \frac{1}{11} \cdot 0 + \frac{1}{11} \cdot 0 + \frac{2}{11} \cdot 0 \approx 0.874 \text{ bits/symbol}
    \]
    This value is significantly lower than the zero-order empirical entropy $\mathcal{H}_0(S)$, reflecting the predictability introduced by considering the preceding character.
    % Note: Corrected H_0 to mathcal{H}_0 for consistency. Added bits/symbol unit. Added calculation details.
\end{example}

\noindent The quantity $n \mathcal{H}_k(S)$ serves as a lower bound for the minimum number of bits attainable by any encoding of $S$, under the condition that the encoding of each symbol may rely only on the $k$ symbols preceding it in $S$. Consistently, any compressor achieving fewer than $n \mathcal{H}_k(S)$ bits would imply the ability to compress symbols originating from the related $k$-th order Markov source to a level below its Shannon entropy.

\begin{remark}
    As $k$ grows large (up to $k=n-1$, and often sooner), the $k$-th order empirical entropy $\mathcal{H}_k(S)$ tends towards zero, given that most long $k$-grams appear only once, making their subsequent symbol perfectly predictable within the sequence $S$. This renders the model ineffective as a lower bound for practical compressors when $k$ is very large relative to $n$. Even before reaching $\mathcal{H}_k(S)=0$, achieving compression close to $n\mathcal{H}_k(S)$ bits becomes practically challenging for high $k$ values. This is due to the necessity of storing or implicitly representing the conditional probabilities (or equivalent coding information) for all $\sigma^k$ possible contexts, which requires significant space overhead ($\approx \sigma^{k+1} \log n$ bits in simple models). In theory, it is commonly assumed that $S$ can be compressed up to $n \mathcal{H}_k(S) + o(n)$ bits for any $k$ such that $k+1 \leq \alpha \log_\sigma n$ for some constant $0 < \alpha < 1$. Under this condition, the overhead for storing the model ($\sigma^{k+1} \log n \leq n^\alpha \log n$) becomes asymptotically negligible compared to the compressed data size ($o(n)$ bits) \cite{navarro2016compact}.
    % Corrected log base and notation consistency. Clarified the overhead concept.
\end{remark}

\begin{definition}[Coarsely Optimal Compression Algorithm] \label{def:coarsely_optimal_compression_algorithm}
    A compression algorithm is \emph{coarsely optimal} if, for every fixed value of $k \ge 0$, there exists a function $f_k(n)$ such that $\lim_{n\to\infty} f_k(n) = 0$, and for all sequences $S$ of length $n$, the compression size achieved by the algorithm is bounded by $n (\mathcal{H}_k(S) + f_k(n))$ bits.
    % Made definition slightly more formal.
\end{definition}

\noindent The \emph{Lempel-Ziv} algorithm family, particularly LZ78, serves as a prominent example of coarsely optimal compression techniques, as demonstrated by Plotnik et al. \cite{plotnik1992upper}. These algorithms typically rely on dictionary-based compression. However, as highlighted by Kosaraju and Manzini \cite{kosaraju2000compression}, the notion of coarse optimality does not inherently guarantee practical effectiveness across all scenarios. The additive term $n \cdot f_k(n)$ might still lead to poor performance on some sequences, especially if $f_k(n)$ converges slowly or if the sequence length $n$ is not sufficiently large for the asymptotic behavior to dominate.
% Corrected cite command, added Kosaraju's first initial. Clarified the limitation.

% \newpage
% \paragraph{Further Comments on LZ77 and LZ78} \todo[inline, color=blue!30]{TBD if to include this section, but I think it's not relevant for the thesis. If included, it should discuss very briefly the LZ77 and LZ78 algorithms, and the differences between them. \cite{ferragina2023pearls}. And then prove two lemmas: one about the compression ration achieved by LZ78 and the other about LZ77 not being coarsely optimal. \cite{ferragina2023pearls}, end of chapter 13}
