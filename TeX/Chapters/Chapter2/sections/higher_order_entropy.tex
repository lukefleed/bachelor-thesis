\clearpage
\section{Higher Order Entropy}

TODO: A bit of introduction
\begin{definition}[Redundacy] \label{def:redundancy}
    TODO: Give a formal definition of redundacy: informally is a measure of the distance between the source's entropy and the compression ration, and can thereby be seen as a measure of how fast the algorithm reaches the entropy of the source.
\end{definition}

% All these measures are very interesting, but unrealistic because it's quite unusual, if not impossibile, to know the entropy of the source that generate the stringe we are going to compress. In order to circumvent this problem, a different empirical approach has been taken by introducing the notion of \emph{k-th order empirical entropy} of a string $S$, denoted by $\mathcal{H}_k(S)$. In \ref{sec:statistical_coding} we have discussed the case where $k=0$, which depends on the frequency of the symbols in the string. Here, we wish with $\mathcal{H}_k(S)$ to empower the entropy definition by considering the frequencies ok $k$-grams in the string $S$, thus taking into account subsequences of symbols of length $k$, hence the \emph{compositional structure} of $S$

\noindent While using measures like \ref{def:redundancy} are certainly intriguing, their feasibility is questionable due to the inherent challenge of determining the entropy of the source generating the string we aim to compress. To address this issue, an alternative empirical approach has been devised, which introduces the concept of the \emph{k-th order empirical entropy} of a string $S$, denoted as $\mathcal{H}_k(S)$. In the preceding discussion on statistical coding (refer to section \ref{sec:statistical_coding}), we delved into the scenario where $k=0$, relying on symbol frequencies within the string. Now, with $\mathcal{H}_k(S)$, our objective is to enrich the entropy concept by examining the frequencies of $k$-grams in string $S$. This entails analyzing subsequences of symbols with a length of $k$, thereby capturing the inherent \emph{compositional structure} of $S$. \cite{ferragina2023pearls} \vspace{0.4cm}

\noindent Let $S$ be a string over the alphabet $\Sigma=\{\sigma_1, \dots, \sigma_n\}$. Denote with $n_\omega$ the number of occurrences of the $k$-gram $\omega$ in $S$. \footnote{We will use the notation $\omega \in \Sigma^k$ to denote a $k$-gram, i.e., a subsequence of $k$ symbols in the string $S$.}

\begin{definition}[k-th Order Empirical Entropy] \label{def:kth_order_empirical_entropy}
    The \emph{k-th order empirical entropy} of a string $S$ is defined as
    \begin{equation}
        \mathcal{H}_k(S) = \frac{1}{|S|} \sum_{\omega \in \Sigma^k} \left ( \sum_{i=1}^h n_{\omega\sigma_i} \log \left ( \frac{n_\omega}{n_{\omega\sigma_i}} \right) \right )
    \end{equation}
    where $|S|$ is the length of the string $S$.
\end{definition}

When considering a concrete sequence $S[1,n]$ we can compute the \emph{empirical k-th entropy} of $S$ by considering the frequencies of symbols depending on the $k$ preceding symbols.
\begin{equation}
    \mathcal{H}_k(S) = \sum_{\omega \in \Sigma^k} \frac{|S_\omega|}{n} \cdot \mathcal{H_0}(S_\omega)
\end{equation}
where $S_\omega$ is a string formed by collecting the symbol that follows each occurrence of the $k$-gram $\omega = \sigma_1 \dots \sigma_k$ in $S$.

\begin{example}
    TODO: add here a numerical example where the first order empirical entropy is much less then the zero order empirical entropy.
\end{example}

\noindent Extending the concept of zero-order empirical entropy, $n \mathcal{H}_k(S)$ serves as a lower bound for the minimum number of bits attainable by any encoding of $S$, under the condition that the encoding of each symbol may rely on itself and the $k$ symbols preceding it in $S$. Consistently, any compressor that surpasses this threshold would also have the capability to compress symbols originating from the related kth-order source to a level lower than its Shannon entropy.

\begin{remark}
    % For large values of $k$ (at most $k=n-1$), and usually sooner) the $k$-th order empirical entropy of $S$ is null since all the $k$-grams appear only once. If we arrive at this point, our model becomes useless as a lower bound for compressors. Even before reaching the value $k$ for which $\mathcal_k(S)=0$, compressors cannot achieve $n\mathcal_k(S)$ bits in practice for very high $k$ values, because they must store the set of $\sigma^{k+1}$ probabilities, or equivalently, the set of $\sigma^{k+1}$ codes\footnote{Similarly, adaptive compressor must record $\sigma^{k+1}$, escape symbols somewhere along the compressed file}, so that the decompressor can reconstruct $S$. In theory, it is common to assume that $S$ can be compressed up to $n \mathcal{H}_k(S) + o(n)$ bits for any $k+1 \leq \alpha log_\sigma n$ and any constant $0 < \alpha < 1$, because in this case one can store $\sigma^{k+1}$ numbers in $[1,n]$ (such as the frequencies of the $k$-grams) within $\sigma^{k+1} \log n \leq n^\alpha log n = o(n)$ bits. \cite{navarro2016compact}
    As $k$ grows large (up to $k=n-1$, and often sooner), the $k$-th order empirical entropy of $S$ reaches null, given that each $k$-gram appears only once. This renders our model ineffective as a lower bound for compressors. Even before reaching the $k$ value where $\mathcal{H}_k(S)=0$, compressors face practical hurdles in achieving the target of $n\mathcal{H}_k(S)$ bits, particularly for high $k$ values. This is due to the necessity of storing the set of $\sigma^{k+1}$ probabilities or codes, adding complexity to compression. Likewise, adaptive compressors must incorporate $\sigma^{k+1}$ escape symbols into the compressed file, further complicating the process. In theory, it is commonly posited that $S$ can be compressed up to $n \mathcal{H}k(S) + o(n)$ bits for any $k+1 \leq \alpha \log\sigma n$ and any constant $0 < \alpha < 1$. In such cases, storing $\sigma^{k+1}$ numbers within the range $[1,n]$ (such as the frequencies of the $k$-grams) requires $\sigma^{k+1} \log n \leq n^\alpha \log n = o(n)$ bits. \cite{navarro2016compact}
\end{remark}

\begin{definition}[Coarsely Optimal Compression Algorithm] \label{def:coarsely_optimal_compression_algorithm}
    A compression algorithm is coarsely optimal if, for every value of $k$, there exists a function $f_k(n)$ that tends to zero as the length of the sequence $n$ approaches infinity, such that for all sequences $S$ of increasing length, the compression ratio achieved by the algorithm remains within $\mathcal{H}_k(S) + f_k(|S|)$.
\end{definition}

\noindent The \emph{Lempel-Ziv} algorithm (\texttt{LZ78}) serves as an example of a coarsely optimal compression technique, as outlined by Plotnik et al. in their paper referenced in \cite{plotnik1992upper}. This algorithm relies on the idea of dictionary-based compression. However, as highlighted by Manzini and Kora\v{r}aju \cite{kosaraju2000compression}, the notion of coarse optimality doesn't necessarily guarantee the effectiveness of an algorithm. Even when the entropy of the string is extremely low, the algorithm might still perform inadequately due to the presence of the supplementary term $f_k(|S|)$.

\paragraph{Further Comments on LZ77 and LZ78} TBD if to include this section, but I think it's not relevant for the thesis. If included, it should discuss very briefly the LZ77 and LZ78 algorithms, and the differences between them. \cite{ferragina2023pearls}. And then prove two lemmas: one about the compression ration achieved by LZ78 and the other about LZ77 not being coarsely optimal. \cite{ferragina2023pearls}, end of chapter 13.
