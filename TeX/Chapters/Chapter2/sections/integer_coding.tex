\clearpage
\section{Integer Coding} \label{sec:integer_coding}

\noindent This chapter examines methods for representing a sequence of positive integers, $S = \{x_1, x_2, \ldots, x_n\}$, potentially containing repetitions, as a compact sequence of bits \cite{ferragina2023pearls}. The primary objective is to minimize the total bits used. A key requirement is that the resulting binary sequence must be \emph{self-delimiting}: the concatenation of individual integer codes must be unambiguously decodable, allowing a decoder to identify the boundaries between consecutive codes.

\noindent The practical importance of efficient integer coding affects both storage space and processing speed in numerous applications. For example, \emph{search engines} maintain large indexes mapping terms to lists of document identifiers (IDs). These \emph{posting lists} can contain billions of integer IDs. Efficient storage is vital. A common approach involves sorting the IDs and encoding the differences (gaps) between consecutive IDs using variable-length integer codes, assigning shorter codes to smaller, more frequent gaps \cite{ferragina2023pearls, witten1999managing}. The engineering considerations for building practical data structures based on these principles, such as providing random access capabilities, are discussed in detail in \autoref{app:compressed_intvec_engineering}, which describes a library developed as part of this work.

\noindent Another significant application occurs in the final stage of many \emph{data compression algorithms}. Techniques such as LZ77, Move-to-Front (MTF), Run-Length Encoding (RLE), or Burrows-Wheeler Transform (BWT) often generate intermediate outputs as sequences of integers, where smaller values typically appear more frequently. An effective integer coding scheme is then needed to convert this intermediate sequence into a compact final bitstream \cite{ferragina2023pearls}. Similarly, compressing natural language text might involve mapping words or characters to integer token IDs and subsequently compressing the resulting ID sequence using integer codes \cite{ferragina2023pearls}.

\noindent This chapter explores techniques for designing such variable-length, prefix-free binary representations for integer sequences, aiming for maximum space efficiency.


\noindent The central concern in this section revolves around formulating an efficient binary representation method for an indefinite sequence of integers. Our objective is to minimize bit usage while ensuring that the encoding remains prefix-free. In simpler terms, we aim to devise a binary format where the codes for individual integers can be concatenated without ambiguity, allowing the decoder to reliably identify the start and end of each integer's representation within the bit stream and thus restore it to its original uncompressed state.

\subsection{Unary Code}
We begin by examining the unary code, a straightforward encoding method that represents a positive integer $x \geq 1$\footnote{This is not a strict condition, but we will assume it for clarity.} using $x$ bits. It represents $x$ as a sequence of $x-1$ zeros followed by a single one, denoted as $U(x)$. The correctness of this encoding is straightforward: the decoder identifies the end of the code upon encountering the first '1', and the value $x$ is simply the total number of bits read.

\noindent This coding method requires $x$ bits to represent $x$. While simple, this is exponentially longer than the $\lceil\log_2 x \rceil$ bits needed by its standard binary representation $B(x)$. Consequently, unary coding is efficient only for very small values of $x$ and becomes rapidly impractical as $x$ increases. This behavior aligns with the principles of Shannon's source coding theorem (\autoref{thm:source_coding_theorem}), which suggests an ideal code length of $-\log_2 P(x)$ bits for a symbol $x$ with probability $P(x)$. The unary code's length of $x$ bits corresponds precisely to this ideal length if the integers follow the specific probability distribution $P(x) = 2^{-x}$ \cite{ferragina2023pearls}.

\begin{theorem}
    The unary code $U(x)$ of a positive integer $x$ requires $x$ bits, and it is optimal for the geometric distribution $P(x)=2^{-x}$.
\end{theorem}

\noindent Despite its theoretical optimality for the $P(x)=2^{-x}$ distribution, the unary code faces practical challenges. Its implementation often involves numerous bit shifts or bit-level operations during decoding, which can be relatively slow on modern processors, especially for large $x$.

\begin{figure}[hbtp]
    \centering
    \begin{tikzpicture}[node distance=0mm, bit/.style={draw, minimum size=5mm, inner sep=0pt}]
        \node[bit] (b1) {0};
        \node[bit, right=of b1] (b2) {0};
        \node[bit, right=of b2] (b3) {0};
        \node[bit, right=of b3] (b4) {0};
        \node[bit, right=of b4] (b5) {1};
        % \draw [decorate, decoration={brace, amplitude=4pt, raise=4pt}] (b1.north west) -- (b5.north east) node [midway, above=6pt] {$x=5$ bits};
    \end{tikzpicture}
    \caption{Unary code $U(5) = \texttt{00001}$. It uses $x=5$ bits, consisting of $x-1=4$ zeros followed by a one.}
    \label{fig:unary_code_example}
\end{figure}


% ------------------------------------ Elias Codes ------------------------------------

\subsection{Elias Codes}
While unary code is simple, its inefficiency for larger integers motivates the development of \emph{universal codes} like those proposed by Elias \cite{Elias1975}, building upon earlier work by Levenstein. The term universal signifies that the length of the codeword for an integer $x$ grows proportionally to its minimal binary representation, specifically as $O(\log x)$, rather than, for instance, $O(x)$ as in the unary code. Compared to the standard binary code $B(x)$ (which requires $\lceil\log_2 x \rceil$ bits but is not prefix-free), the $\gamma$ and $\delta$ codes are only a constant factor longer but possess the crucial property of being prefix-free.

\paragraph{Gamma ($\gamma$) Code} The $\gamma$ code represents a positive integer $x$ by combining information about its magnitude (specifically, the length of its binary representation) with its actual bits. First, determine the length of the standard binary representation of $x$, denoted as $l = \lfloor \log_2 x \rfloor + 1$. The $\gamma$ code, $\gamma(x)$, is formed by concatenating the unary code of this length, $U(l)$, with the $l-1$ least significant bits of $x$ (i.e., $B(x)$ excluding its leading '1' bit, which is implicitly represented by the '1' in $U(l)$).
The decoding process mirrors this structure: read bits until the terminating '1' of the unary part is found to determine $l$, then read the subsequent $l-1$ bits and prepend a '1' to reconstruct $x$. The total length is $|U(l)| + (l-1) = l + (l-1) = 2l-1 = 2(\lfloor \log_2 x \rfloor + 1) - 1$ bits. From Shannon's condition, it follows that this code is optimal for sources where integer probabilities decay approximately as $P(x) \approx 1/x^2$ \cite{ferragina2023pearls}.

\begin{theorem}
    The $\gamma$ code of a positive integer $x$ takes $2(\lfloor \log_2 x \rfloor + 1) - 1$ bits. It is optimal for distributions where $P(x) \propto 1/x^2$ and its length is within a factor of two of the length of the standard binary code $B(x)$.
\end{theorem}

\begin{figure}[hbtp]
    \centering
    \begin{tikzpicture}[
            node distance=0mm,
            bit/.style={draw, minimum size=5mm, inner sep=0pt, anchor=west},
            lbl/.style={font=\footnotesize, text centered}
        ]
        \node[bit] (u1) {0};
        \node[bit, right=of u1] (u2) {0};
        \node[bit, right=of u2] (u3) {1};
        \node[bit, right=of u3] (b1) {1};
        \node[bit, right=of b1] (b2) {0};
    \end{tikzpicture}
    \caption{Elias $\gamma$ code for $x=6$. Binary $B(6)=\texttt{110}$, length $l=3$. The code consists of $U(3)=\texttt{001}$ followed by the $l-1=2$ trailing bits (\texttt{10}). Result: $\gamma(6)=\texttt{00110}$ (5 bits).}
    \label{fig:gamma_code_example}
\end{figure}

\noindent The inefficiency in the $\gamma$ code resides in the unary encoding of the length $l$, which can become long for large $x$. The $\delta$ code addresses this.

\paragraph{Delta ($\delta$) code} The $\delta$ code improves upon $\gamma$ by encoding the length $l = \lfloor \log_2 x \rfloor + 1$ more efficiently using the $\gamma$ code itself. The $\delta$ code, $\delta(x)$, is constructed by first computing $\gamma(l)$, the gamma code of the length $l$. Then, it appends the same $l-1$ least significant bits of $x$ used in $\gamma(x)$ (i.e., $B(x)$ without its leading '1').
Decoding involves first decoding $\gamma(l)$ to find the length $l$, and then reading the next $l-1$ bits to reconstruct $x$. The total number of bits is $|\gamma(l)| + (l-1) = (2\lfloor \log_2 l \rfloor + 1) + (l-1) = 2\lfloor \log_2 l \rfloor + l$. Asymptotically, this is approximately $\log_2 x + 2\log_2 \log_2 x + O(1)$ bits, which is only marginally longer ($1+o(1)$ factor) than the raw binary representation $B(x)$. This code achieves optimality for distributions where $P(x) \approx 1/(x(\log_2 x)^2)$ \cite{ferragina2023pearls}.

\begin{theorem}
    The $\delta$ code of a positive integer $x$ takes $2\lfloor \log_2 (\lfloor \log_2 x \rfloor + 1) \rfloor + \lfloor \log_2 x \rfloor + 1$ bits, approximately $\log_2 x + 2\log_2 \log_2 x$. It is optimal for distributions $P(x) \propto 1/(x(\log_2 x)^2)$ and is within a factor $1+o(1)$ of the length of $B(x)$.
\end{theorem}

\begin{figure}[hbtp]
    \centering
    \begin{tikzpicture}[
            node distance=0mm,
            bit/.style={draw, minimum size=5mm, inner sep=0pt, anchor=west},
            lbl/.style={font=\footnotesize, text centered}
        ]
        % Example: x = 6 -> B(x) = 110, l = 3. B(l)=B(3)=11. gamma(l)=gamma(3)=011
        % Nodes
        \node[bit] (g1) {0};
        \node[bit, right=of g1] (g2) {1};
        \node[bit, right=of g2] (g3) {1};
        \node[bit, right=of g3] (b1) {1};
        \node[bit, right=of g1] (b2) {0}; % <<-- CORREZIONE: posizionato a destra di b1

        % Braces and labels above
        % \draw [decorate, decoration={brace, amplitude=4pt, mirror, raise=4pt}] (g1.south west) -- (g3.south east)
        % node [midway, below=6pt, lbl] (lengthlbl) {Length part: $\gamma(l=3)$};
        % \draw [decorate, decoration={brace, amplitude=4pt, mirror, raise=4pt}] (b1.south west) -- (b2.south east)
        % node [midway, below=6pt, lbl] (valuelbl) {Value part: $l-1$ bits (\texttt{10})};

    \end{tikzpicture}
    \caption{Elias $\delta$ code for $x=6$. $B(6)=\texttt{110}$, length $l=3$. First, encode $l=3$ using $\gamma$: $\gamma(3)=\texttt{011}$. Then, append the $l-1=2$ trailing bits (\texttt{10}). Result: $\delta(6)=\texttt{01110}$ (5 bits).}
    \label{fig:delta_code_example}
\end{figure}

\noindent As with the unary code, decoding Elias codes often involves bit shifts, potentially impacting performance for very large integers compared to byte-aligned or word-aligned codes.

\subsection{Rice Code}
Elias codes offer universality but can be suboptimal if integers cluster around values far from powers of two. Rice codes \cite{rice1979some} (a special case of Golomb codes) address this by introducing a parameter $k > 0$, chosen based on the expected distribution of integers. For an integer $x \ge 1$, the Rice code $R_k(x)$ is determined by calculating the quotient $q = \lfloor (x-1) / 2^k \rfloor$ and the remainder $r = (x-1) \pmod{2^k}$. The code is then formed by concatenating the unary code of the quotient plus one, $U(q+1)$, followed by the remainder $r$ encoded using exactly $k$ bits (padding with leading zeros if necessary), denoted $B_k(r)$. This structure is efficient when integers often yield small quotients $q$, meaning they are close to (specifically, just above) multiples of $2^k$.

\noindent The total number of bits required for $R_k(x)$ is $(q+1) + k$. Rice codes are optimal for geometric distributions $P(x) = p(1-p)^{x-1}$, provided the parameter $k$ is chosen such that $2^k$ is close to the mean or median of the distribution (specifically, optimal when $2^k \approx -\frac{\ln 2}{\ln(1-p)} \approx 0.69 \times \text{mean}(S)$) \cite{ferragina2023pearls, witten1999managing}. The fixed length of the remainder part facilitates faster decoding compared to Elias codes in certain implementations.

\begin{figure}[hbtp]
    \centering
    \begin{tikzpicture}[
            node distance=0mm,
            bit/.style={draw, minimum size=5mm, inner sep=0pt, anchor=west},
            lbl/.style={font=\footnotesize, text centered}
        ]
        \node[bit] (u1) {0};
        \node[bit, right=of u1] (u2) {1};
        \node[bit, right=of u2] (b1) {1};
        \node[bit, right=of b1] (b2) {0};
        \node[bit, right=of b2] (b3) {0};
    \end{tikzpicture}
    \caption{Rice code for $x=13$ with parameter $k=3$. Calculate $q = \lfloor (13-1) / 2^3 \rfloor = 1$ and $r = (13-1) \pmod 8 = 4$. The code is $U(q+1)=U(2)=\texttt{01}$ followed by $r=4$ in $k=3$ bits, $B_3(4)=\texttt{100}$. Result: $R_3(13)=\texttt{01100}$ (5 bits).}
    \label{fig:rice_code_example}

\end{figure}

\subsection{Elias-Fano Code} \label{sec:elias_fano_code}

The Elias-Fano representation, conceived independently by Peter Elias \cite{Elias1975} and Robert M. Fano \cite{Fano1971}, provides an elegant and practically effective method for compressing monotonically increasing sequences of integers. A key advantage of this technique is its ability to achieve near-optimal space occupancy, often requiring only a small overhead above the information-theoretic minimum, while simultaneously supporting efficient random access and search operations directly on the compressed form \cite{ferragina2023pearls, pibiri_et_al}. This makes it highly suitable for applications such as inverted index compression in modern search engines \cite{vigna2013quasi, EFVenturini2014}.

\paragraph{Representation Structure}
Let's consider a sequence of $n$ non-negative increasing integers:

$$S = \{s_0, s_1, \ldots, s_{n-1}\}$$

where $0 \le s_0 < s_1 < \ldots < s_{n-1} < u$. The universe size is $u$, and we typically assume $u > n$. Each integer $s_i$ can be represented using $b = \lceil \log_2 u \rceil$ bits. The Elias-Fano encoding strategy involves partitioning these $b$ bits into two segments, based on a parameter $l$. The choice $l = \lfloor \log_2 (u/n) \rfloor$ minimizes the total space requirement \cite{Elias1975, ferragina2023pearls} (if $u \le n$, we set $l=0$). The two parts are:
\begin{itemize}
    \item The \emph{lower bits}, $L(s_i)$, consisting of the $l$ least significant bits of $s_i$.
    \item The \emph{upper bits}, $H(s_i)$, consisting of the remaining $h = b - l$ most significant bits.
\end{itemize}
The representation then comprises two main components:
\begin{enumerate}
    \item \emph{Lower Bits Array ($L$):} This array is formed by concatenating the $l$-bit lower parts of all integers in the sequence: $L = L(s_0)L(s_1) \ldots L(s_{n-1})$. The total size of this array is exactly $n \cdot l$ bits.
    \item \emph{Upper Bits Bitvector ($H$):} This bitvector encodes the distribution of the upper bits. For each possible value $j$ (from $0$ to $2^h-1$) that the upper bits can assume, let $c_j$ be the number of elements $s_i$ in $S$ for which $H(s_i) = j$. The bitvector $H$ is constructed by concatenating, for $j = 0, 1, \ldots, 2^h-1$, a sequence of $c_j$ ones followed by a single zero ($1^{c_j}0$). This structure results in a bitvector of length exactly $n + 2^h$, containing $n$ ones (one for each element in $S$) and $2^h$ zeros (one acting as a delimiter for each possible upper bit value).
\end{enumerate}
The space bound $n + 2^h \le 2n$ holds if $2^h \le n$, which corresponds to $u/n \le n$. When $u/n$ is small (dense sequences), $l$ is small and $h$ is large; when $u/n$ is large (sparse sequences), $l$ is large and $h$ is small.

% --- Theorem Environment ---
\begin{theorem}[Elias-Fano Space Complexity {{ferragina2023pearls}}] \label{thm:ef_space_revised}
    The Elias-Fano encoding of a strictly increasing sequence $S$ of $n$ integers in the range $[0, u)$ requires $n \lfloor \log_2 (u/n) \rfloor + n + 2^h$ bits, where $h = \lceil \log_2 u \rceil - \lfloor \log_2 (u/n) \rfloor$. This is upper bounded by $n \log_2(u/n) + 2n$ bits, which is provably less than 2 bits per integer above the information-theoretic lower bound. The representation can be constructed in $O(n)$ time.
\end{theorem}

Figure \ref{fig:ef_code_example_revised} illustrates the Elias-Fano encoding for the sequence $S = \{1, 4, 7, 18, 24, 26, 30, 31\}$. In this example, we have $n=8$ integers in the universe $u=32$. The total number of bits needed to represent any number in the universe is $b=\lceil \log_2 32 \rceil = 5$. Since $u/n = 32/8 = 4$, the number of lower bits is $l = \lfloor \log_2 4 \rfloor = 2$, and the number of upper bits is $h=b-l=5-2=3$. The lower bits array $L$ is formed by concatenating the $l=2$ least significant bits of each $s_i$. The upper bits bitvector $H$ is formed by counting the occurrences $c_j$ of each upper bit value $j \in [0, 2^h-1)$ and concatenating $1^{c_j}0$ for each $j$. For instance, $H(s_0)=0$ occurs once ($c_0=1$), $H(s_1)=H(s_2)=1$ occurs twice ($c_1=2$), $H=2$ and $H=3$ never occur ($c_2=0, c_3=0$), $H(s_3)=4$ occurs once ($c_4=1$), $H=5$ never occurs ($c_5=0$), $H(s_4)=H(s_5)=6$ occurs twice ($c_6=2$), and $H(s_6)=H(s_7)=7$ occurs twice ($c_7=2$). Concatenating $1^10$ (for $H=0$), $1^20$ (for $H=1$), $1^00$ (for $H=2$), $1^00$ (for $H=3$), $1^10$ (for $H=4$), $1^00$ (for $H=5$), $1^20$ (for $H=6$), and $1^20$ (for $H=7$) yields the final bitvector $H$.

% --- Figure Environment ---
\begin{figure}[hbtp] % Use [htbp] for placement suggestion: here, top, bottom, page
    \centering
    \footnotesize % Make font smaller for the table
    \begin{tabular}{c | c | c c} \hline
        $i$ & $s_i$ & $H(s_i)$ (val, 3b) & $L(s_i)$ (val, 2b) \\ \hline
        0   & 1     & 0 (\texttt{000})   & 1 (\texttt{01})    \\
        1   & 4     & 1 (\texttt{001})   & 0 (\texttt{00})    \\
        2   & 7     & 1 (\texttt{001})   & 3 (\texttt{11})    \\
        3   & 18    & 4 (\texttt{100})   & 2 (\texttt{10})    \\
        4   & 24    & 6 (\texttt{110})   & 0 (\texttt{00})    \\
        5   & 26    & 6 (\texttt{110})   & 2 (\texttt{10})    \\
        6   & 30    & 7 (\texttt{111})   & 2 (\texttt{10})    \\
        7   & 31    & 7 (\texttt{111})   & 3 (\texttt{11})    \\ \hline
    \end{tabular} \\ % Use \\ for line break after tabular
    \vspace{0.5em} % Add some vertical space
    $L = \texttt{0100111000101011}$ ($n \cdot l = 8 \times 2 = 16$ bits) \\
    $H = \texttt{1011000100110110}$ ($n + 2^h = 8 + 2^3 = 16$ bits)
    \caption[Elias-Fano encoding example]{Elias-Fano encoding example for the sequence $S = \{1, 4, 7, 18, 24, 26, 30, 31\}$ with parameters $n=8$, $u=32$, $l=2$, $h=3$. The table shows the decomposition of each $s_i$ into its upper $H(s_i)$ and lower $L(s_i)$ bits. Below the table are the resulting concatenated lower bits array $L$ and the upper bits bitvector $H$.}
    \label{fig:ef_code_example_revised} % Ensure label is unique
\end{figure}

\paragraph{Query Operations}
A significant advantage of the Elias-Fano representation is its support for direct queries on the compressed data. This requires augmenting the upper bits bitvector $H$ with auxiliary data structures that enable constant-time calculation of \emph{rank} and \emph{select} queries (see Section~\ref{sec:bitvectors} for details). These structures typically add a $o(n)$ bits to the overall space complexity. With these in place, the core operations are:

\emph{Access(i)}: This operation retrieves the $i$-th element $s_i$ (using 0-based indexing for $i$, $0 \le i < n$).
\begin{enumerate}
    \item The lower $l$ bits, $L(s_i)$, are directly read from the array $L$ starting at bit position $i \cdot l$.
    \item The position $p$ in $H$ corresponding to the end of the unary code for $s_i$ is found using $p = \texttt{select}_1(H, i+1)$. The $\texttt{select}_1$ operation finds the position of the $(i+1)$-th bit set to $1$.
    \item The value of the upper $h$ bits, $H(s_i)$, is determined by counting the number of preceding zeros in $H$ up to position $p$. This count is precisely $H(s_i) = p - (i+1)$, as there are $i+1$ ones and $H(s_i)$ zeros up to that point. Alternatively, $H(s_i) = \texttt{rank}_0(H, p)$.
    \item The original integer is reconstructed by combining the upper and lower parts: $s_i = (H(s_i) \ll l) \lor L(s_i)$, where $\ll$ denotes the bitwise left shift and $\lor$ denotes the bitwise OR.
\end{enumerate}
Since reading from $L$ and performing rank/select on $H$ take constant time, \emph{Access(i)} operates in $O(1)$ time \cite{pibiri_et_al}.

\emph{Successor(x)} (or \emph{NextGEQ(x)}): This operation finds the smallest element $s_i$ in $S$ such that $s_i \ge x$, given a query value $x \in [0, u)$.
\begin{enumerate}
    \item Determine the upper $h$ bits $H(x)$ and lower $l$ bits $L(x)$ of the query value $x$.
    \item Identify the range of indices $[p_1, p_2)$ in $S$ corresponding to elements whose upper bits are equal to $H(x)$. The starting index $p_1$ is the number of elements in $S$ with upper bits strictly less than $H(x)$. This can be found by locating the $H(x)$-th zero in $H$ using $pos_0 = \texttt{select}_0(H, H(x)+1)$ (using 1-based index for select); then $p_1 = pos_0 - H(x)$. The ending index $p_2$ (exclusive) is similarly found using the $(H(x)+1)$-th zero: $pos'_0 = \texttt{select}_0(H, H(x)+1+1)$; then $p_2 = pos'_0 - (H(x)+1)$.
    \item Perform a search (e.g., binary search, or linear scan if $p_2 - p_1$ is small) over the lower bits $L[p_1 \cdot l \dots p_2 \cdot l - 1]$. The goal is to find the smallest index $k \in [p_1, p_2)$ such that the reconstructed value $(H(x) \ll l) \lor L(s_k)$ is greater than or equal to $x$.
    \item If such a $k$ is found within the range $[p_1, p_2)$, then $s_k$ is the successor.
    \item If no such element exists in the range (i.e., all elements with upper bits $H(x)$ are smaller than $x$), the successor must be the first element with upper bits greater than $H(x)$. This element is simply $s_{p_2}$, which can be retrieved using \emph{Access($p_2$)} (if $p_2 < n$).
\end{enumerate}
The dominant cost is the search over the lower bits. Since there can be up to roughly $u/n$ elements sharing the same upper bits in the worst case, the search step takes $O(\log(u/n))$ time using binary search. The select operations take $O(1)$ time. Thus, \emph{Successor(x)} takes $O(1 + \log(u/n))$ time \cite{pibiri_et_al, ferragina2023pearls}.

\emph{Predecessor(x)}: Finding the largest element $s_i \le x$ follows a symmetric logic, searching within the same index range $[p_1, p_2)$ identified using $H(x)$. If the search within the lower bits $L[p_1 \cdot l \dots p_2 \cdot l - 1]$ yields a suitable candidate $s_k \le x$, that is the answer (specifically, the largest such $s_k$). If all elements in the range $[p_1, p_2)$ are greater than $x$, or if the range is empty ($p_1=p_2$), the predecessor must be the last element with upper bits less than $H(x)$, which is $s_{p_1-1}$ (if $p_1 > 0$). This can be retrieved using \emph{Access($p_1-1$)}. The time complexity is also $O(1 + \log(u/n))$.
