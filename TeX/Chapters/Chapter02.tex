% Chapter 2

\chapter{Compression Concepts and Techniques} % Chapter title

\label{ch:Chapter2} % For referencing the chapter elsewhere, use \autoref{ch:Chapter2}

Write something about information theory, it's history and bla bla, just one paragraph. From \cite{han2002mathematics,ElementsofInformationTheory}

\section{Shannon Entropy}
\begin{itemize}
    \item Some introduction to entropy and Cloude Shannon and Von Neumann, talk about \cite{Shannon1948}
    \item Definition of entropy of a random variable, from this book \cite{han2002mathematics}
    \item Definition of Worst Case Entropy \cite{navarro2016compact}
    \item Definition of Shannon Entropy \cite{navarro2016compact}
\end{itemize}

\subsection{Properties}
Take things from \cite{han2002mathematics} and talk about additivity and concavity of entropy.

\subsection{Mutual Information}
Take section 2.3 from \cite{han2002mathematics} and talk about mutual information.

\subsection{Fano's inequality}
\begin{itemize}
    \item Statement of Fano's inequality \cite{ElementsofInformationTheory}
    \item Proof of Fano's inequality \cite{ElementsofInformationTheory}
\end{itemize}

\section{Empirical Entropy}
Introduction to empirical entropy, from \cite{han2002mathematics} in section 2.6 and in \cite{navarro2016compact} at section 2.3
\subsection{Bit Sequences}
\subsection{Sequences of Symbols}

\section{Higher Order Entropy}
Section 2.4 from \cite{navarro2016compact}

\section{Source and Coding}
Make an introduction to the fact that there are different types of sources and coding techniques. From \cite{han2002mathematics} and \cite{ElementsofInformationTheory}.

\subsection{Source Coding Theorem}
From \cite{Shannon1948} and \cite{han2002mathematics}

\subsection{Direct Theorem for FF Coding}


\subsection{Strong Converse Theorem for FF Coding}
After the direct theorem, we need a criterion for seeking and implementi a code which performs as well as possible. Hence, it is quite possibile to recognize that there exits no better code than the code designed by the above theorem. This strong converse theorem is a criterion for the optimality of the code. From \cite{han2002mathematics}

\section{Integer Coding}

Introduction to integer coding, and some theorems on unary coding. Almost every thing from \cite{ferragina2023pearls} and \cite{han2002mathematics}

\subsection{Elias Codes: $\gamma$ and $\delta$}
\begin{itemize}
    \item Definition of Elias codes
    \item Theorems on space occupancy
\end{itemize}

\subsection{Rice Code}
Just a brief mention
\subsection{Elias-Fano Code}
All form \cite{ferragina2023pearls,sayood2002lossless}
\begin{itemize}
    \item Definition of Elias-Fano code and examples
    \item Theorem on the space bound of Elias-Fano code
    \item Introduction to \texttt{Access(i)} and \texttt{NextGEQ(x)}
\end{itemize}

\section{Statistical Coding}

Introduction to statistical coding, from \cite{han2002mathematics} and \cite{ferragina2023pearls}

\subsection{Huffman Coding}

\subsection{Arithmetic Coding}

\section{Bitvectors}

Introduction to bitvectors, from \cite{ferragina2023pearls} and \cite{navarro2016compact}

\subsection{Access}
\subsection{Rank}
\subsection{Select}
\subsection{RRR: A Space-Efficient Rank/Select Structure}
