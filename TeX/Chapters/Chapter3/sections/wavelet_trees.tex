\clearpage
\section{Wavelet Trees}

Wavelet trees, introduced in 2003 by Grossi, Gupta, and Vitter \cite{GrossiWT2003}, represent a significant development in the field of succinct data structures. They function as self-indexing structures, capable of supporting fundamental queries like \texttt{rank} and \texttt{select} directly on the compressed representation, while still permitting access to the original sequence data. This blend of capabilities makes them exceptionally useful, particularly in the construction of compressed full-text indexes such as the FM-index \cite{ferragina2000opportunistic}, where they are often employed to efficiently handle rank queries during pattern matching \cite{WTForALL}.

Interestingly, the core idea behind wavelet trees shares resemblance to earlier structures; notably, it can be seen as a generalization of a data structure developed by Chazelle in 1988 \cite{Chazelle1988} for computational geometry problems, designed to represent point grids and manage their reshuffling based on coordinates. Furthermore, Kärkkäinen utilized a related concept in 1999 \cite{karkkainen1999repetition} within the context of repetition-based text indexing. However, the specific formulation and the range of applications envisioned by Grossi et al. were distinct and led to a broader impact \cite{WTForALL}.

The versatility of wavelet trees stems from their ability to be interpreted in multiple ways: (i) as a direct representation of a sequence, (ii) as an encoding of a permutation or reordering of elements, and (iii) as a representation of points on a grid. Since their inception, these perspectives, along with their interactions, have fueled innovative solutions across a surprisingly wide array of problems, extending far beyond their origins in text indexing and computational geometry \cite{WTForALL, WTFromTheoryToPractice, TheMyriadVirtuesWT}.

\subsection*{An introduction to the problem}
Consider a sequence $S[1,n]$ as a generalization of bitvectors whose elements $S[i]$ are drawn from an alphabet $\Sigma$\footnote{The size of the alphabet varies depending on the application. For example, in DNA sequences, the alphabet is $\Sigma = \{A,C,G,T\}$, while in other case it could be of millions of characters, such as in natural language processing.}. We are interested in the following operations on the sequence $S$:
\begin{itemize}
    \item \texttt{Access(i)}: return the $i$-th element of $S$.
    \item \texttt{Rank(c,i)}: return the number of occurrences of character $c$ in the prefix $S[1,i]$.
    \item \texttt{Select(c,i)}: return the position of the $i$-th occurrence of character $c$ in $S$.
\end{itemize}
However, dealing with sequences is much more complex than dealing with bitvectors (as we have seen in \autoref{sec:bitvectors}). Navarro \cite{navarro2016compact} shows how a naive approach to solve this problem would require $n\sigma + o(n\sigma)$ bits of space, which is not space-efficient. Consider $\sigma$ bitvectors of length $n$, one for each symbol in the alphabet such that the $i$-th bit of the $c$-th bitvector is 1 if $S[i] = c$ and 0 otherwise. Then answering a $rank$ and $select$ query would be done by this simple transformation
\begin{align*}
    rank_c(S,i)   & = rank_1(B_c,i)   \\
    select_c(S,j) & = select_1(B_c,j)
\end{align*}
If we try to use the techniques from \autoref{sec:bitvectors} to compress the bitvectors, we would end up with a constant time complexity for the $rank$ and $select$ queries, but with the downside of a space occupancy of $n\sigma + o(n\sigma)$ bits. This is not space-efficient considering that the plain representation of the string requires $n\log \sigma + o(n)$ bits.\footnote{Even if we use a compressed representation of the bitvectors, the space occupancy would still have the dominant term $n\sigma$, which is at least $\Omega(n\sigma \log \log n / \log n)$ bits if constant-time $rank$ and $select$ queries are still required.}
\begin{remark}[Notation]
    From now on, let $S[1,n]$ = $s_1s_2\dots s_n$ be a sequence of length $n$ over an alphabet $\Sigma$ that for simplicity we write as $\Sigma = \{1,\dots,\sigma\}$. In this way, the string can be represented using $n \lceil \log \sigma \rceil = n \log \sigma + o(n)$ bits in plain form.
\end{remark}

\subsection{Structure and construction}

In the beginning of this section we showed that storing one bitvector per symbol is not space-efficient. The wavelet tree is a data structure that solves this problem by using a recursive hierarchical partitioning of the alphabet. Consider the subset $[a,b] \subset [1, \dots, \sigma]$, then a wavelet tree over $[a,b]$ is a balanced binary tree with $b-a+1$ leaves\footnote{if $a=b$ then the tree is just a leaf}. The root node $v_{root}$ is associated with the whole sequence $S[1,n]$, and stores a bitmap $B_{v_{root}}[1,n]$ defined as follows: $B_{v_{root}}[i] = 0$ if $S[i] \leq (a+b)/2$ and $B_{v_{root}}[i] = 1$ otherwise. The tree is then recursively built by associating the subsequence $S_0[1,n_0]$ of elements in $[a, \dots,\lfloor (a+b)/2 \rfloor ]$ to the left child of $v$, and the subsequence $S_1[1,n_1]$ of elements in $[\lfloor (a+b)/2 \rfloor +1,\dots, b]$ to the right child of $v$. This process is repeated until the leaves are reached. In this way the left child of the root node, is a wavelet tree for $S_0[1,n_0]$ over the alphabet $[a,\dots, \lfloor (a+b)/2 \rfloor ]$, and the right child is a wavelet tree for $S_1[1,n_1]$ over the alphabet $[\lfloor (a+b)/2 \rfloor +1,\dots, b]$. \cite{WTForALL}

\noindent Building a wavelet tree is a recursive process that takes $O(n\log \sigma$) time by processing each node of the tree in linear time. The steps are outlined in Algorithm \ref{alg:build_wt}. Excluding the sequence $S$ and the final wavelet tree $T$, the algorithm uses $n \log \sigma$ bits of space \footnote{While building the wavelet tree, we can store the sequence $S$ on disk to free memory.}.

\begin{algorithm}[ht!]
    \caption{Building a wavelet tree}\label{alg:build_wt}
    \begin{algorithmic}
        % \Require Sequence $S[1,n]$ over alphabet $\Sigma = \{1,\dots,\sigma\}$
        % \Ensure Wavelet tree $T$ for $S$
        %

        \Function{build\_wt}{$S,n$}
        \State $T \gets build(S,n,1,\sigma)$
        \State \Return $T$
        \EndFunction


        \Function {build}{$S,n,a,b$} \Comment{\small{Takes a string $S[1,n]$ over $[a,b]$}}
        \If {$a = b$}
        \State Free S
        \State \Return null
        \EndIf
        \State $v \gets$ new node
        \State $m \gets \lfloor (a+b)/2 \rfloor$
        \State $z \gets 0$ \Comment{\small{number of elements in $S$ that are $\leq m$}}
        \For {$i \gets 1$ to $n$}
        \If {$S[i] \leq m$}
        \State $z \gets z+1$
        \EndIf
        \EndFor

        \State Allocate strings $S_{left}[1,z]$ and $S_{right}[1,n-z]$
        \State Allocate bitmap $v.B[1,n]$
        \State $z \gets 0$

        \For {$i \gets 1$ to $n$}
        \If {$S[i] \leq m$}
        \State \texttt{bitclear}($v.B,i$) \Comment{\small{set $i$-th bit of $v.B$ to 0}}
        \State $z \gets z+1$
        \State $S_{left}[z] \gets S[i]$
        \Else
        \State \texttt{bitset}($v.B,i$) \Comment{\small{set $i$-th bit of $v.B$ to 1}}
        \State $S_{right}[i-z] \gets S[i]$
        \EndIf
        \EndFor

        \State Free S
        \State $v.left \gets \texttt{build}(S_{left},z,a,m)$
        \State $v.right \gets \texttt{build}(S_{right},n-z,m+1,b)$
        \State Pre-process $v.B$ for $rank$ and $select$ queries
        \State \Return $v$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{remark} \label{rem:space_occupancy_wavelet_tree}
    The wavelet tree described has $\sigma$ leaves and $\sigma-1$ internal nodes, and the height of the tree is $ \lceil \log \sigma \rceil$. The space occupancy of each level it's exactly $n$ bits, while we have at most $n$ bits for the last level. The total number of bits stored by the wavelet tree is then upper bounded by $n \lceil \log \sigma \rceil$ bits. \cite{WTForALL}. However, if we also interested in storing the topology of the wavelet tree, then another $O(\sigma \log n)$ bits are needed, which can be critical for large alphabets. In \cite{claude2011space,tischler2011wavelet} are presented some techniques to build wavelet tree in a space-efficient way.
\end{remark}

\begin{example}[Building a wavelet tree]
    Consider the sentence
    \begin{center}
        \texttt{wookies\_wield\_wicked\_weapons\_with\_wisdom\$}
    \end{center}
    where spaces are replaced by underscores and the sentence ends with a special character. The sorted alphabet for this example is
    \[
        \Sigma = \{\$, \_, a, c, d, e, h, i, k, l, m, n, o, p, s, t, w \}
    \]
    where we assume that in the lexicon the special character comes before the underscore. We now assign a bit to each symbol in the alphabet, where 0 is assigned to the first half of the alphabet and 1 to the second half.

    \begin{tabular}{*{17}{c}}
        \$                        & \_                        & a                         & c                         & d                         & e                         & h                         & i                         & k                         & l                         & m                         & n                         & o                         & p                         & s                         & t                         & w                         \\
        \textcolor{purple!100}{0} & \textcolor{purple!100}{0} & \textcolor{purple!100}{0} & \textcolor{purple!100}{0} & \textcolor{purple!100}{0} & \textcolor{purple!100}{0} & \textcolor{purple!100}{0} & \textcolor{purple!100}{0} & \textcolor{purple!100}{1} & \textcolor{purple!100}{1} & \textcolor{purple!100}{1} & \textcolor{purple!100}{1} & \textcolor{purple!100}{1} & \textcolor{purple!100}{1} & \textcolor{purple!100}{1} & \textcolor{purple!100}{1} & \textcolor{purple!100}{1} \\
    \end{tabular}

    \noindent We can now build the wavelet tree for this sequence, recursively partitioning the alphabet and assigning a bit to each symbol. The resulting wavelet tree is shown in \autoref{fig:wavelet_tree_example}

\end{example}

\begin{figure}[h]
    \centering
    \tikzstyle{every node}=[font=\footnotesize]
    \begin{tikzpicture}[
            scale=0.5,
            level distance=4.5cm,
            level 1/.style={sibling distance=12.3cm},
            level 2/.style={sibling distance=6.3cm},
            level 3/.style={sibling distance=3cm},
            level 4/.style={sibling distance=1.6cm},
            level 5/.style={sibling distance=0.7cm},
            level 6/.style={sibling distance=0.7cm},
            level 7/.style={sibling distance=0.7cm}]  % Added level 7 style
        \node[align=center] {\texttt{wookies\_wield\_wicked\_weapons\_with\_wisdom\$}\\\texttt{111100101001001001000100111101010010}}
        child {node[align=center] {\texttt{ie\_ied\_iced\_ea\_ih\_id\$}\\\texttt{110111010110100110110}}
                child {node[align=center] {\texttt{\_\_c\_a\_\_\&}\\\texttt{00101000}}
                        child {node[align=center] {\texttt{\_\_\_\_\_\$}\\\texttt{000001}}
                                child {node[align=center] {\texttt{\$}} edge from parent node[left] {0}}  % Added level 7 node
                                child {node[align=center] {\texttt{\_}} edge from parent node[right] {1}}  % Added level 7 node
                                edge from parent node[left] {0}}
                        child {node[align=center] {\texttt{ca}\\\texttt{10}}
                                child {node[align=center] {\texttt{a}} edge from parent node[left] {0}}  % Added level 7 node
                                child {node[align=center] {\texttt{c}} edge from parent node[right] {1}}  % Added level 7 node
                                edge from parent node[right] {1}}
                        edge from parent node[left] {0}}
                child {node[align=center] {\texttt{ieiediedeihid}\\\texttt{1010010001110}}
                        child {node[align=center] {\texttt{iiiihi}\\\texttt{111101}}
                                child {node[align=center] {\texttt{h}} edge from parent node[left] {0}}  % Added level 7 node
                                child {node[align=center] {\texttt{i}} edge from parent node[right] {1}}  % Added level 7 node
                                edge from parent node[left] {0}}
                        child {node[align=center] {\texttt{eededed}\\\texttt{1101010}}
                                child {node[align=center] {\texttt{d}} edge from parent node[left] {0}}  % Added level 7 node
                                child {node[align=center] {\texttt{e}} edge from parent node[right] {1}}  % Added level 7 node
                                edge from parent node[right] {1}}
                        edge from parent node[right] {1}}
                edge from parent node[left] {0}}
        child {node[align=center] {\texttt{wookswlwkwponswtwsom}\\\texttt{11101101011101111110}}
                child {node[align=center] {\texttt{klknm}\\\texttt{00011}}
                        child {node[align=center] {\texttt{klk}\\\texttt{010}}
                                child {node[align=center] {\texttt{k}} edge from parent node[left] {0}}  % Added level 7 node
                                child {node[align=center] {\texttt{l}} edge from parent node[right] {1}}  % Added level 7 node
                                edge from parent node[left] {0}}
                        child {node[align=center] {\texttt{nm}\\\texttt{10}}
                                child {node[align=center] {\texttt{m}} edge from parent node[left] {0}}  % Added level 7 node
                                child {node[align=center] {\texttt{n}} edge from parent node[right] {1}}  % Added level 7 node
                                edge from parent node[right] {1}}
                        edge from parent node[left] {0}}
                child {node[align=center] {\texttt{wooswwwposwtwso}\\\texttt{100111100111110}}
                        child {node[align=center] {\texttt{oopoo}\\\texttt{00100}}
                                child {node[align=center] {\texttt{o}} edge from parent node[left] {0}}  % Added level 7 node
                                child {node[align=center] {\texttt{p}} edge from parent node[right] {1}}  % Added level 7 node
                                edge from parent node[left] {0}}
                        child {node[align=center] {\texttt{swwwswtws}\\\texttt{1011101110}}
                                child {node[align=center] {\texttt{s}}}
                                child {node[align=center] {\texttt{wwwwtw}\\\texttt{111101}}
                                        child {node[align=center] {\texttt{t}} edge from parent node[left] {0}}  % Added level 7 node
                                        child {node[align=center] {\texttt{w}} edge from parent node[right] {1}}  % Added level 7 node
                                        edge from parent node[right] {1}}
                                edge from parent node[right] {1}}
                        edge from parent node[right] {1}}
                edge from parent node[right] {1}};
    \end{tikzpicture}
    \caption{\small Wavelet tree for the sequence \texttt{wookies\_wield\_\dots}} \label{fig:wavelet_tree_example}
\end{figure}

\subsubsection*{Tracking symbols}
We have seen how the wavelet tree serves as a representation for a string $S$, but more than that it is a succinct data structure for the string. Thus, it takes space asymptotically close to the plain representation of the string and allows us to access the $i$-th symbol of the string in $O(\log \sigma)$ time.

\subsubsection{Access}
In algorithm \ref{alg:access_wt} we show how extract the $i$-th symbol of the string $S$ using a wavelet tree $T$, this operation is called \texttt{Access}. In order to find $S[i]$, we first look at the bitmap associated with the root node of the wavelet tree, and depending on the value of the $i$-th bit of the bitmap, we move to the left or right child of the root node and continue recursively. However, the problem is to determine where our $i$ has been mapped to: if we move to the left child, then we need to find the $i$-th 0 in the bitmap of the left child, and if we move to the right child, then we need to find the $i$-th 1 in the bitmap of the right child. This is done by the $rank_0$ and $rank_1$ functions, respectively. We continue this process until we reach a leaf node, and then we return the value of the leaf node.

\begin{algorithm}[h!]
    \caption{\texttt{Access} queries on a wavelet tree}\label{alg:access_wt}
    \begin{algorithmic}
        \Function {access}{$T,i$} \Comment{\small{$T$ is the sequence $S$ seen as a wavelet tree}}
        \State $v \gets T_{root}$ \Comment{\small{start at the root node}}
        \State $[a,b] \gets [1,\sigma]$
        \While{$a \neq b$}
        \If{$access(v.B,i) =0$} \Comment{\small{$i$-th bit of the bitmap of $v$}}
        \State $i \gets rank_0(v.B,i)$
        \State $v \gets v.left$ \Comment{\small{move to the left child of node $v$}}
        \State $b \gets \lfloor (a+b)/2 \rfloor$
        \Else
        \State $i \gets rank_1(v.B,i)$
        \State $v \gets v.right$ \Comment{\small{move to the right child of node $v$}}
        \State $a \gets \lfloor (a+b)/2 \rfloor +1$
        \EndIf
        \EndWhile
        \State \Return $a$
        \EndFunction
    \end{algorithmic}
\end{algorithm}


\subsubsection{Select}
In addition to retrieving the $i$-th symbol of the string, we might also need to perform the inverse operation. That is, given a symbol's position at a leaf node, we aim to determine the position of the symbol in the string. This operation is referred to as \texttt{Select} and is outlined in Algorithm \ref{alg:select_wt}. Assume we start at a given leaf node $v$ and want to find the position of the $j$-th occurrence of symbol $c$ in the string. We recursively move to the left or right child of the node $v$: if the leaf is the right child of its parent, then we need to find the $j$-th 1 in the bitmap of the parent node, and if the leaf is the left child of its parent, then we need to find the $j$-th 0 in the bitmap of the parent node. This is done by the $select_0$ and $select_1$ functions, respectively. We continue this process until we reach the root node, and then we return the position of the symbol in the string. As we have seen in \autoref{sec:bitvectors}, this two single operations can be solved in constant time if we use the \texttt{RRR} data structure \cite{RRR2002} on each bitmap. Thus, the time complexity to perform a \texttt{Select} query on a wavelet tree is $O(\log \sigma)$.

\begin{algorithm}
    \caption{\texttt{Select} queries on a wavelet tree}\label{alg:select_wt}
    \begin{algorithmic}
        % add two functions to the algorithmic environment
        \Function{$\text{select}_c$}{$S,j$}
        % \Require Sequence $S$ (as a wavelet tree $T$), symbol $c$, position $j$
        % \Ensure The position of the $j$-th occurrence of $c$ in $S$

        \State \Return $\text{select}(T._{root},1, \sigma, c, j)$
        \EndFunction



        \Function{$\text{select}$}{$v,a,b,c,j$}
        % \Comment $v$ is the current node, $[a,b]$ is the alphabet range, $c$ is the symbol, $j$ is the position
        \If{$a = b$}
        \State \Return $j$
        \EndIf

        \If {$c \leq \lfloor (a+b)/2 \rfloor$}
        \State $j$ $\gets$ $\text{select}(v.left, a, \lfloor (a+b)/2 \rfloor, c, j)$
        \Return $select_0(v.B,j)$

        \Else
        \State $j$ $\gets$ $\text{select}(v.right, \lfloor (a+b)/2 \rfloor +1, b, c, j)$
        \State \Return $select_1(v.B,j)$
        \EndIf

        \EndFunction

    \end{algorithmic}
\end{algorithm}

\subsubsection{Rank}
During the $select$ algorithm, we track upwards the path from the leaf to the root. The process for solving a $rank$ query is similar, but instead of moving from the leaf to the root, we move from the root to the leaf. Algorithm \ref{alg:access_wt} also gives us the number of occurrences of a symbol $S[i]$ in the prefix $S[1,i]$, i.e $rank_{S[i]}(S,i)$. We now want to generalize this operation to solve any rank query $rank_c(S,i)$, where $c$ is a symbol in the alphabet. This procedure is shown in algorithm \ref{alg:rank_wt}.

\noindent As mentioned in Remark \ref{rem:space_occupancy_wavelet_tree}, storing the explicit tree topology can incur an $O(\sigma \log n)$ bit overhead, problematic for large alphabets. This redundancy can be eliminated by adopting a \emph{pointer-less} representation \cite{WTForALL}. The balanced tree shape is slightly modified to ensure levels are fully populated (except potentially the last, which is filled left-to-right). This allows concatenating all the bitmaps at each level into a single large bitmap $B_{level}$ for each level, and then concatenating these level bitmaps into one global bitmap $B_{global}$. Navigation replaces explicit child pointers with calculations: knowing the bitmap segment $[l, r]$ for a node $v$ at level $k$, the segment for its left child (at level $k+1$) is determined by the range $[1 + (k+1)n + rank_0(B_{global}, l-1), (k+1)n + rank_0(B_{global}, r)]$ within $B_{global}$, and similarly for the right child using $rank_1$. This eliminates the pointers, reducing the extra space to the $o(n)$ bits per level required by the rank/select structures on the bitmaps (as seen in \ref{sec:bitvectors}), achieving a total space of $n \lceil \log \sigma \rceil + o(n \log \sigma)$ bits \cite{MAKINEN2007332,MAKINEN2006703}. While space-efficient, this approach might slightly increase query times in practice due to the additional rank operations needed for navigation compared to directly following pointers.

\begin{algorithm}[h!]
    \caption{\texttt{Rank} queries on a wavelet tree}\label{alg:rank_wt}
    \begin{algorithmic}
        \Function{$\text{rank}_c$}{$S,i$}
        \State $v \gets T_{root}$ \Comment{\small{start at the root node}}
        \State $[a,b] \gets [1,\sigma]$
        \While {$a \neq b$}
        \If {$c \leq \lfloor (a+b)/2 \rfloor$}
        \State $i \gets rank_0(v.B,i)$
        \State $v \gets v.left$ \Comment{\small{move to the left child of node $v$}}
        \State $b \gets \lfloor (a+b)/2 \rfloor$
        \Else
        \State $i \gets rank_1(v.B,i)$
        \State $v \gets v.right$ \Comment{\small{move to the right child of node $v$}}
        \State $a \gets \lfloor (a+b)/2 \rfloor +1$
        \EndIf
        \EndWhile
        \State \Return $i$
        \EndFunction
    \end{algorithmic}

\end{algorithm}

\subsection{Compressed Wavelet Trees} \label{sec:compressed_WT}
In order to make the wavelet tree more space efficient, we ask ourselves if we can compress this data structure. The answer is yes, and in this section we will see how a wavelet tree can be compressed to the zero-order entropy of the input string, while still being able to answer rank and select queries in $O(\log \sigma)$ time. The literature on this topic mainly focus one two different approaches: compressing the bitvectors and altering the shape of the wavelet tree itself.

\subsubsection{Compressing the bitvectors} \label{subsec:compressing_bitvectors}
In \cite{GrossiWT2003} Grossi et. al showed that if the bitvectors of each single node are compressed to their zero-order entropy, then their overall space occupance is $n H_0(S)$. So if we suppose that the bitmap associated to the root node has a skewed distribution of $0$s and $1$s, then the zero-order compressing it yields a space of
\begin{equation}
    n_0 \log \frac{n}{n_0} + n_1 \log \frac{n}{n_1}
\end{equation}
where $n_0$ and $n_1$ are the number of $0$s and $1$s in the bitmap, respectively. This is the same as the zero-order entropy of the bitmap. The same reasoning can be applied to the bitmaps of the children of the root node, and so on. This way, one can easy prove by induction \cite{navarro2016compact} that the overall space of the wavelet tree is
\begin{equation}
    \sum_{c \in \Sigma} n_c \log (\frac{n}{n_c}) = n H_0(S)
\end{equation}

\noindent We can now choose from the literature any zero-order entropy coding method for the bitvectors that supports rank and select queries in $O(1)$ time. Some of the most popular methods are RRR \cite{RRR2002} that we have vastly discussed in \autoref{sec:bitvectors}, that for each bitvector of length $n$ uses $n H_0(B) + o(n \log \log n / \log n)$ bits. In \cite{patrascu2008succincter} the authors showed\footnote{In this case, the time complexity of rank and select queries is $O(c)$.} that this value can be further reduced to $n H_0(B) + o(n/\log^c n)$ for any positive constant $c$.

\subsubsection{Huffman-Shaped Wavelet Trees} \label{subsec:huffman_shaped_wavelet_trees}
Since working in practice with compressed bitvectors can be less efficient then in theory, we want a method for still obtaining nearly zero-order entropy compression, but while maintaining the bitvectors in plain form. The key idea for the compression method that we are going to analyze is that, as noted by Grossi et. al in \cite{grossi2004indexing}, the shape of the wavelet tree has no impact on the space occupance of the structure. They proposed to use this fact to alter the shape of the tree in order to optimize the average query time. Recalling how we built an Huffman Tree in \ref{subsec:huffman_coding}, we can adapt the same idea to the wavelet tree: given the frequencies $f_c$ with which each leaf node appears in the tree, we can create an Huffman-shaped wavelet tree, obtaining an average access time of
\begin{equation}
    \sum_{c \in \Sigma} f_c \log \frac{1}{f_c} \leq \log \sigma
\end{equation}
A counter effect noted by the authors in \cite{grossi2004indexing} is that in the worst case, we could and up with a time complexity of $O(\log n)$, for example in the case of a very infrequent symbol.However, if we choose $i$ uniformly at random from $[1, n]$ then the average access\footnote{And also for $rank_c(S,j)$ or $select_c(S,j)$ with $c = S=[1]$} time is
\begin{equation}
    O(\frac{1}{n} \sum_c f_c |h(c)| = O(1 + H_0(S))
\end{equation}
That is better than the $O(\log \sigma)$ time of the original balanced wavelet tree.

\begin{remark}
    On a further note, if we bound the depth of the Huffman Tree, we can keep worst care access time to $O(\log \sigma)$, with extra $O(n/\sigma)$ bits of redundancy
\end{remark}

\noindent Another possibile approach following the same idea of a Huffman-shaped wavelet tree, proposed in \cite{makinen2004new}, is to use the frequencies with which the symbols appear in the string. If we use this frequencies to build the Huffman Tree, we can then attach to each node $v$ a bitvector $B_v$ in the same way that we would do for the balanced wavelet tree (\ref{alg:build_wt}). In this way, the bits of $B_v$ are the bits of of the path from the root to $v$ in the Huffman Tree, i.e the Huffman codes of the symbols. Let's see the space occupance of this structure. Consider a leaf corresponding to a symbol $c$, at depth $|h(c|)$ (where $h(c)$ is the bitwise Huffman code for $c$), representing $f_c$ symbols. Each of this occurrences leads to a bit in each bitvector that is in the path from the root to the leaf; that is $|h(c)|$ bits. Thus, the occurrences of $c$ lead to $f_c |h(c)|$ bits in total. If we add this values to all the leaves we obtain same number of bits outputted by the Huffman coding of the string, that is
\begin{equation}
    \sum_{c \in \Sigma} f_c |h(c)| \leq n(H_0(S) + 1)
\end{equation}
If we also want to add the space to support the rank and select queries and the tree pointers needed to navigate the tree, we arrive to a space occupance of
\begin{equation}
    n(H_0(S) + 1) + o(n(H_0(S) + 1)) + O(n \log \sigma)
\end{equation}
For the sake of completeness, we also mention that the shape of an Huffman tree is not the only one that can be given to a wavelet tree. In \cite{grossi2012wavelet} Grossi and Ottaviano gave the wavelet the shape of a trie, making it possible to handle a sequence of strings.

\subsubsection{Higher Order Entropy Coding}

While compressing wavelet trees to their zero-order entropy $H_0(S)$ is effective, many sequences exhibit further compressibility captured by higher-order entropy $H_k(S)$. This measure leverages the statistical dependencies between symbols by considering the context of $k$ preceding symbols (as defined in Section \ref{sec:higher_order_entropy}). Achieving $H_k$ compression often involves the Burrows-Wheeler Transform (\textsc{Bwt}) \cite{burrows1994block}, a reversible permutation that groups symbols with similar preceding contexts together in the transformed string, denoted $S^{\textsc{Bwt}}$. Compressing the resulting runs or substrings ($S_A$ for each context $A$) within $S^{\textsc{Bwt}}$ to their respective zero-order entropies effectively achieves $H_k(S)$ for the original sequence $S$ \cite{manzini2001analysis}.

Early applications connecting wavelet trees and $H_k$ followed this logic. Grossi et al. \cite{GrossiWT2003}, compressed context-based subsequences using wavelet trees. Ferragina et al. \cite{ferragina2004alphabet} explicitly used the \textsc{Bwt} output and partitioned it, applying wavelet trees to compress the parts corresponding to different contexts, thereby reaching $nH_k(S) + o(n \log \sigma)$ bits.

A significant advancement came with the realization, notably explored by Grossi et al. in 2004 \cite{grossi2004indexing}, that partitioning $S^{\textsc{Bwt}}$ might not be necessary. A single wavelet tree built over the entire $S^{\textsc{Bwt}}$, equipped with appropriate compression schemes for its internal bitmaps, could achieve $H_k$ compression directly. Their initial approach using run-length encoding on the bitmaps yielded approximately $2nH_k(S)$ bits plus redundancy.

Subsequently, Mäkinen and Navarro \cite{navarro2007compressed} demonstrated that using more sophisticated bitmap representations, such as the fully indexable dictionaries of Raman et al. \cite{RRR2002} (mentioned in Section \ref{subsec:compressing_bitvectors}), within the single wavelet tree over $S^{\textsc{Bwt}}$ allowed reaching the theoretically tighter bound of $nH_k(S) + o(n \log \sigma)$ bits, while still supporting efficient rank/select operations needed for \textsc{Bwt}-based indexing.

Ferragina and Manzini \cite{TheMyriadVirtuesWT} provided a thorough analysis of wavelet tree variants for compression, comparing bitmap encodings like Run-Length Encoding (RLE) and Gap Encoding (GE). Their findings supported the suitability of RLE-based approaches (like the one in \cite{grossi2004indexing}) for achieving $H_k$ in conjunction with the \textsc{Bwt}. More recently, simpler strategies have also proven effective; Kärkkäinen and Puglisi \cite{karkkainen2011fixed} showed that partitioning $S^{\textsc{Bwt}}$ into fixed blocks and using Huffman-shaped wavelet trees (Section \ref{subsec:huffman_shaped_wavelet_trees}) on each block can practically achieve $H_k$ compression, offering a potentially faster alternative.

\subsection{Wavelet Matrices and Quad Vectors}

While the pointerless and Huffman-shaped wavelet trees offer significant space advantages over the basic pointer-based structure, they can still face challenges. Pointerless trees, especially the strict variant, require extra rank operations for navigation, potentially slowing down queries compared to pointer-based trees in practice, despite having the same asymptotic complexity \cite{claude2015wavelet}. Furthermore, for very large alphabets $\sigma$, even the overhead associated with Huffman models or the $o(n \log \sigma)$ term in pointerless structures might be undesirable. Moreover, a primary bottleneck for query performance in practice, especially with large sequences that do not fit in cache, is memory latency. Each level traversal in a standard wavelet tree typically incurs cache misses, leading to a query time proportional to $\log \sigma$ times the latency of a cache miss.

Two main approaches have emerged to address these issues: the Wavelet Matrix and the use of higher-arity trees (specifically 4-ary or Quad trees).

\subsubsection{The Wavelet Matrix}
Introduced by Claude, Navarro, and Ordóñez \cite{claude2015wavelet}, the wavelet matrix offers an alternative representation that retains the functionality of the pointerless wavelet tree but simplifies navigation and often improves practical performance, particularly for large alphabets.

The core idea is to decouple the alignment between parent and child bitmap segments. Instead of ensuring that the bits corresponding to a node $v$ at level $l$ map to a contiguous block within the level-$(l+1)$ bitmap $B_{l+1}$ that aligns with $v$'s conceptual children, the wavelet matrix uses a global reordering at each level. All the 0-bits from the level-$l$ bitmap $B_l$ are mapped to the beginning of the level-$(l+1)$ bitmap $B_{l+1}$, followed by all the 1-bits from $B_l$. To navigate, we only need to know the total number of 0s at level $l$, denoted $z_l$.
\begin{itemize}
    \item If $B_l[i] = 0$, the corresponding position at level $l+1$ is $rank_0(B_l, i)$.
    \item If $B_l[i] = 1$, the corresponding position at level $l+1$ is $z_l + rank_1(B_l, i)$.
\end{itemize}
This mapping eliminates the need to calculate node boundaries $[s_v, e_v]$ during traversal. Access and rank operations still require one rank query per level, just like the standard pointer-based tree, but without the pointer overhead or the extra rank operations of the strict pointerless variant. Select operations can also be implemented efficiently using this mapping.

The space requirement is identical to the pointerless wavelet tree: $n \lceil \log \sigma \rceil$ bits for the bitmaps plus $o(n \log \sigma)$ for rank/select support (the $z_l$ values require negligible $O(\log \sigma \log n)$ bits total). Experiments in \cite{claude2015wavelet} show that the wavelet matrix is significantly faster than pointerless wavelet trees in practice and competitive with, or sometimes faster than, pointer-based wavelet trees, while using substantially less space, especially for large $\sigma$. It can also be combined with bitmap compression or Huffman shaping (requiring a specific code assignment strategy \cite{claude2015wavelet} to maintain the necessary properties).


\subsubsection{4-ary (Quad) Wavelet Trees}
Addressing the latency bottleneck caused by cache misses during the $\log \sigma$ levels of traversal, Venturini et al. \cite{QWT} proposed using a 4-ary wavelet tree structure. By increasing the arity from 2 to 4, the height of the tree is roughly halved to $\lceil (\log \sigma) / 2 \rceil$.

In this structure, each internal node has four children, corresponding to partitioning the current alphabet range $[a,b]$ into four sub-ranges based on the two most significant bits differentiating symbols in that range. Consequently, the data stored at each node is not a bitmap but a quad vector - a sequence over the alphabet $\{00, 01, 10, 11\}$ (or equivalently, $\{0, 1, 2, 3\}$).

To support the standard wavelet tree operations, rank and select queries must now operate on these quad vectors. Venturini et al. \cite{QWT} developed space-efficient data structures for quad vector rank/select, adapting block-based techniques used for binary vectors. Their implementation achieves constant query times with a space overhead slightly larger than that for binary vectors (e.g., $\approx$ 6.25\%-7.81\% overhead in their experiments compared to $\approx$ 3.12\% for binary rank structures like \cite{gonzalez2005practical}).

The primary benefit is significantly reduced query latency. By halving the number of traversal steps (levels), the number of dependent memory accesses and potential cache misses is also halved. Experimental results in \cite{QWT} demonstrate that their 4-ary wavelet tree (specifically, a 4-ary wavelet matrix implementation, combining both ideas) improves the latency of rank and select queries by a factor of approximately 2 compared to widely used binary wavelet tree implementations from libraries like SDSL \cite{gog2014theory}.

The space complexity for a 4-ary wavelet tree using quad vectors is $n \lceil \log \sigma \rceil$ bits (since $n \times 2$ bits are stored per level, but there are only $\approx (\log \sigma)/2$ levels) plus the overhead for quad rank/select support, which is $o(n \log \sigma)$.
