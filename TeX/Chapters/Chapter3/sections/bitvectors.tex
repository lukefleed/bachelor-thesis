Building upon the concepts of data compression and information theory from the preceding chapters, we now address the construction of \emph{succinct data structures}. As motivated in Section~\ref{sec:why_succinct}, the objective is to represent discrete structures using space close to their information-theoretic minimum, while supporting efficient query operations directly on their compressed representation.

This chapter focuses on the fundamental \textsf{rank} and \textsf{select} operations. Given a sequence, \textsf{rank} counts occurrences of specified elements up to a given position, whereas \textsf{select} finds the position of the $i$-th occurrence of a specified element. The efficient implementation of these queries is critical for the functionality of many succinct data structures. We will investigate methods to support \textsf{rank} and \textsf{select} efficiently, typically achieving constant query time, through the use of auxiliary structures whose space requirement is sublinear relative to the input size.

Our examination proceeds in three stages. First, we consider the foundational case of \emph{bitvectors} (binary sequences). We will analyze techniques, including hierarchical decomposition methods, for constructing auxiliary structures that use $o(n)$ bits of space, where $n$ is the bitvector length, enabling constant-time \textsf{rank} and \textsf{select} queries on the original bitvector. Second, we generalize these concepts to sequences defined over larger, finite alphabets. We will study \emph{Wavelet Trees}, a structure that reduces \textsf{rank} and \textsf{select} operations on general strings to corresponding operations performed on underlying bitvectors. Finally, the chapter addresses the more recent case of \emph{degenerate strings}, which are sequences where each position may represent a subset of characters from the alphabet. We will review approaches that extend \textsf{rank} and \textsf{select} capabilities to this setting, by adapting the principles established for standard strings and bitvectors.

\section{Bitvectors} \label{sec:bitvectors}
We begin our study with the most fundamental sequence type, the \emph{bitvector} $B[1..n]$, a sequence of $n$ bits from $\{0, 1\}$. Our primary objective is to support two essential query operations on $B$ efficiently: \emph{\textsf{rank}}$_b(B, i)$, which counts the occurrences of bit $b$ in the prefix $B[1..i]$, and \emph{\textsf{select}}$_b(B, i)$, which finds the index of the $i$-th occurrence of bit $b$ in $B$. While these operations can be answered by scanning $B$ in $O(n)$ time, we seek constant-time solutions, $O(1)$, by augmenting $B$ with \emph{succinct} auxiliary data structures. These structures should occupy $o(n)$ bits, leading to a total space usage of $n+o(n)$ bits when storing $B$ explicitly. We now formally define these operations.

\begin{definition}[\textsf{Rank}]\label{def:rank}
    Given a bitvector $B[1..n]$, the \textsf{rank} of an index $i$ ($1 \le i \le n$) relative to a bit $c \in \{0, 1\}$ is the number of occurrences of $c$ in the prefix $B[1..i]$. We denote it as $rank_c(i)$.
    Specifically, for $c=1$:
    \begin{equation*}
        rank_1(i) = \sum_{j=1}^{i} B[j]
    \end{equation*}
    The rank for $c=0$ can be derived as $rank_0(i) = i - rank_1(i)$.
\end{definition}

\begin{definition}[\textsf{Select}]\label{def:select}
    Given a bitvector $B[1..n]$, the \textsf{select} of the $i$-th occurrence of a bit $c \in \{0, 1\}$ is the index $j$ such that $B[j]=c$ and $rank_c(j) = i$. We denote it as $select_c(i)$. If the $i$-th occurrence of $c$ does not exist, $select_c(i)$ is undefined (or returns a special value). Unlike \textsf{rank}, $select_0(i)$ cannot generally be computed directly from $select_1(i)$ in constant time.
\end{definition}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=0.6,
            bitnode/.style={draw, rectangle, minimum width=6mm, minimum height=6mm, inner sep=0pt, outer sep=0pt}, % Ensure outer sep is 0
            indexlabel/.style={below=2mm of #1.south, font=\footnotesize, anchor=north} % Adjusted label positioning
        ]
        % Use a pgf array for easier indexing
        \def\bitvectorarray{{1,0,1,1,0,1,0,0,1,1,0,1,0,1,1,1,0,0,1,0}}

        % Draw the first node
        \node[bitnode] (B1) at (0, 0) {\pgfmathparse{\bitvectorarray[0]}\pgfmathresult}; % Access first element
        \node[indexlabel=B1] {1};

        % Draw subsequent nodes relative to the previous one
        \foreach \idx in {1,...,19} { % Loop from index 1 to 19 (to draw nodes 2 to 20)
                \pgfmathtruncatemacro{\nodeidx}{\idx+1} % Current node index (2 to 20)
                \pgfmathtruncatemacro{\contentidx}{\idx} % Array content index (1 to 19)
                \node[bitnode, right=0pt of B\idx, anchor=west] (B\nodeidx) {\pgfmathparse{\bitvectorarray[\contentidx]}\pgfmathresult};
                \node[indexlabel=B\nodeidx] {\nodeidx};
            }

        % --- Annotations ---

        % Rank_1(15) = 9
        % Draw arrow pointing slightly above the bottom right corner of B15
        \coordinate (rank_arrow_start) at ([yshift=-9mm]B15.south east);
        \coordinate (rank_arrow_end) at ([xshift=-0.5mm,yshift=0.5mm]B15.south east); % Point just inside border
        \draw[->, red, thick] (rank_arrow_start) -- (rank_arrow_end);
        % Place text below the arrow start
        \node[red, font=\footnotesize, align=center, anchor=north] at (rank_arrow_start) {$\textsf{rank}_1(15) = 9$};
        % Highlight the prefix B[1..15] slightly
        \foreach \idx in {1,...,15} {
                \fill[red!10, opacity=0.5] (B\idx.south west) rectangle (B\idx.north east);
            }

        % Select_1(7) = 12
        % Draw arrow pointing slightly below the top middle of B12
        \coordinate (select_arrow_start) at ([yshift=5mm]B12.north);
        \coordinate (select_arrow_end) at ([yshift=-0.5mm]B12.north); % Point just inside border
        \draw[->, blue, thick] (select_arrow_start) -- (select_arrow_end);
        % Place text above the arrow start
        \node[blue, font=\footnotesize, align=center, anchor=south] at (select_arrow_start) {$\textsf{select}_1(7) = 12$};
        % Highlight the 7th '1' (at index 12)
        \draw[blue, thick, rounded corners] (B12.north west) rectangle (B12.south east);
    \end{tikzpicture}
    \caption{Example of a bitvector $B[1..20]$. The prefix $B[1..15]$ (shaded red) contains 9 ones, so $\textsf{rank}_1(15)=9$. The 7th $1$ (circled blue) occurs at index 12, so $\textsf{select}_1(7)=12$.}
    \label{fig:bitvector_example}
\end{figure}

\begin{example}
    Let $B$ be the bitvector of length $n=20$ shown in Figure \ref{fig:bitvector_example}:
    \begin{equation*}
        B = 10110100110101110010
    \end{equation*}
    \begin{itemize}
        \item $\textsf{rank}_1(15)$: We count the number of 1s in the prefix $B[1..15]$.
              \begin{equation*}
                  B[1..15] = \underbrace{101101001101011}_{15 \text{ bits}}
              \end{equation*}
              By scanning, there are 9 ones. Therefore, $\textsf{rank}_1(15) = 9$.

        \item $\textsf{select}_1(7)$: We find the index of the 7th occurrence of $1$ in $B$.
              Scanning $B$: the 1st $1$ is at index 1, 2nd at 3, 3rd at 4, 4th at 6, 5th at 9, 6th at 10, and the 7th $1$ is at index 12.
              Therefore, $\textsf{select}_1(7) = 12$.
    \end{itemize}
    % These manual calculations illustrate the definitions. Efficient data structures avoid such linear scans.
\end{example}

Bitvectors supporting efficient \textsf{rank} and \textsf{select} operations are indeed foundational components for many compressed and succinct data structures. Attaining high performance for these operations is therefore a central concern. The following sections will describe methods to construct the $o(n)$-bit auxiliary structures that achieve constant query times. Furthermore, it is often the case that bitvectors encountered in applications exhibit skewed distributions of $0$s and $1$s (i.e., they are sparse). While the $n+o(n)$ structures operate on the explicit bitvector, separate lines of research have explored compressing the bitvector $B$ itself by leveraging these statistical properties, thereby reducing the initial $n$-bit storage requirement.

% Significant research, exemplified by the work of Sadakane and Grossi \cite{sadakane2006squeezing}, has addressed the integration of compression with query support. Such approaches aim to achieve space bounds related to the empirical entropy of the bitvector, for instance $n\mathcal{H}_k(B) + o(n)$ bits using $k$-th order entropy, while simultaneously maintaining constant query times for \textsf{rank} and \textsf{select}. This direction differs from the direct application of general-purpose compression algorithms presented in Chapter \ref{ch:Chapter2}.

\begin{remark}
    Applying standard symbol-wise coding techniques from Chapter \ref{ch:Chapter2} (such as Huffman or Arithmetic coding) directly to a bitvector $B$ typically yields a compressed size related to its zero-order entropy, approximately $n\mathcal{H}_0(B)$ bits. While potentially reducing space, especially for biased bitvectors, this form of compression generally obstructs efficient random access and the direct computation of \textsf{rank} and \textsf{select} queries without significant decompression overhead. The specialized structures detailed in this chapter are expressly designed to provide both space efficiency and fast query capabilities.
\end{remark}

\subsection{\textsf{Rank}} \label{subsec:rank}

A fundamental approach to support \textsf{rank} queries in constant time using sublinear additional space was introduced by Jacobson \cite{jacobson1988succinct}. The technique relies on a hierarchical decomposition of the bitvector $B[1..n]$ and precomputation of ranks at different granularities. The auxiliary structures occupy $o(n)$ bits in total.

The structure typically employs two levels of blocking on top of the original bitvector $B$. First, $B$ is conceptually divided into \emph{superblocks} of size $Z$. Second, each superblock is further divided into \emph{blocks} of size $z$. Common parameter choices yielding $o(n)$ overhead are $Z = \Theta(\log^2 n)$ and $z = \Theta(\log n)$, for example $Z = \lfloor \log^2 n \rfloor$ and $z = \lfloor (1/2) \log n \rfloor$. We assume for simplicity that $z$ divides $Z$ and $Z$ divides $n$.

Two auxiliary arrays store precomputed rank information. The first array, $R_S$, stores the absolute rank at the beginning of each superblock:
\begin{equation*}
    R_S[k] = \textsf{rank}_1(B, k \cdot Z) \qquad k = 0, \dots, n/Z - 1
\end{equation*}
The second array, $R_B$, stores the rank within a superblock at the beginning of each block, relative to the start of the superblock. Specifically, for the $l$-th block overall, which belongs to superblock $k = \lfloor l \cdot z / Z \rfloor$
\begin{equation*}
    R_B[l] = \textsf{rank}_1(B, l \cdot z) - \textsf{rank}_1(B, k \cdot Z)
\end{equation*}
Finally, a \emph{lookup table}, often denoted $T$, is used to determine the rank within a small block of size $z$. For every possible $z$-bit pattern $p$, and every position $j \in [1, z]$, $T[p][j]$ stores the value $\textsf{rank}_1(p, j)$, i.e., the number of set bits in the first $j$ positions of the pattern $p$.

Figure \ref{fig:RRR} shows a visual representation of the hierarchical data structure.

\begin{figure}[hbtp]
    \begin{flushright}
        \begin{tikzpicture}[scale=0.5] % Adjust the scale as needed
            \foreach \x/\bit in {0/\footnotesize{\dots}, 1/0, 2/1, 3/0, 4/1, 5/0, 6/1, 7/0, 8/1, 9/0, 10/1, 11/0, 12/1, 13/0, 14/1, 15/0, 16/1, 17/0, 18/1, 19/\footnotesize{\dots}} {
                    \ifnum\x<1
                        \draw (\x,0) rectangle (\x+1,1) node[midway] {\bit};
                    \else
                        \ifnum\x<10
                            \draw[fill=lightgray] (\x,0) rectangle (\x+1,1) node[midway] {\bit};
                        \else
                            \draw (\x,0) rectangle (\x+1,1) node[midway] {\bit};
                        \fi
                    \fi
                }
            % Add dashed lines
            \draw[dashed] (1,-1) -- (1,2);
            \draw[dashed] (10,-1) -- (10,2);
            \draw[dashed] (19,-1) -- (19,2);

            % add double arrow
            \draw[<->] (1,-0.5) -- (10,-0.5) node[midway, below] {\footnotesize{$Z=9$}};
            \draw[<->] (10,-0.5) -- (19,-0.5) node[midway, below] {\footnotesize{$Z=9$}};

            % Add label over the blocks 2, 10
            \node[above] at (1.5,1) {\footnotesize{$r_i$}};
            \node[above] at (11,1) {\footnotesize{$r_{i+1}$}};

            % Add one line that starts from the bottom angle of block 1, and goes down inclined
            \draw[-] (1,0) -- (-4,-3);
            \draw[-] (10,0) -- (15,-3); % Adjusted target for second arrow

            \foreach \x/\bit in {0/0, 1/1, 2/0, 3/1, 4/0, 5/1, 6/0, 7/1, 8/0} {
                    \draw (\x*2-3.5,-5) rectangle (\x*2-1.5,-4) node[midway] {\bit};
                }

            % Add dashed lines
            \draw[dashed] (-3.5,-6) -- (-3.5,-3);
            \draw[dashed] (2.5,-6) -- (2.5,-3);
            \draw[dashed] (8.5,-6) -- (8.5,-3);
            \draw[dashed] (14.5,-6) -- (14.5,-3); % Adjusted target for second arrow line end point

            % add double arrow
            \draw[<->] (-3.5,-5.5) -- (2.5,-5.5) node[midway, below] {\footnotesize{$z=3$}};
            \draw[<->] (2.5,-5.5) -- (8.5,-5.5) node[midway, below] {\footnotesize{$z=3$}};
            \draw[<->] (8.5,-5.5) -- (14.5,-5.5) node[midway, below] {\footnotesize{$z=3$}};

            % Add label over the blocks 2, 10
            \node[above] at (-2.2,-4) {\footnotesize{$r_{i,0} = 0$}};
            \node[above] at (3.8,-4) {\footnotesize{$r_{i,1} = 1$}};
            \node[above] at (9.8,-4) {\footnotesize{$r_{i,2} = 3$}};


            \foreach \x/\bit in {-4.5/0, 1.5/2, 7.5/3, 13.5/5} { % Coordinates need adjustment if representing LUT
                    \draw (\x,-8) rectangle (\x+2,-7) node[midway] {\bit}; % Example values
                }
        \end{tikzpicture}
    \end{flushright}
    \caption{The hierarchical \textsf{rank} data structure. The first level is composed of superblocks of size $Z$ (storing absolute ranks $r_i$), the second level of blocks of size $z$ (storing relative ranks $r_{i,j}$), and the third level uses a lookup table (represented by the bottom row).} \label{fig:RRR}
\end{figure}


The lookup table $T$ allows determining the number of set bits within any prefix of a $z$-bit block in constant time. Table \ref{tab:lookup} shows an example for $z=3$. For a block $p$, the entry in column $j$ (corresponding to $\textsf{rank}_1(p, j)$) gives the precomputed result.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|}
        % \hline
        %                & \multicolumn{3}{c|}{cumulative rank at bit index}                                           \\
        \hline
        block & $\mathbf{r_{i,0}}$ & $\mathbf{r_{i,1}}$ & $\mathbf{r_{i,2}}$ \\
        \hline
        000   & 0                  & 0                  & 0                  \\
        001   & 0                  & 0                  & 1                  \\
        010   & 0                  & 1                  & 1                  \\
        011   & 0                  & 1                  & 2                  \\
        100   & 1                  & 1                  & 1                  \\
        101   & 1                  & 1                  & 2                  \\
        110   & 1                  & 2                  & 2                  \\
        111   & 1                  & 2                  & 3                  \\
        \hline
    \end{tabular}
    \caption{Example of a lookup table $T$ for intra-block rank computation with $z = 3$. Each row corresponds to a possible $z$-bit pattern $p$. The cell for pattern $p$ and index $j$ stores $\textsf{rank}_1(p, j)$.} \label{tab:lookup}
\end{table}

We can state the following theorem regarding the space and time complexity \cite{Jacobson}.

\begin{theorem} \label{th:rank}
    Given a bitvector $B[1..n]$, there exists an auxiliary data structure using $o(n)$ bits that allows computing $\textsf{rank}_b(B, i)$ for any $i \in [1,n]$ and $b \in \{0,1\}$ in $O(1)$ time. The original bitvector $B$ is accessed only in read mode. The total space required is $n + o(n)$ bits.
\end{theorem}
\begin{proof}
    We construct the auxiliary data structure using a standard two-level hierarchical decomposition of the bitvector $B$. We define a \emph{superblock} size $Z = \lfloor \log^2 n \rfloor$ and a \emph{block} size $z = \lfloor (1/2) \log n \rfloor$. For analytical simplicity, assume $n$ is a multiple of $Z$, and $Z$ is a multiple of $z$.

    The structure comprises three main components designed to store precomputed rank information at different scales.

    First, the \emph{Superblock Rank Directory ($R_S$)} is an array storing the absolute rank at the start of each superblock. Specifically, for $k \in [0, n/Z - 1]$, $R_S[k] = \textsf{rank}_1(B, k \cdot Z)$. Since each rank value is at most $n$, storing these requires $\lceil \log n \rceil$ bits per entry. The total space for $R_S$ is:
    \begin{align*}
        \text{Space}(R_S) & = \frac{n}{Z} \lceil \log n \rceil = \frac{n}{\lfloor \log^2 n \rfloor} \lceil \log n \rceil \\
                          & = O\left(\frac{n \log n}{\log^2 n}\right) = O(n / \log n) \text{ bits}.
    \end{align*}

    Second, the \emph{Block Rank Directory ($R_B$)} stores ranks relative to the start of the containing superblock. For each block index $l \in [0, n/z - 1]$, let $k = \lfloor l \cdot z / Z \rfloor$ be its superblock index. $R_B[l]$ stores the relative rank $\textsf{rank}_1(B, l \cdot z) - \textsf{rank}_1(B, k \cdot Z)$. This value is at most $Z$, requiring $\lceil \log Z \rceil$ bits per entry. The total space for $R_B$ is:
    \begin{align*}
        \text{Space}(R_B) & = \frac{n}{z} \lceil \log Z = \frac{n}{\lfloor (1/2) \log n \rfloor} \lceil \log(\lfloor \log^2 n \rfloor) \rceil \\
                          & = O\left(\frac{n}{\log n} \log(\log^2 n)\right)                                                                   \\
                          & = O\left(\frac{n \log \log n}{\log n}\right) \text{ bits}.
    \end{align*}

    We verify that the space complexity for both directories $R_S$ and $R_B$ is sublinear, i.e., $o(n)$. For $R_S$, we have $O(n / \log n)$. As $n \to \infty$, $(n / \log n) / n = 1 / \log n \to 0$. For $R_B$, we have $O(n \log \log n / \log n)$. As $n \to \infty$, $(n \log \log n / \log n) / n = \log \log n / \log n \to 0$. Thus, both Space$(R_S)$ and Space$(R_B)$ are $o(n)$.

    Third, the \emph{Intra-Block Rank Mechanism ($T$)} is responsible for determining the rank within any $z$-bit block in constant time. A direct precomputation approach involves storing, for each of the $2^z$ possible $z$-bit patterns $p$ and each position $j \in [1, z]$, the value $\textsf{rank}_1(p, j)$. The space requirement for such a naive table would be:
    \begin{equation*}
        \text{Space}(\text{Naive } T) = O(2^z \cdot z \cdot \log z).
    \end{equation*}
    Substituting $z = \lfloor (1/2) \log n \rfloor$, this becomes
    \begin{equation*}
        O(2^{(1/2)\log n} \log n \log \log n) = O(\sqrt{n} \log n \log \log n)
    \end{equation*}
    which is not sublinear ($o(n)$). The crucial insight, originally provided by Jacobson \cite{Jacobson}, is that this intra-block rank functionality can indeed be implemented using auxiliary structures occupying only $o(n)$ bits while still supporting $O(1)$ query time. This often involves techniques like further table compression or specialized indexing strategies not detailed here, but whose existence and performance guarantees we rely upon.

    The total auxiliary space is the sum of the space complexities for $R_S$, $R_B$, and the efficient implementation of $T$. This sum is $O(n / \log n) + O(n \log \log n / \log n) + o(n)$, which simplifies to $o(n)$. Therefore, the total space including the original bitvector is $n + o(n)$ bits.
\end{proof}

It is worth noting a crucial optimization for practical implementations, especially when the block size $z$ is chosen such that it fits within a machine word (e.g., $z \le 64$ on standard 64-bit architectures). In this scenario, the theoretical $o(n)$-bit lookup mechanism $T$ for intra-block rank can often be replaced by direct computation using highly optimized hardware instructions.

Specifically, to compute the rank within the $z$-bit block $p = B[l \cdot z + 1 .. (l+1) \cdot z]$ up to position $j$, i.e., $\textsf{rank}_1(p, j)$, one can perform bitwise operations. First, isolate the $j$-bit prefix of $p$\footnote{For example, using a bitmask like $p \ \& \ ((1 \ll j) - 1)$}. Then, the number of set bits in this prefix can be computed efficiently using the processor's population count (popcount) instruction. Modern programming languages often expose this functionality directly; for instance, the Rust standard library provides the \texttt{count\_ones()} method on primitive integer types. This operation is typically executed in constant time (often a single machine instruction) and can be significantly faster in practice than accessing a more complex precomputed table structure, especially for small values of $z$.  We will discuss this in more detail in Section \ref{subsec:practical_considerations}.

To answer a query $\textsf{rank}_1(B, i)$, we perform the following constant-time steps:
Calculate the relevant indices: superblock $k = \lfloor (i-1)/Z \rfloor$, block $l = \lfloor (i-1)/z \rfloor$, and intra-block position $j = (i-1) \pmod z + 1$.
Retrieve the precomputed ranks: $rank_S = R_S[k]$ from the superblock directory and $rank_B = R_B[l]$ from the block directory.
Access the $z$-bit block $p = B[l \cdot z + 1 .. (l+1) \cdot z]$ from the original bitvector $B$. This access takes $O(1)$ time on a Word RAM where the word size $w \ge \log n \ge z$.
Compute the intra-block rank $rank_T = T(p, j)$ using the $o(n)$-space constant-time mechanism $T$.
The final rank is obtained by summing these components:
\begin{equation*}
    \textsf{rank}_1(B, i) = rank_S + rank_B + rank_T.
\end{equation*}
All steps involve only constant-time operations (arithmetic, array lookups, memory access of $z$ bits, and the $T$ lookup), hence the total query time is $O(1)$.

The query $\textsf{rank}_0(B, i)$ is then computed simply as $i - \textsf{rank}_1(B, i)$, also in constant time.

\subsection{\textsf{Select}} \label{subsec:select}

The \textsf{select} operation serves as the inverse to \textsf{rank}. Formally, for a bitvector $B$ and an integer $i$, $\textsf{select}_b(B, i)$ returns the index $j$ such that $B[j] = b$ and $\textsf{rank}_b(B, j) = i$. This relationship can be expressed as:
\begin{equation*}
    \textsf{rank}_b(B, \textsf{select}_b(B, i)) = i
\end{equation*}
provided the $i$-th occurrence of $b$ exists.

Supporting \textsf{select} queries efficiently requires a different auxiliary structure compared to the fixed-size blocking used for \textsf{rank}. An approach providing constant-time select within this space framework was developed by Clark \cite{clark1997compact}. It relies on a multi-level hierarchy guided by the number of set bits (specifically, 1s, as \textsf{select}$_0$ can be handled symmetrically or via \textsf{select}$_1$ on the complemented bitvector with additional rank structures). The core idea is to partition the bitvector $B$ based on the cumulative count of 1s and use multiple levels of indexing structures to locate the $i$-th 1 quickly.

We begin by designing the first level of the select data structure. The bitvector $B$ is conceptually divided into variable-length \emph{chunks}, such that each chunk (except possibly the last) contains exactly $K$ set bits. A typical choice is $K = \Theta(\log n \log \log n)$. We store an array $P_1$ containing the starting position (index in $B$) of each chunk. The number of chunks is $\lceil m/K \rceil$, where $m = \textsf{rank}_1(B, n)$ is the total number of 1s. The space for $P_1$ is
\begin{equation*}
    O(m/K \cdot \log n) = O(n / (\log n \log \log n) \cdot \log n) = O(n / \log \log n)
\end{equation*}
which is $o(n)$. Given a query $\textsf{select}_1(i)$, the relevant chunk index can be determined as $k = \lceil i/K \rceil$, and its starting position retrieved from $P_1$.

The second step addresses how to find the target 1 within its chunk. Let the length of a chunk be $Z$. We categorize chunks into two types based on their density. A chunk is considered \emph{sparse} if its length $Z$ is large compared to $K$, specifically $Z > K^2$. Conversely, a chunk is \emph{dense} if $Z \le K^2$. For sparse chunks, we can afford to store the relative positions (offsets from the chunk's start) of the $K$ set bits explicitly. The total space required across all sparse chunks for these offsets can be shown to be $o(n)$ \cite{clark1997compact}. If the target 1 falls within a sparse chunk, its position is found by calculating its relative rank $i' = (i-1) \pmod K + 1$ and retrieving the $i'$-th stored offset. For dense chunks ($Z \le K^2$), explicitly storing offsets is too costly. Instead, we introduce a second level of structure within these dense chunks.

This second level structure subdivides each dense chunk into smaller variable-length \emph{sub-chunks}, each containing exactly $k$ set bits, where $k = \Theta((\log \log n)^2)$. We store an array $P_2$ for each dense chunk, holding the starting positions of these sub-chunks relative to the start of the dense chunk. The total space for all $P_2$ structures across all dense chunks is $O(m/k \cdot \log Z_{max}) = O(n/k \cdot \log K^2) = O(n / (\log \log n)^2 \cdot \log(\log n \log \log n)) = o(n)$.

Finally, we need to handle the sub-chunks. Let a sub-chunk have length $z$. Similar to the chunk level, we distinguish between \emph{sparse sub-chunks} ($z > k^2$) and \emph{dense sub-chunks} ($z \le k^2$). For sparse sub-chunks, we again store the relative positions of the $k$ set bits explicitly; the total space across all sparse sub-chunks remains $o(n)$ \cite{clark1997compact}. For dense sub-chunks ($z \le k^2$), we require a mechanism to find the $i''$-th 1 (where $i''$ is the rank relative to the sub-chunk start) within the $z$ bits in constant time. It is known \cite{clark1997compact} that constant-time \textsf{select} within a block of size $z=O(\text{polylog } n)$ can be achieved using an auxiliary structure of size $o(z)$ bits associated with the block (e.g., using precomputed tables or other techniques). Summing over all dense sub-chunks, the total space for these Level 3 mechanisms is $o(n)$.

The overall query process involves navigating this hierarchy. The algorithm can be summarized by the following pseudocode.

\begin{algorithm}[hbtp]
    \caption{$Select_1$ Algorithm} \label{alg:select}
    \begin{algorithmic}[1] \small
        \Function{$Select_1$}{$B, i$}
        \State $k \gets \lceil i/K \rceil$
        \State $pos_1 \gets \text{GetChunkStartPos}(k)$
        \State $ChunkInfo \gets \text{GetChunkInfo}(k)$
        \If{$ChunkInfo$ indicates \emph{sparse} ($Z > K^2$)}
        \State $i' \gets (i-1) \pmod K + 1$
        \State $offset \gets \text{GetSparseChunkOffset}(k, i')$
        \State \Return $pos_1 + offset$
        \Else
        \State $i' \gets (i-1) \pmod K + 1$
        \State $l \gets \lceil i'/k \rceil$
        \State $pos_2 \gets \text{GetSubChunkStartPos}(k, l)$
        \State $SubChunkInfo \gets \text{GetSubChunkInfo}(k, l)$
        \If{$SubChunkInfo$ indicates \emph{sparse} ($z > k^2$)}
        \State $i'' \gets (i'-1) \pmod k + 1$
        \State $offset \gets \text{GetSparseSubChunkOffset}(k, l, i'')$
        \State \Return $pos_1 + pos_2 + offset$
        \Else
        \State $i'' \gets (i'-1) \pmod k + 1$
        \State $offset \gets \text{DenseSubChunkSelect}(k, l, i'')$
        \State \Return $pos_1 + pos_2 + offset$
        \EndIf
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

As for the rank data structure, we can state the following theorem:

\begin{theorem} \label{th:select}
    Given a bitvector $B[1..n]$, there exists an auxiliary data structure using $o(n)$ bits that allows computing $\textsf{select}_b(B, i)$ for any valid $i$ and $b \in \{0,1\}$ in $O(1)$ time. The original bitvector $B$ is accessed only in read mode. The total space required is $n + o(n)$ bits.
\end{theorem}
\begin{proof}
    The constant query time follows from the algorithm described, where each step (calculating indices, accessing pointer structures $P_1, P_2$, retrieving stored offsets for sparse cases, or using the constant-time Level 3 dense mechanism) takes $O(1)$ time. The total auxiliary space is the sum of the space required for $P_1$, the relative offsets for sparse chunks, the $P_2$ arrays, the relative offsets for sparse sub-chunks, the Level 3 dense sub-chunk mechanisms, and the structures needed to distinguish between sparse and dense cases. As analyzed during the description, each component requires $o(n)$ bits with parameters $K=\Theta(\log n \log \log n)$ and $k=\Theta((\log \log n)^2)$, relying on established results \cite{clark1997compact}. Therefore, the total auxiliary space is $o(n)$ bits, yielding an overall space of $n+o(n)$ bits.
\end{proof}

\begin{remark}[Practical Considerations]
    The threshold
    \[k^2 = \Theta((\log \log n)^4)\]
    for handling the smallest dense blocks can be extremely small for practical values of $n$. In implementations, if $k$ is small enough (e.g., fits within a machine word or cache line), scanning the dense sub-chunk directly to find the $i''$-th occurrence of 1 might be faster than using the more complex theoretical $o(n)$ mechanism or precomputed tables. This again relates to broadword programming techniques, similar to the optimization mentioned for \textsf{rank}.
\end{remark}

\subsection{Compressing Sparse Bitvectors} \label{subsec:elias_fano_compression}

The \textsf{rank} and \textsf{select} structures discussed previously (\ref{subsec:rank}, \ref{subsec:select}) operate on the plain bitvector $B[1..n]$, achieving a total space occupancy of $n + o(n)$ bits. However, in many practical scenarios, the bitvector $B$ exhibits a skewed distribution, containing significantly fewer 1s than 0s (or vice-versa). Let $m = \textsf{rank}_1(B, n)$ be the total number of set bits. When $m \ll n$, storing the full $n$-bit vector is inefficient.

In such sparse settings, we can leverage compression techniques that exploit the low density of set bits. The Elias-Fano representation, previously introduced in Section~\ref{sec:elias_fano_code}, provides a highly effective method for this task. Recall that Elias-Fano encodes a monotonically increasing sequence of $m$ integers up to a maximum value $n$. We can represent the bitvector $B$ by encoding the sequence of indices $\{ i \mid B[i]=1 \}$.

As detailed by Vigna \cite{vigna2013quasi} in the context of quasi-succinct indices for information retrieval, the Elias-Fano representation achieves a space complexity of approximately $m \log_2(n/m) + O(m)$ bits. This is remarkably close to the information-theoretic lower bound for representing a subset of size $m$ from a universe of size $n$, often expressed as $n\mathcal{H}_0(B) + O(m)$ bits, where $\mathcal{H}_0(B)$ is the empirical zero-order entropy of the bitvector $B$. The crucial advantage is that the space depends primarily on $m$, the number of set bits, rather than the full length $n$, leading to significant compression when $m$ is small.

This compressed representation directly supports efficient operations. The $\textsf{select}_1(i)$ operation, finding the position of the $i$-th set bit, can typically be implemented in constant time on average, often leveraging auxiliary pointers within the Elias-Fano structure as engineered in \cite{vigna2013quasi}. However, this space efficiency comes at the cost of potentially slower $\textsf{rank}_1$ and accessing $B[i]$ operations compared to the $n+o(n)$ structures. These operations usually involve decoding parts of the Elias-Fano structure and may take $O(\log(n/m))$ time or depend on the specific implementation details \cite{navarro2016compact}. Therefore, Elias-Fano presents a compelling space-time trade-off, offering near-optimal compression for sparse bitvectors at the expense of $\textsf{rank}_1$ and access time complexity. The choice between plain bitvector structures and Elias-Fano depends critically on the sparsity of the data and the required query performance profile.

\subsection{Practical Considerations} \label{subsec:practical_considerations}

While the asymptotic analysis guarantees $O(1)$ query time and $o(n)$ extra space for the \textsf{rank} and \textsf{select} structures presented earlier, achieving high performance in practice requires careful consideration of architectural factors and constant overheads hidden in the $o(n)$ term. Memory latency, cache efficiency, and instruction-level parallelism often dominate the actual running time on modern processors.

A particularly effective approach for optimizing \textsf{rank} and \textsf{select} implementations leverages \emph{broadword programming} (also known as \textsc{Swar} - \textsc{Simd} within a Register). This technique treats machine registers as small parallel processors, performing operations on multiple data fields packed within a single word using standard arithmetic and logical instructions. Vigna \cite{vigna2008broadword} applied these techniques to \textsf{rank} and \textsf{select} queries, leading to highly efficient practical implementations.

The \texttt{rank9} structure proposed by Vigna \cite{vigna2008broadword} exemplifies this approach. It employs a two-level hierarchy, similar in concept to the structure in Section~\ref{subsec:rank}, but critically relies on broadword algorithms for the final \textsf{rank} computation within a machine word (specifically, sideways addition or population count). Instead of large precomputed lookup tables for small blocks, \texttt{rank9} uses carefully designed constants and bitwise operations (detailed in Algorithm 1 of \cite{vigna2008broadword}) to compute the \textsf{rank} within a 64-bit word quickly. This typically involves storing relative counts for sub-blocks (e.g., seven 9-bit counts within a 64-bit word) in the second level. The advantages of this approach include speed, resulting from the exploitation of fast register operations and the avoidance of large table lookups, often outperforming other methods in practice. It also offers space efficiency, requiring relatively low overhead (typically around 25\% on top of the original bitvector $B$) mainly for the cumulative counts. Furthermore, broadword algorithms are generally branch-free, benefiting performance on modern pipelined processors by avoiding potential misprediction penalties.

Similarly, Vigna \cite{vigna2008broadword} developed broadword algorithms for selection within a word (Algorithm 2 in the paper). The companion \texttt{select9} structure integrates these intra-word selection capabilities with a multi-level inventory scheme. The objective of \texttt{select9} is to support high-performance selection queries, often achieving near constant-time execution, through hierarchical indexing combined with efficient broadword search for the final location. This capability involves an additional space cost, typically measured at approximately 37.5\% relative to the \texttt{rank9} structure.

Furthermore, a major bottleneck in \textsf{rank}/\textsf{select} operations is often memory access latency. To mitigate this, \emph{interleaving} the auxiliary data structures is highly recommended. For instance, storing a first-level (superblock) rank count immediately followed by its corresponding second-level (sub-block) counts increases the probability that all necessary auxiliary information for a query resides within the same cache line. This simple layout optimization can dramatically reduce cache misses compared to storing different levels of the hierarchy in separate arrays.
