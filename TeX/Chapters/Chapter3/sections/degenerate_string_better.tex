\clearpage
\section{Improvements Over Previous Methods} \label{sec:degenerate_string_better}

In the previous section \ref{sec:subset_wavelet_trees} we have seen how in \cite{SubsetWT} the authors introduced the Subset Wavelet Tree (SWT) to solve the subset rank-select problem. Their data-structure supports both \texttt{subset-rank} and \texttt{subset-select} queries in $O(\log \sigma)$ time and uses $2 (\sigma - 1)n + o(n\sigma)$ bits of space in the general case. \vspace{1em}

\noindent In this section, we will detail the significant improvements made by Bille et. al in \cite{bille2023rank} over the previous methods. The authors introduce a series of novel reductions and data structures that not only enhance the theoretical bounds but also demonstrate substantial empirical improvements. \vspace{1em}

\noindent They made three significant contributions in this context First, they introduced the parameter $N$ and revisited the problem through reductions to the regular rank-select problem, deriving flexible complexity bounds based on existing rank-select structures, as detailed in Theorem~\ref{thm:general_upper_bound}. Second, they established a worst-case lower bound of $N\log \sigma - o(N\log \sigma)$ bits for structures supporting \texttt{subset-rank} or \texttt{subset-select}, and demonstrated that, by leveraging standard rank-select structures, their bounds often approach this lower limit while maintaining optimal query times (Theorem \ref{thm:space_lower_bound}). Lastly, they implemented and compared their reductions to prior implementations, achieving twice the query speed of the most compact structure from \cite{SubsetWT} while maintaining comparable space usage. Additionally, they designed a vectorized structure that offers a 4-7x speedup over compact alternatives, rivaling the fastest known solutions.

% The authors further demonstrate that by plugging a standard rank-select data structure into Theorem~\ref{thm:general_upper_bound}, they can achieve exponential improvements in query times while essentially matching or improving space usage compared to the Subset Wavelet Tree. For instance, using the rank-select structure by Golynski, Munro, and Rao~\cite{GMR06}, which uses $\ell\log\sigma + o(\ell\log\sigma)$ bits and supports \Rank{} in $O(\log\log \sigma)$ time and select in constant time, they achieve an $N\log \sigma + N + o(N\log \sigma + N)$ bit data structure supporting \Drank{} in $O(\log\log \sigma)$ time and \Dselect{} in constant time. This significantly improves the constant on the space bound from $2$ to $1 + 1/\log \sigma$ and the query time from $O(\log \sigma)$ for both queries to $O(\log\log \sigma)$ for \Drank{} and constant for \Dselect{}.

\begin{theorem}[General Upper Bound]
    \label{thm:general_upper_bound}
    Let $X$ be a degenerate string of length $n$, size $N$, and containing $n_0$ empty sets over an alphabet $[1, \sigma]$. Let $\mathcal{D}$ denote a $\mathcal{D}_b(\ell, \sigma)$-bit data structure for a length-$\ell$ string over $[1, \sigma]$, supporting:
    \begin{itemize}
        \item \Rank{} queries in $\mathcal{D}_r(\ell, \sigma)$ time, and
        \item \Select{} queries in $\mathcal{D}_s(\ell, \sigma)$ time.
    \end{itemize}
    The subset rank-select problem on $X$ can be solved under the following conditions:

    \begin{enumerate}[label=(\roman*)]
        \item \textbf{Case $n_0 = 0$:}
              The structure requires:
              \[
                  \mathcal{D}_b(N, \sigma) + N + o(N) \text{ bits,}
              \]
              and supports:
              \[
                  \Drank{} \text{ in } \mathcal{D}_r(N, \sigma) + O(1) \text{ time,}
              \]
              \[
                  \Dselect{} \text{ in } \mathcal{D}_s(N, \sigma) + O(1) \text{ time.}
              \]

        \item \textbf{Case $n_0 > 0$:}
              The bounds from case (i) apply with the following substitutions:
              \[
                  N' = N + n_0 \quad \text{and} \quad \sigma' = \sigma + 1.
              \]

        \item \textbf{Alternative Bound:}
              The structure uses additional $\mathcal{B}_b(n, n_0)$ bits of space and supports:
              \[
                  \Drank{} \text{ in } \mathcal{D}_r(N, \sigma) + \mathcal{B}_r(n, n_0) \text{ time,}
              \]
              \[
                  \Dselect{} \text{ in } \mathcal{D}_s(N, \sigma) + \mathcal{B}_s(n, n_0) \text{ time.}
              \]
              Here, $\mathcal{B}$ refers to a data structure for a length-$n$ bitstring containing $n_0$ \texttt{1}s, which:
              \begin{itemize}
                  \item uses $\mathcal{B}_b(n, n_0)$ bits,
                  \item supports $\Rank{}(\cdot, \texttt{1})$ in $\mathcal{B}_r(n, n_0)$ time, and
                  \item supports $\Select{}(\cdot, \texttt{0})$ in $\mathcal{B}_s(n, n_0)$ time.
              \end{itemize}
    \end{enumerate}
\end{theorem}

\noindent In theorem \ref{thm:general_upper_bound}, (i) and (ii) extend prior reductions from \cite{alanko2023small}, while (iii) introduces an alternative strategy to handle empty sets using an auxiliary bitvector. By applying succinct rank-select structures to these bounds, they achieved improvements in query times without increasing space usage. For instance, substituting their structure into Theorem~\ref{thm:general_upper_bound} (i) results in a data structure occupying $N\log\sigma + N + o(N\log\sigma + N)$ bits, supporting \Drank{} in $O(\log\log\sigma)$ time and \Dselect{} in constant time. This improves the space constant from $2$ to $1 + 1/\log\sigma$ compared to Alanko et al. \cite{SubsetWT}, while exponentially reducing query times. \vspace{1em}

\noindent For $n_0 > 0$, Theorem~\ref{thm:general_upper_bound} (ii) modifies the bounds to $(N+n_0)\log(\sigma+1) + (N+n_0) + o(n_0\log\sigma + N\log\sigma + N + n_0)$ bits, maintaining the same improved query times. When $n_0 = o(N)$ and $\sigma = \omega(1)$, the space matches the $n_0=0$ case. Alternatively, Theorem~\ref{thm:general_upper_bound}(iii) allows for tailored bitvector structures sensitive to $n_0$.

\begin{theorem}[Space Lower Bound]
    \label{thm:space_lower_bound}
    Let $X$ be a degenerate string of size $N$ over an alphabet $[1,\sigma]$. Any data structure supporting \Drank{} or \Dselect{} on $X$ must use at least $N\log \sigma - o(N\log \sigma)$ bits in the worst case.
\end{theorem}

\noindent In Theorem \ref{thm:space_lower_bound} we aim to establish a lower bound on the space required to represent $X$ while supporting $\Drank{}$ or $\Dselect{}$. Since these operations allow us to reconstruct $X$ fully, any valid data structure must encode $X$ completely. Our approach is to determine the number $L$ of distinct degenerate strings possible for given parameters $N$ and $\sigma$, and to show that distinguishing between these instances necessitates at least $\log_2 L$ bits.

\begin{proof}
    Let $N$ be sufficiently large, and let $\sigma = \omega(\log N)$. Without loss of generality, assume $\log N$ and $N / \log N$ are integers. Consider the class of degenerate strings $X_1, \ldots, X_n$ where $|X_i| = \log N$ for each $i$ and $n = N / \log N$. The number of such strings is given by
    \begin{equation}
        \binom{\sigma}{\log N}^{N / \log N}
    \end{equation}
    This is because each $X_i$ can be formed by choosing $\log N$ characters from $\sigma$ symbols, and there are $n$ such subsets. The number of bits required to represent any degenerate string $X$ must be at least:
    \begin{align*}
        \log \binom{\sigma}{\log N}^{N / \log N} & = \frac{N}{\log N} \log \binom{\sigma}{\log N}                                    \\
                                                 & \geq \frac{N}{\log N} \log \left( \frac{\sigma - \log N}{\log N} \right)^{\log N} \\
                                                 & = N \log \left( \frac{\sigma - \log N}{\log N} \right)                            \\
                                                 & = N \log \sigma - o(N \log \sigma).
    \end{align*}
    Thus, any representation of $X$ that supports $\Drank{}$ or $\Dselect{}$ must use at least $N \log \sigma - o(N \log \sigma)$ bits in the worst case, concluding the proof.
\end{proof}

\subsection{Reductions}{\label{sec:reductions}}

Let $X, \mathcal{D}, \mathcal{B}$ be as in Theorem~\ref{thm:general_upper_bound} and consider $\mathcal{V}$ a data structure (for example the one described by Jacobson in \cite{Jacobson}), which uses $n +o(n)$ bits for a bitstring of length $n$ and supports \Rank{} in constant time and \Select{} in $O(1)$ time. \vspace{1em}

\noindent The reductions in Theorem~\ref{thm:general_upper_bound} rely on the construction of two auxiliary strings $S$ and $R$ derived from the sets $X_i$. When $n_0 = 0$, each $S_i$ is the concatenation of elements in $X_i$, and $R_i$ is a single \texttt{1} followed by $|X_i| - 1$ \texttt{0}s. The global strings $S$ and $R$ are formed by concatenating these, appending a \texttt{1} after $R_n$. The data structure consists of $\mathcal{D}$ built over $S$ and Jacobson's structure $\mathcal{V}$ over $R$, using $\mathcal{D}(N,\sigma) + N + o(N)$ bits. Figure \ref{fig:reduction_example} from \cite{bille2023rank} illustrates this reduction for $n_0 = 0$. \vspace{1em}

\begin{figure}[h!]
    \centering

    \begin{tabular}{c@{\hskip 0.5em}c@{\hskip 0.5em}c@{\hskip 0.5em}c@{\hskip 0.5em}c}
        $X = $                                                               & $\Bigg\{\,\begin{matrix}\texttt{A}\\\texttt{C}\\\texttt{G}\end{matrix}\,\Bigg\}$ &
        $\Bigg\{\,\begin{matrix}\texttt{A}\\\texttt{T}\end{matrix}\,\Bigg\}$ &
        $\Bigg\{\,\begin{matrix}\texttt{C}\end{matrix}\,\Bigg\}$             &
        $\Bigg\{\,\begin{matrix}\texttt{T}\\\texttt{G}\end{matrix}\,\Bigg\}$                                                                                                            \\
                                                                             & $X_1$                                                                            & $X_2$ & $X_3$ & $X_4$
    \end{tabular}\qquad\begin{tabular}{c@{\hskip 0.5em}c@{\hskip 0.5em}c@{\hskip 0.5em}c@{\hskip 0.5em}c@{\hskip 0.5em}c}
        $S =$  & \texttt{ACG} & \texttt{AT} & \texttt{C} & \texttt{TG} &            \\
        $R = $ & \texttt{100} & \texttt{10} & \texttt{1} & \texttt{10} & \texttt{1} \\
               & $S_1$        & $S_2$       & $S_3$      & $S_4$
    \end{tabular}
    \caption{\emph{Left:} A degenerate string $X$ over the alphabet $\{\texttt{A}, \texttt{C}, \texttt{G}, \texttt{T}\}$ where $n = 4$ and $N = 8$. \emph{Right:} The reduction from Theorem~\ref{thm:general_upper_bound} (i) on $X$. White space is for illustration purposes only.}
    \label{fig:reduction_example}
\end{figure}

\noindent Queries are supported as follows: \Drank{} computes the start position of $S_{i+1}$ using $\Select{}_R$, then evaluates the rank in $S$. Conversely, \Dselect{} determines the $i$th occurrence of $c$ in $S$ and identifies the corresponding set via $\Rank{}_R$. Let's consider the pratical example in Figure \ref{fig:reduction_example}: to compute $\Drank{}(2,\texttt{A})$, we first compute $\Select{}_R(3,\texttt{1}) = 6$. Now we know that $S_2$ ends at position $5$, so we return $\Rank{}_S(5, \texttt{A}) = 2$. To compute $\Dselect{}(2, \texttt{G})$ we compute $\Select{}_S(2,\texttt{G}) = 8$, and compute $\Rank{}_R(8,\texttt{1}) = 4$ to determine that position 8 corresponds to $X_4$. \vspace{1em}


\noindent Since \Rank{} and \Select{} on $R$ are constant time, these operations achieve $\mathcal{D}_r(N,\sigma) + O(1)$ and $\mathcal{D}_s(N,\sigma) + O(1)$ time, as required by Theorem~\ref{thm:general_upper_bound} (i).
\vspace{1em}

\noindent For $n_0 \neq 0$, empty sets are replaced by singletons containing a new character $\sigma + 1$, effectively reducing the problem to the $n_0 = 0$ case with $N' = N + n_0$ and $\sigma' = \sigma + 1$. This achieves the bounds of Theorem~\ref{thm:general_upper_bound} (ii).

\paragraph{Alternative Bound} Let $E$ be a bitvector of length $n$, where $E[i] = 1$ if $X_i = \emptyset$ and $E[i] = 0$ otherwise. Define $X''$ as the simplified string derived from $X$ by removing all empty sets. The data structure consists in a reduction (i) applied to $X''$, along with a bitvector structure $\mathcal{B}$ built on $E$. This requires $\mathcal{D}_b(N, \sigma) + N + o(N) + \mathcal{B}_b(n, n_0)$ bits of space. \vspace{1em}

\noindent To support $\Drank{}_X(i, c)$, calculate $k = i - \Rank{}_E(i, \texttt{1})$, which maps $X_i$ to its corresponding set $X''_{k}$. Then, return $\Drank{}_{X''}(k, c)$. This operation runs in $\mathcal{B}_r(n, n_0) + \mathcal{D}_r(N, \sigma) + O(1)$ time. \vspace{1em}

\noindent To support $\Dselect{}_X(i, c)$, first determine $k = \Dselect{}_{X''}(i, c)$, and then return $\Select{}_E(k, \texttt{0})$, which identifies the position of the $k$-th zero in $E$ (i.e., the $k$-th non-empty set in $X$). This operation runs in $\mathcal{B}_s(n, n_0) + \mathcal{D}_s(N, \sigma) + O(1)$, achieving the stated performance bounds.

\subsection{Empirical Results}
The authors conducted a comprehensive evaluation of various data structures for subset rank queries on genomic datasets. Their work emphasizes both space efficiency and query performance, benchmarking methods from \cite{SubsetWT} alongside their proposed designs. The experiments utilized two primary datasets: a pangenome of 3,682 *E. coli* genomes and a human metagenome containing 17 million sequence reads. Testing was conducted in two modes: integrating the subset rank-select structures into a $k$-mer query index and isolating these structures to evaluate their performance on $20$ million $\Drank$ queries, which were randomly generated for controlled comparison. Each result reflects an average over five iterations to ensure robustness. \vspace{1em}

\noindent The study introduces the \emph{dense-sparse decomposition} (DSD) as a novel method extending the principles of subset wavelet trees. This decomposition refines the classic split representation by categorizing sets into empty, singleton, and larger subsets, with optimized handling for each category. The authors incorporated advanced rank-select techniques into this framework, including SIMD-based optimizations. Compared to subset wavelet trees and their modern implementations, the DSD structures consistently demonstrated significant improvements. For example, the SIMD-enhanced DSD achieved query times that were $4$ to $7$ times faster than Concat~(ef), a competitive baseline, while maintaining similar space efficiency. Furthermore, the DSD~(rrr) variant provided comparable space usage to the compact Concat~(ef) structure but offered double the query speed. \vspace{1em}

\noindent The experiments revealed nuanced trade-offs between space and time across all tested structures. While subset wavelet trees, such as Split~(ef) and Split~(rrr), remain strong contenders, the authors' DSD approach often outperformed them in both dimensions. The DSD~(scan) structure, for example, provided a competitive balance, achieving space usage close to entropy bounds while delivering faster query times than comparable subset wavelet tree configurations. The SIMD-enhanced DSD design was particularly noteworthy, achieving near-optimal space efficiency with remarkable query performance.
